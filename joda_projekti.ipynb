{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JoDa kurssin Tampereen BNB projekti. Regressiomallin toteuttaminen hinnalle käyttäen NAS.\n",
    "Koska en tiedä, pitääkö projekti olla yhdessä Jupyter skriptissä vai ei. Niin teen tähän aika pitkän skriptin joka tekee kaiken. Lisäksi laitan mukaan erilliset Jupyter skriptit joissa hieman paremmin ositeltu eri asiat. \n",
    "Projekti löytyy myös \n",
    "[JoDa kurssin repo](https://github.com/THalfar/JoDa) \n",
    "\n",
    "Tämä on hieman hätiköidysti yhteen tehty, koska sähläämiseni takia oletin dediksen olevan vasta ensi viikolla. Aikaa tähän kyllä käytin huhtikuun alussa paljon, kunnes Kaggle kisa vei mennessään. \n",
    "\n",
    "Alla on erikseen tähän projektiin liittyvät skriptit: \n",
    "\n",
    "- \"tampereBNB_datan_kasittely.ipynb\"  Datan esikäsittely\n",
    " \n",
    "- \"tampere_reg.ipynb\" Itse regressiomalli jossa käytetään NAS ja pyritään eri ositteluiden tuottamien neuroverkkojen toiseksi viimeisen kerroksen ulostuloa käyttää ominaisuuksina XGBoost \n",
    "\n",
    "- \"baseline.ipynb\" Tämä on pohjamalli, joka käyttää XGBoost jotta voin verrata tuloksia siihen. Vaikutti siltä, että käytössä oleva aineisto on liian pieni syväoppimiseen, koska tämä tuotti parhaita tuloksia.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Tarvittavat kirjastot \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler \n",
    "import optuna\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from keras import regularizers, layers, optimizers, initializers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN, ModelCheckpoint\n",
    "import time \n",
    "from optuna.integration import TFKerasPruningCallback\n",
    "import pickle \n",
    "from datetime import timedelta\n",
    "import gc\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "import warnings\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! Jos liian ei liian isot tulosteet rasita, kannattaa ottaa nämä mukaan. Itse pidän koko tulosteen katsomisesta, mutta se ei ehkä sovi kaikille.\n",
    "# pd.set_option('display.max_row', None) \n",
    "# pd.set_option('display.max_columns', None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datan esikäsittely. \n",
    "\n",
    "Tässä osiossa käsittelen dataa koneoppimisen mallia varten. \n",
    "\n",
    "Käytännössä tämä on \"tampereBNB_datan_kasittely.ipynb\" johon lisätty kommentteja mitä tapahtuu. Osiossa tarkastellaan käytössä olevaa dataa ja siivotaan sitä jatkokäyttöä varten. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luetaan data \n",
    "df_train = pd.read_csv('./data/Tampere_BNB_training_listing.csv')\n",
    "df_test = pd.read_csv('./data/Tampere_BNB_testing_listing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df_train.head()\n",
    "df_train['Talot.'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "puuttuvat_arvot = df_train.isnull().sum()\n",
    "puuttuvat_arvot = puuttuvat_arvot[puuttuvat_arvot > 0]\n",
    "print(f\"Puuttuvat arvot:\\n {puuttuvat_arvot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "puuttuvat_arvot_prosentteina = (puuttuvat_arvot / len(df_train)) * 100\n",
    "print(f\"Puuttuvat arvot prosentteina:\\n {puuttuvat_arvot_prosentteina}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "puuttuvat_arvot_prosentteina = puuttuvat_arvot_prosentteina.sort_values() \n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.barplot(x=puuttuvat_arvot_prosentteina, y=puuttuvat_arvot_prosentteina.index)\n",
    "plt.title('Puuttuvien arvojen prosenttiosuus sarakkeittain')\n",
    "plt.xlabel('Prosenttiosuus (%)')\n",
    "plt.ylabel('Sarakkeet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muutetaan Rv sarakkeen tyyppi kokonaisluvuksi\n",
    "df_train['Rv'] = df_train['Rv'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df_train['Rv'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Korvataan \"Kunto\" sarakkeen tyhjät arvot \"Ei tietoa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df_train['Kunto'] = df_train['Kunto'].fillna('Ei tietoa')\n",
    "df_train['Kunto'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "kunto_mappaus = {\n",
    "    'Ei tietoa': 0,\n",
    "    'huono': 1,\n",
    "    'tyyd.': 2,\n",
    "    'hyvä': 3,\n",
    "}\n",
    "df_train['Kunto'] = df_train['Kunto'].map(kunto_mappaus)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df_train['Kunto'].value_counts()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df_train['Hissi'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df_train['Hissi'] = df_train['Hissi'].astype('category')\n",
    "df_train['Hissi'] = df_train['Hissi'].cat.codes\n",
    "df_train['Hissi'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df_train['Kaupunginosa'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df_train['Kaupunginosa'] = df_train['Kaupunginosa'].astype('category')   \n",
    "df_train['Kaupunginosa'].value_counts().plot(kind='bar', figsize=(25,5)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Koska kaupunginosia on niin paljon, tein ratkaisun missä liian pienet paikat yhdistetään yhdeksi. Tämä todennäköisesti jälkikäteen mietittynä vähän huono ratkaisu, mutta mennään tällä. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "raja_arvo = 0.01\n",
    "maarat = df_train['Kaupunginosa'].value_counts(normalize=True)\n",
    "pienet_ryhmat = maarat[maarat < raja_arvo].index\n",
    "df_train['Kaupunginosa'] = df_train['Kaupunginosa'].replace(pienet_ryhmat, 'Muu')\n",
    "df_train['Kaupunginosa'].value_counts().plot(kind='bar', figsize= (25,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df_train['Kaupunginosa'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df_train['Kaupunginosa'] = df_train['Kaupunginosa'].astype('category')\n",
    "df_train['Kaupunginosa'] = df_train['Kaupunginosa'].cat.codes\n",
    "df_train['Kaupunginosa'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df_train['Asunnon tyyppi'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "tyyppimat = {\n",
    "    'Yksiö' : 1,\n",
    "    'Kaksi huonetta' : 2,\n",
    "    'Kolme huonetta' : 3,\n",
    "    'Neljä huonetta tai enemmän' : 4\n",
    "}\n",
    "df_train['Asunnon tyyppi'] = df_train['Asunnon tyyppi'].map(tyyppimat)\n",
    "df_train['Asunnon tyyppi'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Tarkastellaan mitä arvoja tämä syönyt \n",
    "df_train['Huoneisto'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alhaalla alan muokkailla \"Huoneisto\" sarakkeen arvoja. Siellä vaikutti olevna paljon epästandartinmukaisia. Tuli hieman harjoiteltua myös regex pitkästä aikaa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df_train['Huoneisto'] = df_train['Huoneisto'].str.lower()\n",
    "df_train['Huoneisto'] = df_train['Huoneisto'].str.replace(' ', '')\n",
    "df_train['Huoneisto'] = df_train['Huoneisto'].str.replace('+', ',')\n",
    "df_train['Huoneisto'] = df_train['Huoneisto'].str.replace('...', \"\")\n",
    "df_train['Huoneisto'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df_train['Huoneisto'] = df_train['Huoneisto'].str.replace('/', ',')\n",
    "df_train['Huoneisto'] = df_train['Huoneisto'].str.replace('[0-9]+h', '', regex=True)\n",
    "df_train['Huoneisto'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df_train['Huoneisto'] = df_train['Huoneisto'].str.replace('^,', '',regex=True)\n",
    "df_train['Huoneisto'] = df_train['Huoneisto'].str.replace('^[-0-9]+', '',regex=True)\n",
    "df_train['Huoneisto'] = df_train['Huoneisto'].str.replace(',$', '',regex=True)\n",
    "df_train['Huoneisto'] = df_train['Huoneisto'].str.replace('^,', '',regex=True)\n",
    "df_train['Huoneisto'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "huoneisto_split = df_train['Huoneisto'].str.split(',')\n",
    "exploded = huoneisto_split.explode()\n",
    "# exploded_unique = exploded.nunique()\n",
    "exploded_unique_count = exploded.value_counts()\n",
    "print(f\"Unique values: {exploded_unique_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Termeille löytyi wikistä \"yleisesti\" hyväksytyt lyhenteet. Joten yritän näiden mukaisesti epämääräisiä arvoja parsia. Alla on tavoitteena saada erikseen irti eri \"Huoneisto\" sarakkeen termejä omiksi kategoriokseen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Käytetään wikisivustoa https://fi.wikipedia.org/wiki/Luettelo_asuntokaupassa_k%C3%A4ytett%C3%A4vist%C3%A4_lyhenteist%C3%A4 termeille \n",
    "\n",
    "import re\n",
    "\n",
    "huoneisto_split = df_train['Huoneisto'].str.split(',')\n",
    "\n",
    "huoneisto_split = huoneisto_split.apply(lambda lst: [\"parveke\" if  re.search('^p$|^parv$', item) else item for item in lst])\n",
    "huoneisto_split = huoneisto_split.apply(lambda lst: [\"wc\" if  re.search('^w$', item) else item for item in lst])\n",
    "huoneisto_split = huoneisto_split.apply(lambda lst: [\"sauna\" if  re.search('^s$', item) else item for item in lst])\n",
    "huoneisto_split = huoneisto_split.apply(lambda lst: [\"2wc\" if  re.search('^erill.wc$|^2xwc$', item) else item for item in lst])\n",
    "huoneisto_split = huoneisto_split.apply(lambda lst: [\"kph\" if  re.search('^kh$', item) else item for item in lst])\n",
    "huoneisto_split = huoneisto_split.apply(lambda lst: [\"alkovi\" if  re.search('^alk$', item) else item for item in lst])\n",
    "huoneisto_split = huoneisto_split.apply(lambda lst: [\"lasit\" if  re.search('^l', item) else item for item in lst])\n",
    "huoneisto_split = huoneisto_split.apply(lambda lst: [\"avok\" if  re.search('^avokeitti&#$', item) else item for item in lst])\n",
    "\n",
    "\n",
    "exploded = huoneisto_split.explode()\n",
    "exploded_lkm = exploded.value_counts()\n",
    "print(f\"Unique values: {exploded_lkm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Koska jotain kummallisuuksia löytyi, joita oli vain pieni määrä. Korvataan nämä \"määrittämätön\" arvolla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "minimi_lkm = 10\n",
    "suodatetut_idx = exploded_lkm[exploded_lkm >= minimi_lkm].index\n",
    "suodatettu_lista = huoneisto_split.apply(lambda lst: [item for item in lst if item in suodatetut_idx])\n",
    "suodatettu_lista = suodatettu_lista.apply(lambda lst: [\"määrittämätön\"] if len(lst) == 0 else lst)\n",
    "\n",
    "df_train['Huoneisto'] = suodatettu_lista.apply(lambda lst: ','.join(lst))\n",
    "\n",
    "# Tarkistetaan vielä vähän tymästi, että onko kaikki kunnossa\n",
    "huoneisto_split = df_train['Huoneisto'].str.split(',')\n",
    "exploded = huoneisto_split.explode()\n",
    "exploded_lkm = exploded.value_counts()\n",
    "print(f\"Uniikit arvot values: {exploded_lkm}\")\n",
    "print(f\"Määrä arvot: {exploded_lkm.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df_train['Huoneisto'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print(df_train.shape)\n",
    "# Splitataan 'Huoneisto' -sarake ja muunnetaan se listaksi pilkun perusteella\n",
    "split_data = df_train['Huoneisto'].str.split(',')\n",
    "\n",
    "# Käytetään explode()-metodia muuntaaksemme listan elementit omiksi riveikseen\n",
    "exploded_data = split_data.explode()\n",
    "\n",
    "# Valinnainen: Suodatetaan pois harvinaiset kategoriat ennen one-hot-enkoodausta\n",
    "# value_counts = exploded_data.value_counts()\n",
    "# to_keep = value_counts[value_counts >= 5].index\n",
    "# filtered_data = exploded_data[exploded_data.isin(to_keep)]\n",
    "\n",
    "# Suoritetaan one-hot-enkoodaus\n",
    "one_hot_encoded = pd.get_dummies(exploded_data)\n",
    "\n",
    "# Summataan yhteen samat rivit, koska explode() luo useita rivejä samalle alkuperäiselle indeksille\n",
    "one_hot_summed = one_hot_encoded.groupby(one_hot_encoded.index).sum()\n",
    "\n",
    "# Yhdistetään one-hot-enkoodatut sarakkeet takaisin alkuperäiseen DataFrameen\n",
    "df_train = df_train.join(one_hot_summed)\n",
    "\n",
    "print(df_train.shape)\n",
    "\n",
    "print(df_train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df_train[\"Talot.\"].value_counts()\n",
    "df_train[\"Talot.\"] = df_train[\"Talot.\"].astype('category')\n",
    "df_train[\"Talot.\"] = df_train[\"Talot.\"].cat.codes\n",
    "df_train[\"Talot.\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df_train[\"Krs\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Päätin korvata puuttuvat arvot 0/0 kerroksella. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Krs\"].isnull().sum()\n",
    "df_train[\"Krs\"] = df_train[\"Krs\"].fillna(\"0/0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df_train[\"Krs\"] = df_train[\"Krs\"].str.replace('^-', '', regex=True)\n",
    "df_train[\"Krs\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kerros_split = df_train[\"Krs\"].str.split('/', expand=True)  \n",
    "df_train[\"kerros\"] = kerros_split[0].astype(int)\n",
    "df_train[\"max_kerros\"] = kerros_split[1].astype(int)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df_train_filtered = df_train.drop(['Huoneisto', 'Krs'], axis=1)\n",
    "df_train_filtered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kommentoin tämä pois, mutta päätin pitää jatkuvuuden kannalta. Koska tämä yksi skripti vedetään putkeen epäjärjevästi. Olen epävarma, joten parempi ratkaisu että yksi typerä putkiskripti ja sitten eriteltynä miten alunperin tein.\n",
    "# df_train_filtered.to_pickle('./data/df_train_filtered.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressiomalli käyttäen NAS kaikilla opetus/validointi ositteluille ja lopussa XGBoost yhdistämässä näitä "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Koska Talot. tyyppejä ei ole kovin montaa, niin yhdistetään ne kaupunginosan kanssa jonka mukaan tehdään testi data setti\n",
    "\n",
    "df_train_filtered['combined'] = df_train_filtered[['Kaupunginosa', 'Talot.']].astype(str).agg('-'.join, axis=1)\n",
    "counts = df_train_filtered['combined'].value_counts()\n",
    "df_train_filtered['combined'] = df_train_filtered['combined'].map(lambda x: 'other' if counts[x] < 2 else x)\n",
    "X = df_train_filtered.drop('Hinta', axis=1)\n",
    "y = df_train_filtered['Hinta']\n",
    "\n",
    "# Splitataan data käyttäen yllä olevaa osittelujoukkona. Nyt kun tätä pohdin monen viikon tauon jälkeen, olisi parempi tapa minkä opin Kagglesta. Aikataulusähläyksen takia en nyt uskalla alkaa kokeilee sitä. \n",
    "\n",
    "X_train, X_test, _ , _ = train_test_split(X, y, test_size=0.1, stratify=df_train_filtered['combined'], random_state=42)\n",
    "X_train.drop('combined', axis=1, inplace=True)\n",
    "X_test.drop('combined', axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jos tätä kukaan jaksaa edes lukea ja kiinnostaa, niin aika näppärä osittelutapa:\n",
    "[Kaggle notebook](https://www.kaggle.com/code/awsaf49/planttraits2024-kerascv-starter-notebook?scriptVersionId=161494049&cellId=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skaalataan numeeriset muuttujat. Koska koordinaatit ja Rv sekä m2 ei ole mielestäni sellaisia, että pitäisi murehtia train ja test setin välillä vuotaisi tietoa, niin skaalataan ne kaikki yhdessä\n",
    "\n",
    "df_train_NN = df_train_filtered.copy()\n",
    "df_train_NN[['Pituusaste', 'Leveysaste']] = MinMaxScaler().fit_transform(df_train_NN[['Pituusaste', 'Leveysaste']])\n",
    "df_train_NN['Rv'] = MinMaxScaler().fit_transform(df_train_NN[['Rv']])\n",
    "df_train_NN['m2'] = MinMaxScaler().fit_transform(df_train_NN[['m2']])\n",
    "\n",
    "# One hot koodataan kategoriset muuttujat\n",
    "df_hot = pd.get_dummies(df_train_NN['Kaupunginosa'], prefix='Kaupunginosa').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN['kerros'], prefix='kerros').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN['max_kerros'], prefix='max_kerros').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN['Kunto'], prefix='Kunto').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN['Hissi'], prefix='Hissi').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN['Asunnon tyyppi'], prefix='Asunnon tyyppi').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN[\"Talot.\"], prefix='Talot.').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "\n",
    "df_train_NN.drop(['Kaupunginosa', 'kerros', 'max_kerros', 'Kunto', 'Hissi', 'Asunnon tyyppi', \"Talot.\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tämä on hieman hatara kohta. Muistelen, että oletin tuon random_state asetuksen pakottavan traintest splitin antamaan saman osittelun. Koska tarvitsen lopussa XGBoost mallille alkuperäisen datan, että NN muokatun "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train_NN.drop('Hinta', axis=1)\n",
    "y = df_train_NN['Hinta']\n",
    "\n",
    "X_train_NN, X_test_NN, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=df_train_NN['combined'], random_state=42)\n",
    "df_strat = X_train_NN['combined'].reset_index(drop=True)\n",
    "\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_train_NN.drop('combined', axis=1, inplace=True)\n",
    "X_test_NN.drop('combined', axis=1, inplace=True)\n",
    "\n",
    "X_train_NN = X_train_NN.to_numpy().astype('float32')    \n",
    "X_test_NN = X_test_NN.to_numpy().astype('float32')\n",
    "\n",
    "y_train = y_train.to_numpy().astype('float32')\n",
    "y_test = y_test.to_numpy().astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alla suoritetaan NAS haku käyttäen Optunaa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Haun nimi\n",
    "study_name = '502_kiireonomamokaa'\n",
    "# Montako osittelua käytettiin\n",
    "folds = 5\n",
    "# Montako epochia kullekin osittelulle\n",
    "epochs_search = 50\n",
    "# Montako satunnaista hakua kieroksella\n",
    "num_random = 3\n",
    "# Montako TPE hakua kieroksella\n",
    "num_tpe = 3\n",
    "\n",
    "# Aika sekuntteina jota hakuun käytetän\n",
    "max_search_time = 180\n",
    "\n",
    "# Neuroneiden maksimimäärä \n",
    "max_units_all = \n",
    "\n",
    "# Logaritminen virhefunktio joka parempi kuin MSE tai MAE hintaregressioon \n",
    "def rmsle_loss(y_true, y_pred):\n",
    "    penalty = tf.constant(1e5, dtype=tf.float32)\n",
    "    valid_mask = tf.math.greater(y_pred, 0.0)\n",
    "    safe_y_pred = tf.where(valid_mask, y_pred, penalty)\n",
    "    rmsle = tf.sqrt(tf.reduce_mean(tf.square(tf.math.log1p(safe_y_pred) - tf.math.log1p(y_true))))\n",
    "    return tf.where(tf.reduce_any(~valid_mask), penalty, rmsle)\n",
    "\n",
    "custom_objects = {\"rmsle_loss\": rmsle_loss}\n",
    "\n",
    "# TF turhaan itkee asioita joita ei tarvitse tietää\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "# optuna.logging.set_verbosity(optuna.logging.WARNING)     \n",
    "\n",
    "\n",
    "\n",
    "def create_model(trial):\n",
    "        \n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=(X_train_NN.shape[1],), name = 'input'))\n",
    "    \n",
    "    num_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "    max_units = max_units_all\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        \n",
    "        num_units = trial.suggest_int(f'n_units_{i}', 16, max_units)\n",
    "        dropout_rate = trial.suggest_float(f'dropout_{i}', 0.0, 0.5)\n",
    "        kernel_regularizer = regularizers.l1_l2(\n",
    "            l1= trial.suggest_float(f'l1_reg_{i}', 1e-8, 1, log=True),\n",
    "            l2= trial.suggest_float(f'l2_reg_{i}', 1e-8, 1, log=True)\n",
    "        )\n",
    "        activation = trial.suggest_categorical(f'activation_{i}', ['relu', 'elu', 'LeakyReLU', 'tanh', 'selu', 'swish'])\n",
    "        \n",
    "        model.add(keras.layers.Dense(num_units, activation=activation, kernel_regularizer=kernel_regularizer, name = f'layer_{i}'))\n",
    "\n",
    "        normalize = trial.suggest_categorical(f'normalize_{i}', [True, False])\n",
    "        if normalize:\n",
    "            model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "        model.add(keras.layers.Dropout(rate=dropout_rate))\n",
    "        \n",
    "        # Valitaan näistä pienempi jolla varmistetaan ettei seuraavilla kerroksilla ala olla enemmän neuroneita\n",
    "        max_units = min(max_units, num_units)  \n",
    "    \n",
    "    model.add(keras.layers.Dense(1, activation='linear')) \n",
    "    \n",
    "    # Optimisaattorin ja oppimisnopeuden valinta\n",
    "    optimizer_options = ['adam', 'rmsprop', 'Nadam', 'adamax', 'Adagrad', 'Adadelta']\n",
    "    optimizer_selected = trial.suggest_categorical('optimizer', optimizer_options)\n",
    "    \n",
    "    if optimizer_selected == 'adam':\n",
    "        optimizer = optimizers.Adam()\n",
    "    elif optimizer_selected == 'rmsprop':\n",
    "        optimizer = optimizers.RMSprop()\n",
    "    elif optimizer_selected == 'Nadam':\n",
    "        optimizer = optimizers.Nadam()\n",
    "    elif optimizer_selected == 'Adagrad':\n",
    "        optimizer = optimizers.Adagrad()\n",
    "    elif optimizer_selected == 'Adadelta':\n",
    "        optimizer = optimizers.Adadelta()\n",
    "    else:\n",
    "        optimizer = optimizers.Adamax()\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=rmsle_loss, metrics=['mse', 'mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Edellinen toteutus oli typerä, koska se kadotti aina parhaan löydetyn mallin painot. Tässä wrapataan Optunaa varten tuo objective funktio, jotta saadaan talteen paras malli.\n",
    "def get_objective(fold, study_name, study):\n",
    "    \n",
    "    def objective(trial):\n",
    "\n",
    "        # Kerätään roskat. Ehkä ei näin pienissä malleissa ongelma, mutta isommissa NAS hauissa ehdoton juttu\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "                \n",
    "        \n",
    "        model = create_model(trial)\n",
    "\n",
    "        checkpoint_filepath = f'./NN_search/{study_name}_fold{fold}_checkpoint.h5'\n",
    "\n",
    "        callbacks = [\n",
    "            TFKerasPruningCallback(trial, 'val_loss'),\n",
    "            ReduceLROnPlateau('val_loss', patience=5, factor=0.5, verbose = 0), \n",
    "            TerminateOnNaN(),\n",
    "            ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_loss', save_best_only=True, verbose=0)\n",
    "        ]\n",
    "\n",
    "        batch_size = trial.suggest_int('batch_size', 16, 128, log=True)    \n",
    "\n",
    "        history = model.fit(X_train_b, y_train_b, epochs=epochs_search, validation_data=(X_val_b, y_val_b), batch_size=batch_size, verbose=0, callbacks=callbacks)\n",
    "        val_loss = np.min(history.history['val_loss'])\n",
    "\n",
    "        # Jos löydetty malli on parempi kuin paras tähän mennessä, niin tallennetaan se\n",
    "        \n",
    "        if trial.number > 0: # Vähän purkkaratkaisu, koska ensimmäisellä trialilla ei ole vielä best_valuea\n",
    "            if val_loss < study.best_value:\n",
    "                model = keras.models.load_model(checkpoint_filepath, custom_objects=custom_objects)\n",
    "                savepath = f'./NN_search/{study_name}_best.h5'\n",
    "                \n",
    "                print('#' * 50)\n",
    "                print(f'New best model found for fold {fold} with value {val_loss}')\n",
    "                print(f'Saving model to {savepath}')\n",
    "                print('#' * 50)\n",
    "                model.save(savepath)\n",
    "        else:\n",
    "            print(f'First trial for fold {fold} completed with value {val_loss}')\n",
    "            model = keras.models.load_model(checkpoint_filepath, custom_objects=custom_objects)\n",
    "            savepath = f'./NN_search/{study_name}_best.h5'\n",
    "            model.save(savepath)\n",
    "\n",
    "        return val_loss\n",
    "\n",
    "    return objective\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "total_time_start = time.time()  \n",
    "search_time_start = time.time() \n",
    "num_completed_trials = 0\n",
    "search_rounds = 0\n",
    "time_taken = 0\n",
    "\n",
    "while time_taken < max_search_time:\n",
    "        \n",
    "    fold = 0 \n",
    "    time_fold_start = time.time()    \n",
    "    skf =  StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "\n",
    "    stratified_labels = df_strat\n",
    "    \n",
    "    for train_index, val_index in skf.split(X_train_NN, stratified_labels):\n",
    "\n",
    "        print('-------------------')\n",
    "        print(f\"Starting fold {fold} search...\")\n",
    "        X_train_b, X_val_b = X_train_NN[train_index], X_train_NN[val_index]    \n",
    "        y_train_b, y_val_b = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        fold_name = f'{study_name}_{fold}'\n",
    "       \n",
    "        # Jokaiselle foldille on oma study\n",
    "        study = optuna.create_study(direction='minimize',\n",
    "                                    pruner=optuna.pruners.MedianPruner(),\n",
    "                                    study_name=fold_name,\n",
    "                                    storage=f'sqlite:///tre_reg.db',\n",
    "                                    load_if_exists=True                                 \n",
    "                                    )\n",
    "        \n",
    "    \n",
    "        fold_time = time.time()    \n",
    "\n",
    "        fold_random = time.time()\n",
    "        # optuna.logging.set_verbosity(optuna.logging.WARNING)     # TODO OTA POIS KUHA TOIMII\n",
    "        QMCs = optuna.samplers.QMCSampler(warn_independent_sampling = False)\n",
    "        TPEs = optuna.samplers.TPESampler(n_startup_trials=0, multivariate=True, warn_independent_sampling = False)\n",
    "\n",
    "        if num_random > 0:   \n",
    "            study.sampler = QMCs\n",
    "            print(f'Random search for fold {fold} with {num_random} trials...')\n",
    "            objective_function = get_objective(fold, fold_name, study)\n",
    "            study.optimize(objective_function, n_trials=num_random)\n",
    "            print(f'Time taken for random search: {str(timedelta(seconds=(time.time() - fold_random)))}')\n",
    "\n",
    "        fold_tpe = time.time()  \n",
    "        if num_tpe > 0:\n",
    "            study.sampler = TPEs\n",
    "            print(f'TPE search for fold {fold} with {num_tpe} trials...')\n",
    "            objective_function = get_objective(fold, fold_name, study)\n",
    "            study.optimize(objective_function, n_trials=num_tpe)\n",
    "            print(f'Time taken for TPE search: {str(timedelta(seconds=(time.time() - fold_tpe)))}')\n",
    "\n",
    "        num_completed_trials += num_random + num_tpe\n",
    "        print('-------------------')\n",
    "        print(f'Finished fold {fold} search.')\n",
    "        print(f\"Time taken for this fold: {str(timedelta(seconds=(time.time() - fold_time)))}\")                \n",
    "        print(f'Fold {fold} best value so far: {study.best_value}')\n",
    "        print(f'Best parameters so far: {study.best_params}')\n",
    "        print(f'Mean time for one trial this fold: {str(timedelta(seconds=(time.time() - fold_time) / (num_random + num_tpe)))}')\n",
    "        print(f'This fold has made total {study.trials_dataframe().shape[0]} trials.')\n",
    "\n",
    "        fold += 1\n",
    "\n",
    "    search_rounds += 1\n",
    "    \n",
    "    time_taken = time.time() - search_time_start\n",
    "    \n",
    "    print(f'\\n# Completed search round: {search_rounds} #')\n",
    "    print(f'Time taken for all folds this round: {str(timedelta(seconds=(time.time() - time_fold_start)))}')\n",
    "    print(f'Total time taken for search: {str(timedelta(seconds=(time.time() - search_time_start)))}')\n",
    "    print(f'Made trials this far: {num_completed_trials}')\n",
    "    print(f\"Current mean time for one trial: {str(timedelta(seconds=(time.time() - search_time_start) / num_completed_trials))}\\n\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nyt meillä on kullekin foldille NAS avulla haettu viisi eri verkkoa. \n",
    "Katsotaan ensin näiden verkkojen virheet \"sanitycheck\" ja sitten jatketaan XGBoost pariin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def rmsle_score(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(np.log1p(y_true+1), np.log1p(y_pred+1)))\n",
    "\n",
    "best_models = []\n",
    "\n",
    "for fold in range(folds):\n",
    "    \n",
    "    model_savepath = f'./NN_search/{study_name}_{fold}_best.h5'\n",
    "    print(f'Loading model for fold {fold} from {model_savepath}')\n",
    "    model = keras.models.load_model(model_savepath, custom_objects=custom_objects)\n",
    "    best_models.append(model)\n",
    "    model.summary()\n",
    "    predictions = model.predict(X_test_NN, verbose = 0)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    rmsle = rmsle_score(y_test, predictions)\n",
    "    \n",
    "    print(f'Model for fold {fold} evaluation:')\n",
    "    print(f\"MSE: {mse:.3f}\")\n",
    "    print(f\"MAE: {mae:.3f}\")\n",
    "    print(f\"R2: {r2:.3f}\")\n",
    "    print(f\"RMSLE: {rmsle:.3f}\")\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huhu! Koodi on kuukausi sitten tehty (29.3) ja ei palaudu hetkessä kaikki mieleen.\n",
    "Hitsi, pahoittelut jos joku tätä sotkua yrittää edes lukea. \n",
    "\n",
    "Ideana oli Optunalla valita NAS mallit mitä napataan mukaan XGBoost. Tästä koko featureryppäästä vielä valitaan scikit-learn SelectKBest mukaisesti parhaat featuret. \n",
    "Setä kyllä ampuu niin tykillä kärpästä :D \n",
    "Muistinkin miksi Kaggle kisa vetäisi mukaansa. Baselinenä toiminut pelkkä XGBoost antoi parempia tuloksia kuin tämä härveli. Data on yksinkertaisesti liian pieni mielestäni syväoppimiseen, ettei oikein ylioppimista voi välttää. \n",
    "Olinkin unohtanut tuon ominaisuuksien valinnan joka oli mielestäni ihan fiksua. Pitääkin se napata mukaan Kaggle viritelmääni jossa ei aikataulukiirettä. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from tensorflow.keras.models import Model\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "import time\n",
    "from tensorflow.keras.models import Model\n",
    "import glob\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "# Montko quasirandom samplea ja montako tpe samplea ja kauanko tätä härveliä pyöritellään \n",
    "num_random = 3\n",
    "num_tpe = 3\n",
    "max_time_xgb = 180\n",
    "\n",
    "\n",
    "X_train_features_list = []\n",
    "X_test_features_list = []\n",
    "features_names_list = []\n",
    "\n",
    "for idx, model in enumerate(best_models):\n",
    "    feature_extractor = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "    X_train_features = feature_extractor.predict(X_train_NN)\n",
    "    X_test_features = feature_extractor.predict(X_test_NN)\n",
    "\n",
    "    has_nan = np.isnan(X_train_features).any() or np.isnan(X_test_features).any()\n",
    "    print(f'Model {idx} has nan values: {has_nan}')\n",
    "    \n",
    "    X_train_features_list.append(X_train_features)\n",
    "    X_test_features_list.append(X_test_features)\n",
    "\n",
    "    num_features = X_train_features.shape[1]\n",
    "    model_feature_names = [f\"model_{idx}_feature_{feature_idx}\" for feature_idx in range(num_features)]\n",
    "    features_names_list.append(model_feature_names)\n",
    "\n",
    "\n",
    "original_feature_names = list(X_train.columns) \n",
    "combined_feature_names = original_feature_names + features_names_list\n",
    "\n",
    "# Yhdistetään ominaisuusvektorit\n",
    "\n",
    "def select_models(X_train_features_list, X_test_features_list, features_names_list, which_models):\n",
    "\n",
    "    X_train_selected = []\n",
    "    X_test_selected = []\n",
    "    selected_names = []\n",
    "\n",
    "    if all(not choosenode for choosenode in which_models):        \n",
    "        X_train_selected = None\n",
    "        X_test_selected = None\n",
    "        selected_names = None\n",
    "\n",
    "    else:\n",
    "        for idx, choosenode in enumerate(which_models):\n",
    "            if choosenode: \n",
    "                X_train_selected.append(X_train_features_list[idx])\n",
    "                X_test_selected.append(X_test_features_list[idx])\n",
    "                selected_names.extend(features_names_list[idx])                \n",
    "            \n",
    "        X_train_selected = np.concatenate(X_train_selected, axis=1)\n",
    "        X_test_selected = np.concatenate(X_test_selected, axis=1)\n",
    "        \n",
    "        \n",
    "    return X_train_selected, X_test_selected, selected_names\n",
    "    \n",
    "\n",
    "X_train_combined = np.concatenate(X_train_features_list, axis=1)\n",
    "X_test_combined = np.concatenate(X_test_features_list, axis=1)\n",
    "\n",
    "X_train_combined = np.concatenate([X_train_combined, X_train], axis=1)   \n",
    "X_test_combined = np.concatenate([X_test_combined, X_test], axis=1)\n",
    "\n",
    "\n",
    "def select_features(X_train_combined, X_test_combined, y_train, combined_feature_names, method, max_feature):\n",
    "    \n",
    "    if method == 'f_regression':\n",
    "        method_function = f_regression\n",
    "    elif method == 'mutual_info_regression':\n",
    "        method_function = mutual_info_regression\n",
    "    else:\n",
    "        method_function = None\n",
    "\n",
    "    if method_function is not None:\n",
    "        selector = SelectKBest(method_function, k=max_feature)\n",
    "        X_train_combined_selected = selector.fit_transform(X_train_combined, y_train)\n",
    "        X_test_combined_selected = selector.transform(X_test_combined)\n",
    "        selected_indices = selector.get_support(indices=True)\n",
    "        selected_features_names = np.array(combined_feature_names)[selected_indices]\n",
    "        selected_features_names = selected_features_names.tolist()\n",
    "        \n",
    "        return X_train_combined_selected, X_test_combined_selected, selected_features_names\n",
    "    else:\n",
    "        return X_train_combined, X_test_combined, combined_feature_names\n",
    "\n",
    "\n",
    "def rmsle(y_true, y_pred):\n",
    "    if np.any(y_pred <= 0):\n",
    "        return 1e6\n",
    "    return np.sqrt(np.mean(np.square(np.log1p(y_pred) - np.log1p(y_true))))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Määritetään parametrit, jotka optimoidaan XGBoosrt\n",
    "    param = {\n",
    "        \"booster\": \"dart\",        \n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 6),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-4, 1.0, log=True),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-4, 1.0, log=True),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-2, 2, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.2, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0.1, 42, log = True),     \n",
    "        'nthread' : -2\n",
    "    }\n",
    "\n",
    "    num_boost_round = trial.suggest_int('BRounds', 1, 142)  \n",
    "\n",
    "    # Alla olevat liittyvät ominaisuuksien valitsemiseen. Valitettavasti nyt tämä härveli tukee vaan 5 foldia. Ei aikaa kiire on yöllä, että dynaamisesti tämä foldien mukaan. \n",
    "    selector = trial.suggest_categorical('S', choices = ['f_regression', 'mutual_info_regression', 'None'])\n",
    "    select_0 = trial.suggest_categorical('S0', [True, False])\n",
    "    select_1 = trial.suggest_categorical('S1', [True, False])\n",
    "    select_2 = trial.suggest_categorical('S2', [True, False])\n",
    "    select_3 = trial.suggest_categorical('S3', [True, False])\n",
    "    select_4 = trial.suggest_categorical('S4', [True, False])\n",
    "\n",
    "    X_train_combined_selected, X_test_combined_selected , selected_features_names = select_models(X_train_features_list, X_test_features_list, features_names_list, [select_0, select_1, select_2, select_3, select_4])\n",
    "\n",
    "    if X_train_combined_selected is not None:\n",
    "        combined_feature_names = selected_features_names + original_feature_names\n",
    "        X_train_combined_selected = np.concatenate([X_train_combined_selected, X_train], axis=1)\n",
    "        X_test_combined_selected = np.concatenate([X_test_combined_selected, X_test], axis=1)\n",
    "    else:\n",
    "        X_train_combined_selected = X_train\n",
    "        X_test_combined_selected = X_test\n",
    "        combined_feature_names = original_feature_names\n",
    "\n",
    "\n",
    "    num_selected = trial.suggest_int('N_fea', 1, X_train_combined_selected.shape[1])\n",
    "    X_train_combined_selected, _ , combined_feature_names = select_features(X_train_combined_selected, X_test_combined_selected, y_train, combined_feature_names, selector, num_selected)\n",
    "    # print(f\"Selected features: {combined_feature_names}\")\n",
    "    \n",
    "    \n",
    "    rmsle_scores = []\n",
    "    dtrain_full = xgb.DMatrix(X_train_combined_selected, label=y_train, feature_names=combined_feature_names)\n",
    "\n",
    "    skf =  StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "    stratified_labels = df_strat\n",
    "    for train_index, val_index in skf.split(X_train_NN, stratified_labels):\n",
    "        \n",
    "        dtrain = dtrain_full.slice(train_index)\n",
    "        dval = dtrain_full.slice(val_index)\n",
    "\n",
    "        evals_result = {}\n",
    "        bst = xgb.train(param, dtrain, num_boost_round=num_boost_round, evals=[(dval, 'val')], evals_result=evals_result, verbose_eval=False, early_stopping_rounds=100)\n",
    "        best_iteration = bst.best_iteration\n",
    "        preds = bst.predict(dval, iteration_range=(0, best_iteration + 1))\n",
    "        y_true = y_train[val_index]       \n",
    "        loss = rmsle(y_true, preds)\n",
    "        rmsle_scores.append(loss)\n",
    "\n",
    "    average_rmsle = np.mean(rmsle_scores)\n",
    "    return average_rmsle\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "\n",
    "# optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "# optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "\n",
    "\n",
    "time_xgb_start = time.time()\n",
    "\n",
    "QMCsampler = optuna.samplers.QMCSampler(warn_independent_sampling = False)\n",
    "TPEsampler = optuna.samplers.TPESampler(multivariate=True, n_startup_trials=0, warn_independent_sampling = False)\n",
    "\n",
    "# Koulutuslooppi joka kouluttaa XGBoost kunnes aikaraja saavutettu\n",
    "while time.time() - time_xgb_start < max_time_xgb:\n",
    "\n",
    "    \n",
    "    print(f'Random sampling {num_random} trials...')\n",
    "    study.optimize(objective, n_trials=num_random)\n",
    "    study.sampler = QMCsampler    \n",
    "    print(f'TPE sampling {num_tpe} trials...')\n",
    "    study.sampler = TPEsampler\n",
    "    study.optimize(objective, n_trials=num_tpe)\n",
    "    print(f'Time taken so far: {str(timedelta(seconds=(time.time() - time_xgb_start)))}')\n",
    "\n",
    "print(f'Time taken for XGBoost optimization: {str(timedelta(seconds=(time.time() - time_xgb_start)))}')\n",
    "print(f'Time taken for one trial: {str(timedelta(seconds=(time.time() - time_xgb_start) / (num_random + num_tpe)))}')\n",
    "\n",
    "print(f\"Best val: {study.best_trial.value}\")\n",
    "print(f'Best params: {study.best_params}')\n",
    "\n",
    "\n",
    "# Nyt kun malli on valittu, niin voidaan kouluttaa se koko datalla ja testata sitä testidatalla\n",
    "# Alla ensin otetaan Optunan valitsemat mallit jotka tuossa lopun listassa ovat \"päälle/pois\"\n",
    "X_train_combined_selected, X_test_combined_selected , selected_features_names = select_models(X_train_features_list, X_test_features_list, features_names_list, [study.best_params['S0'], study.best_params['S1'], study.best_params['S2'], study.best_params['S3'], study.best_params['S4']])\n",
    "\n",
    "if X_train_combined_selected is not None:\n",
    "    combined_feature_names = selected_features_names + original_feature_names\n",
    "    X_train_combined_selected = np.concatenate([X_train_combined_selected, X_train], axis=1)\n",
    "    X_test_combined_selected = np.concatenate([X_test_combined_selected, X_test], axis=1)\n",
    "else:\n",
    "    X_train_combined_selected = X_train\n",
    "    X_test_combined_selected = X_test\n",
    "    combined_feature_names = original_feature_names\n",
    "\n",
    "# Optuna myös valitsi kuinka monta parasta ominaisuutta napattiin tästä ominausuuskasasta mukaan.\n",
    "X_train_combined_selected, X_test_combined_selected, selected_features_names = select_features(X_train_combined_selected, X_test_combined_selected, y_train, combined_feature_names, study.best_params['S'], study.best_params['N_fea'])\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train_combined_selected, label=y_train, feature_names=selected_features_names)\n",
    "best_model = xgb.train(study.best_params, dtrain, num_boost_round=study.best_params['BRounds'])\n",
    "\n",
    "# Testataan mallia koulutusdatalla jotta voidaan arvioda onko tässä koko hommassa mitään järkeä\n",
    "pred_train = best_model.predict(dtrain)\n",
    "mae_train = mean_absolute_error(y_train, pred_train)\n",
    "mse_train = mean_squared_error(y_train, pred_train)\n",
    "r2_train = r2_score(y_train, pred_train)\n",
    "rmsle_train = rmsle(y_train, pred_train)\n",
    "print(f\"Train MAE: {mae_train}, Train MSE: {mse_train}, Train R2: {r2_train}, Train RMSLE: {rmsle_train}\")\n",
    "\n",
    "# Testataan mallia testidatalla  \n",
    "dtest = xgb.DMatrix(X_test_combined_selected, label=y_test, feature_names=selected_features_names)\n",
    "predictions = best_model.predict(dtest)\n",
    "\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "rmsle_val = rmsle(y_test, predictions)\n",
    "print(f\"MAE: {mae}, MSE: {mse}, R2: {r2}, RMSLE: {rmsle_val}\")\n",
    "\n",
    "# Visualisoidaan ennustettuja arvoja verrattuna todellisiin arvoihin\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(y_test, predictions, edgecolors=(0, 0, 0))\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "plt.xlabel('Measured')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Measured vs. Predicted Values')\n",
    "plt.show()\n",
    "\n",
    "# Katsellaan vielä mistä XGBoost oli kiinnostunut\n",
    "fig, ax = plt.subplots(figsize=(20, 30))\n",
    "xgb.plot_importance(best_model, importance_type='weight', ax=ax)\n",
    "ax.set_title('Feature Importance by Weight', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 30))\n",
    "xgb.plot_importance(best_model, importance_type='gain', ax=ax)\n",
    "ax.set_title('Feature Importance by Gain', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 30))\n",
    "xgb.plot_importance(best_model, importance_type='cover', ax=ax)\n",
    "ax.set_title('Feature Importance by Cover', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "joda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
