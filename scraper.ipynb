{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d44bd9f2",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "As a data scientist, you've found a data treasure on TampereBNB - the top online platform for short-term housing rentals in the city of Tampere. From thousands of listings, you can extract valuable insights to inform decision-making, marketing, and research. Your goal is to scrape and clean this data to enable further analysis.\n",
    "\n",
    "## Accessing the HTML dataset\n",
    "The quick way to get HTML data is by saving the HTML file to your computer manually. Also, a web page's HTML is known to change over time. Scraping code can break easily when web redesigns occur, which makes scraping brittle and not recommended for projects with longevity. So, to ease access and manage the traffic, we have scraped the TampereBNB website and extracted and stored data in the `data/accom` folder. Please, just use these HTML files provided to you and pretend like you saved them yourself. I recommend that you do and open the HTML files in your preferred text editor to inspect the HTML for the how the website structured information.\n",
    "\n",
    "## TODO\n",
    "\n",
    "- Students are expected to extract following information:\n",
    "    - Region\n",
    "    - Price of the accommodation\n",
    "    - Apartment type\n",
    "    - Square meters m2\n",
    "    - Apartment floor\n",
    "    - Construction year\n",
    "    - Apartment status\n",
    "    - The availability of an elevator\n",
    "    - Longitude\n",
    "    - Latitude\n",
    "- change the data format of, if necessary:\n",
    "    - Floor\n",
    "    - Size\n",
    "    - Construction\n",
    "    - Longitude\n",
    "    - Latitude\n",
    "- Save the data frame in a pickle file\n",
    "\n",
    "Screenshot of how the dataframe is expected to look like:\n",
    "\n",
    "![df_screenshot](./data/images/df_screenshot.png) \n",
    "## Notes\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Do not:</b> change the Jupyter Notebook file's name.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Use:</b> the given list as column names.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Save:</b> the dataframe as a pickle file with the name \"TampereBNB.pkl\".\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> Please keep in mind that although this template is designed in a linear style, generally, data wrangling is an iterative process.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6223b381",
   "metadata": {},
   "source": [
    "# Reminder\n",
    "\n",
    "This section is devoted to refreshing your memory on prerequisites to complete the assignment. \n",
    "\n",
    "## How does web scraping work? \n",
    "Website data is written in HTML (HyperText Markup Language) which uses tags to structure the page. Because HTML and its tags are just text, the text can be accessed using parsers . We'll be using a Python parser called [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/).\n",
    "\n",
    "The following script can be used to download HTML files programmatically. **However, please note that this repository cames with an offline copy of the files so there is no need to run this script.**\n",
    "\n",
    "```python\n",
    "for listing_index in range(0, 1080):\n",
    "     print(\"getting the %1d url:\" % listing_index)\n",
    "     url = 'https://infotuni.github.io/tamperebnb/accom/M20%1d23' % listing_index\n",
    "     print(url)\n",
    "     page = requests.get(url)\n",
    "     fname = 'data/accom/M20%1d23.html' % listing_index\n",
    "     print(fname)\n",
    "     with open(fname, 'wb') as f:\n",
    "         f.write(page.content)\n",
    "     time.sleep(1 + random.random() * 2)\n",
    "```\n",
    "\n",
    "## HTML file structure\n",
    "\n",
    "The Hypertext Markup Language (or HTML) is the language used to create documents for the World Wide Web. You can use [w3school](https://www.w3schools.com/html/default.asp) to refresh your memory. \n",
    "\n",
    "The HTML element is everything from the start tag to the end tag: <br />\n",
    "\n",
    "`<opening tag> content...</closing tag>`\n",
    "\n",
    "### HTML Elements\n",
    "#### Heading\n",
    "elements are used for section headings.\n",
    "\n",
    "```\n",
    "<h1>Heading 1</h1>\n",
    "<h2>Heading 2</h2>\n",
    "<h3>Heading 3</h3>\n",
    "<h4>Heading 4</h4>\n",
    "<h5>Heading 5</h5>\n",
    "<h6>Heading 6</h6>\n",
    "```\n",
    "\n",
    "#### Paragraph\n",
    "elements are used for standard blocks of text\n",
    "\n",
    "```\n",
    "<p>This is just a block of text.</p>\n",
    "```\n",
    "\n",
    "#### Span\n",
    "elements are used to group text within another block of text, often for styling.\n",
    "```\n",
    "<p>This block of text has a <span>element</span> inside it.</p>\n",
    "\n",
    "```\n",
    "#### Image\n",
    "elements are used to embed images in a web page\n",
    "```\n",
    "<img src=\"image-file.jpg\" alt=\"text that describes the image\" />\n",
    "```\n",
    "\n",
    "### Trees represent hierarchical data\n",
    "\n",
    "Web developers use trees to represent the data that makes up websites. Elements belong to each other or are descended from one another.\n",
    "\n",
    "\n",
    "![Tree](./data/images/trees.png)\n",
    "\n",
    "We can create a tree structure in HTML by putting elements inside other elements. To do this we often use a `<div>  `element as a container. `<div` elements are used to group chunks of content together.\n",
    "\n",
    "```\n",
    "<div>\n",
    "    <h1>This is a heading.</h1>\n",
    "    <p>This is a paragraph.</p>\n",
    "</div>\n",
    "```\n",
    "\n",
    "![HTML&Tree](./data/images/HTML&Tree.png)\n",
    "\n",
    "\n",
    "## Using Beautiful Soup\n",
    "\n",
    "#### Use the Find, select Methods\n",
    "`find()` is one of the most popular Beautiful Soup methods. It is similar to the find feature in a text editor. <br/>\n",
    "`select_one()` which finds only the first tag that matches a selector. <br />\n",
    "To find classes you can use: \n",
    "* `{'class': 'class_name'}`\n",
    "* `class_='class_name'`\n",
    "* `.class_name`\n",
    "\n",
    "#### Example:\n",
    "Here we attempt to use find method to get the title of the webpage:\n",
    "```\n",
    "soup.find('text_string')\n",
    "```\n",
    "\n",
    "When we apply this to get the title of our webpage:\n",
    "```\n",
    "soup.find('title')\n",
    "```\n",
    "\n",
    "We get the title element of the webpage with its opening and closing tags\n",
    "\n",
    "```\n",
    "<title>Tampere BNB</title>\n",
    "\n",
    "```\n",
    "\n",
    "To get the webpage title only, we can use `.text`:\n",
    "```\n",
    "soup.find('title').text\n",
    "```\n",
    "\n",
    "which gives us:\n",
    "```\n",
    "'Tampere BNB'\n",
    "```\n",
    "\n",
    "\n",
    "## Installing packages\n",
    "\n",
    "using conda:\n",
    "```python\n",
    "    conda install <package name>\n",
    "```\n",
    "\n",
    "using pip:\n",
    "```python\n",
    "    pip install <package name>\n",
    "```\n",
    "\n",
    "using pip within Jupyter Notebook:\n",
    "```python\n",
    "    !pip install <package name>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852b0df0",
   "metadata": {},
   "source": [
    "# Gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319c09fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import os # establish the interaction between the user and the operating system\n",
    "import glob # allows for Unix-style pathname pattern \n",
    "\n",
    "import re #check whether a given string matches a given pattern\n",
    "from bs4 import BeautifulSoup # pull data out of HTML and XML files\n",
    "\n",
    "import pandas as pd #pulling data out of HTML and XML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaea492",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check currect directory\n",
    "path = os.getcwd()\n",
    "print (\"The current working directory is %s\" % path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6975c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading files\n",
    "indices = glob.glob('./data/accom/*.html')\n",
    "len(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb77adf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data from local\n",
    "local_files = list()\n",
    "for fname in glob.glob('./data/accom/*.html'):\n",
    "    local_files.append(fname.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278c97ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please, use the following list as a column name\n",
    "column = [\"Region\", \"Price\", \"Type\", \"Size\", \"Floor\", \"Construction\", \"Condition\", \"Elevator\", \"Longitude\", \"Latitude\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd348f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dictionary of lists with pre-specified columns/keys\n",
    "accom = dict()\n",
    "for col in column:\n",
    "    accom[col] = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dfbd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in indices:\n",
    "    with open(fname) as f:\n",
    "        content = f.read()\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    page_content = soup.find('div', {'class': 'container'})\n",
    "    #getting the price\n",
    "    price = page_content.select_one('.basic-info .profile_price').text\n",
    "    price = price.split(\" \")[0].strip()\n",
    "    accom[\"Price\"].append(price)\n",
    "\n",
    "    # TODO: get all info on space\n",
    "    \n",
    "    # TODO: get the region\n",
    "    region = page_content.select_one('.about_space_basic .about_space_region').text\n",
    "    region = region.split(\" \")[-1].strip()\n",
    "    accom[\"Region\"].append(region)\n",
    "    \n",
    "    \n",
    "    # TODO: get the apartment type\n",
    "    type = page_content.select_one('.about_space_basic .about_space_type').text\n",
    "    type = type.split(\":\")[-1].strip()\n",
    "    accom[\"Type\"].append(type)\n",
    "    \n",
    "\n",
    "    # TODO: get the apartment size\n",
    "    size = page_content.select_one('.about_space_basic .about_space_area').text\n",
    "    size = size.split(\":\")[-1].strip().split(\" \")[0].strip()\n",
    "    accom[\"Size\"].append(size)\n",
    "    \n",
    "    # TODO: get the apartment floor\n",
    "    floor = page_content.select_one('.about_space_basic .about_space_floor').text\n",
    "    floor = floor.split(\":\")[-1].strip().split(\" \")[4].strip()\n",
    "    accom['Floor'].append(floor)\n",
    "    \n",
    "    \n",
    "    # TODO: get the apartment construction year\n",
    "    construction = page_content.select_one('.about_space_basic .about_space_year').text\n",
    "    construction = construction.split(\":\")[-1].strip().split(\" \")[7]\n",
    "    accom['Construction'].append(construction)\n",
    "    \n",
    "\n",
    "    # TODO: get the apartment status\n",
    "    condition = page_content.select_one('.about_space_basic .Guest.about_space_condition').text\n",
    "    condition = condition.split(\":\")[-1].strip().split(\" \")[13].strip()\n",
    "    accom['Condition'].append(condition)\n",
    "    \n",
    "\n",
    "    # TODO: get the availability of the elevator\n",
    "    elevator = page_content.select_one('.offers').text\n",
    "    elevator = elevator.split(\" \")[8]\n",
    "    accom['Elevator'].append(elevator)\n",
    "    \n",
    "    # TODO: get the longitude\n",
    "    longitude = page_content.select_one('.map').text\n",
    "    longitude = longitude.split(\"longitude\")[1].strip().split(\" \")[1]\n",
    "    accom['Longitude'].append(longitude)\n",
    "    \n",
    "\n",
    "    latitude = page_content.select_one('.map').text\n",
    "    latitude = latitude.split(\"latitude\")[1].strip().split(\" \")[1].rstrip('.')\n",
    "    accom['Latitude'].append(latitude)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60790dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in accom:\n",
    "#     print(key, len(accom[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13837d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#constructing a DataFrame from a dict\n",
    "df = pd.DataFrame.from_dict(accom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11e0209",
   "metadata": {},
   "source": [
    "# Access\n",
    "\n",
    "Assessing your data is the second step in data wrangling. When assessing, you're like a detective at work, inspecting your dataset for two things: data quality issues (i.e. content issues) and lack of tidiness (i.e. structural issues).\n",
    "- Quality: issues with content. Low quality data is also known as dirty data.\n",
    "- Tidiness: issues with structure that prevent easy analysis. Untidy data is also known as messy data. Tidy data requirements:\n",
    "    - Each variable forms a column.\n",
    "    - Each observation forms a row.\n",
    "    - Each type of observational unit forms a table.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> This part is not graded and the codes are provided, you can just run the cells and access your dataset.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d0eac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading pickle file and getting the first five rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed79dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading pickle file and getting the last five rows\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599c880d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gettign a summary of the data set\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0b8e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check null values in Price feature\n",
    "df[df['Price'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb5cadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check duplicated values\n",
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd49faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gettign a summary statistics of the construction feature\n",
    "df[[\"Construction\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081343d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gettign a series of unique values of apartment Type in feature\n",
    "df[\"Type\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276a6f7f",
   "metadata": {},
   "source": [
    "# Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87571237",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a copy of this object’s indices and data.\n",
    "df_clean = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3441433",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the data format\n",
    "\n",
    "#changing the data format of floor feature\n",
    "df_clean[\"Floor\"] = pd.to_numeric(df_clean[\"Floor\"])\n",
    "# TODO: change the data format of Size feature\n",
    "df_clean[\"Size\"] = pd.to_numeric(df_clean[\"Size\"])\n",
    "\n",
    "# TODO: change the data format of Longitude feature\n",
    "df_clean[\"Longitude\"] = pd.to_numeric(df_clean[\"Longitude\"])\n",
    "\n",
    "# TODO: change the data format of Latitude feature\n",
    "df_clean[\"Latitude\"] = pd.to_numeric(df_clean[\"Latitude\"])\n",
    "\n",
    "# TODO: change the data format of Construction feature\n",
    "df_clean[\"Construction\"] = pd.to_numeric(df_clean[\"Construction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d8ce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check changes\n",
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c9bac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save and load latest changes\n",
    "df_clean.to_pickle(\"TampereBNB.pkl\")\n",
    "unpickled_df = pd.read_pickle(\"TampereBNB.pkl\")\n",
    "unpickled_df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
