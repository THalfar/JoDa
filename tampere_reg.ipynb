{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_train_filtered = pd.read_pickle('./data/df_train_filtered.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_train_filtered.drop('Hinta', axis=1)\n",
    "y = df_train_filtered['Hinta']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=df_train_filtered['Kaupunginosa'], random_state=42)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score,  mean_absolute_error\n",
    "\n",
    "def rmsle_score(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(np.log1p(y_true+1), np.log1p(y_pred+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 9450.37\n",
      "Mean absolute error: 68.02\n",
      "R²-arvo: 0.63\n",
      "RMSLE: 0.32\n",
      "Parhaan mallin R²-arvo: 0.6270273210094915\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "predictions = lr.predict(X_test)\n",
    "\n",
    "r2 = r2_score(y_test, predictions)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print(f\"Mean squared error: {mse:.2f}\\nMean absolute error: {mae:.2f}\\nR²-arvo: {r2:.2f}\\nRMSLE: {rmsle_score(y_test, predictions):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "ax.scatter(y_test, predictions, edgecolors=(0, 0, 0))\n",
    "ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Measured')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.title('Measured vs. Predicted Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "virhe_mallit = [xgboost.XGBRegressor(objective='reg:absoluteerror'), xgboost.XGBRegressor(objective='reg:squarederror')]\n",
    "virhe_nimi = ['neg_mean_absolute_error', 'neg_mean_squared_error']\n",
    "\n",
    "\n",
    "for idx, malli in enumerate(virhe_mallit):\n",
    "\n",
    "    param_space = {\n",
    "        'n_estimators': np.arange(1, 500, 10),\n",
    "        'max_depth': np.arange(3, 11),\n",
    "        'learning_rate': [0.1, 0.01, 0.001],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "        'gamma': [0, 1, 5],\n",
    "        'reg_alpha': [0, 0.1, 0.5],\n",
    "        'reg_lambda': [1, 1.5, 2]\n",
    "    }\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=malli,\n",
    "        param_distributions=param_space,\n",
    "        cv=5,\n",
    "        n_jobs=-2,\n",
    "        n_iter= 1000,\n",
    "        verbose=1,\n",
    "        scoring=virhe_nimi[idx],  \n",
    "    )\n",
    "\n",
    "\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    best_index = random_search.best_index_\n",
    "    cv_results = random_search.cv_results_\n",
    "    cv_splits = random_search.cv\n",
    "    best_scores = [cv_results[f'split{i}_test_score'][best_index] for i in range(cv_splits)]\n",
    "\n",
    "\n",
    "    print(f\"With error: {virhe_nimi[idx]}\")\n",
    "    for i, score in enumerate(best_scores):\n",
    "        print(f\"Ositus {i}: {-score}\")\n",
    "\n",
    "    best_model = random_search.best_estimator_\n",
    "    predictions = best_model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)    \n",
    "    print(f\"Mean squared error: {mse:.2f}\\nMean absolute error: {mae:.2f}\\nRMSLE: {rmsle_score:.4f}\\nParhaan mallin R²-arvo: {r2:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.scatter(y_test, predictions, edgecolors=(0, 0, 0))\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "    plt.xlabel('Measured')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.title(f'{virhe_nimi[idx]} Measured vs. Predicted Values')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler \n",
    "\n",
    "# Skaalataan numeeriset muuttujat\n",
    "robust_scaler = RobustScaler()\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "df_train_NN = df_train_filtered.copy()\n",
    "df_train_NN[['Pituusaste', 'Leveysaste']] = minmax_scaler.fit_transform(df_train_NN[['Pituusaste', 'Leveysaste']])\n",
    "df_train_NN['Rv'] = minmax_scaler.fit_transform(df_train_NN[['Rv']])\n",
    "df_train_NN['m2'] = minmax_scaler.fit_transform(df_train_NN[['m2']])\n",
    "\n",
    "# One hot koodataan kategoriset muuttujat\n",
    "df_hot = pd.get_dummies(df_train_NN['Kaupunginosa'], prefix='Kaupunginosa').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN['kerros'], prefix='kerros').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN['max_kerros'], prefix='max_kerros').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN['Kunto'], prefix='Kunto').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN['Hissi'], prefix='Hissi').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN['Asunnon tyyppi'], prefix='Asunnon tyyppi').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN[\"Talot.\"], prefix='Talot.').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "\n",
    "df_train_NN.drop(['Kaupunginosa', 'kerros', 'max_kerros', 'Kunto', 'Hissi', 'Asunnon tyyppi', \"Talot.\"], axis=1, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muodostetaan X ja y sekä jaetaan data harjoitus- ja testijoukkoihin\n",
    "\n",
    "X = df_train_NN.drop('Hinta', axis=1)\n",
    "y = df_train_NN['Hinta']\n",
    "\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from keras_tuner import Hyperband\n",
    "# from keras import regularizers, layers, optimizers, initializers\n",
    "# from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "# from sklearn.model_selection import KFold, train_test_split\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import time\n",
    "\n",
    "\n",
    "# models_hyperband = []\n",
    "# mse_scores = []\n",
    "# mae_scores = []\n",
    "# r2_scores = []\n",
    "# best_hyperparameters_hyperband = []\n",
    "\n",
    "# search_time_start = time.time() \n",
    "\n",
    "# def build_model(hp):\n",
    "#     model = keras.Sequential()\n",
    "#     model.add(layers.Input(shape=(X_train.shape[1],)))\n",
    "\n",
    "#     # Luodaan kerroksia käyttäen Hyperband-optimoinnin hyperparametreja\n",
    "#     for i in range(hp.Int('num_layers', 1, 4)):  # Vaihteluväli kerrosten määrälle\n",
    "#         model.add(layers.Dense(\n",
    "#             units=hp.Int(f'units_{i}', min_value=8, max_value=512, step=16),\n",
    "#             activation=hp.Choice(f'activation_{i}', values=['relu', 'linear', 'selu', 'elu', 'sigmoid', 'tanh']),\n",
    "#             kernel_regularizer=regularizers.l1_l2(\n",
    "#                 l1=hp.Float(f'l1_reg_{i}', min_value=1e-6, max_value=1, sampling='log'),\n",
    "#                 l2=hp.Float(f'l2_reg_{i}', min_value=1e-6, max_value=1, sampling='log')),\n",
    "#             kernel_initializer=hp.Choice('initializer', values=['he_normal', 'glorot_uniform', 'lecun_normal', 'glorot_normal'])\n",
    "#             )\n",
    "#         )\n",
    "#         model.add(layers.Dropout(hp.Float(f'dropout_{i}', min_value=0.0, max_value=0.5, step=0.05)))\n",
    "\n",
    "#     model.add(layers.Dense(1, activation='linear'))\n",
    "#     optimizer_choice = hp.Choice('optimizer', values=['rmsprop', 'nadam', 'adamax'])\n",
    "#     learning_rate = hp.Choice('learning_rate', values=[1.0, 1e-1, 1e-2, 1e-3])\n",
    "    \n",
    "#     if optimizer_choice == 'rmsprop':\n",
    "#         optimizer = optimizers.RMSprop(learning_rate=learning_rate)\n",
    "#     elif optimizer_choice == 'nadam':\n",
    "#         optimizer = optimizers.Nadam(learning_rate=learning_rate)\n",
    "#     else:\n",
    "#         optimizer = optimizers.Adamax(learning_rate=learning_rate)\n",
    "\n",
    "#     model.compile(optimizer=optimizer, loss='mse') # TODO RMSLE on parempi metriikka! huomenna se testiin!\n",
    "#     return model\n",
    "\n",
    "# # Callbacks määritelty\n",
    "# callbacks = [    \n",
    "#     ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=5, min_lr=1e-6, verbose=1),\n",
    "#     TerminateOnNaN()\n",
    "# ]\n",
    "\n",
    "# # Käytetään Hyperband-tuneria\n",
    "\n",
    "\n",
    "# kf = KFold(n_splits=5, random_state=42)\n",
    "# fold = 0\n",
    "# for train_index, val_index in kf.split(X_train):\n",
    "\n",
    "\n",
    "#     X_train_b, X_val_b = X_train[train_index], X_train[val_index]    \n",
    "#     y_train_b, y_val_b = y_train[train_index], y_train[val_index]\n",
    "#     y_train_b = tf.data.Dataset.from_tensor_slices(y_train_b)\n",
    "#     X_train_b = tf.data.Dataset.from_tensor_slices(X_train_b)\n",
    "#     train_dataset = tf.data.Dataset.zip((X_train_b, y_train_b)).batch(64)\n",
    "#     train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    \n",
    "#     tuner = Hyperband(\n",
    "#     build_model,\n",
    "#     objective='val_loss',\n",
    "#     max_epochs=300,\n",
    "#     factor=3,\n",
    "#     directory='NN_search',\n",
    "#     project_name=f'kt_hyperband_fold_{fold}',\n",
    "#     # overwrite=True, # Otetaan pois niin saan vanhat tulokset mukaan optimointiin\n",
    "#     hyperband_iterations=3\n",
    "#     )\n",
    "    \n",
    "#     tuner.search(train_dataset, validation_data=(X_val_b, y_val_b), callbacks=callbacks, verbose=2)\n",
    "\n",
    "#     best_model = tuner.get_best_models(num_models=1)[0]\n",
    "#     models_hyperband.append(best_model)\n",
    "#     best_hyperparameters_hyperband.append(tuner.get_best_hyperparameters(num_trials=1)[0])\n",
    "\n",
    "#     # Evaluoi malli\n",
    "#     predictions = best_model.predict(X_test)\n",
    "#     mse_scores.append(mean_squared_error(y_test, predictions))\n",
    "#     mae_scores.append(mean_absolute_error(y_test, predictions))\n",
    "#     r2_scores.append(r2_score(y_test, predictions))\n",
    "#     fold += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO lisää sns y ja predicted jakaumat x-y reunoille ja keskelle R2, MSE, MAE\n",
    "# search_time_end = time.time()\n",
    "# print(f\"Hyperband search took {search_time_end - search_time_start} seconds\")\n",
    "\n",
    "# # Tulosetaan kaikki tulokset alkuun\n",
    "# for idx, (mae, mse, r2) in enumerate(zip(mae_scores, mse_scores, r2_scores), start=1):\n",
    "#     print(f\"Model {idx} - MAE: {mae}, MSE: {mse}, R2: {r2}\")\n",
    "\n",
    "# # Käydään vielä eri mallit lävitse selvyyden vuoksi\n",
    "# for idx, model in enumerate(models_hyperband):\n",
    "\n",
    "#     predictions = model.predict(X_test)\n",
    "#     plt.figure(figsize=(20, 10)) \n",
    "#     plt.scatter(y_test, predictions, edgecolors=(0, 0, 0))\n",
    "#     plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "#     plt.xlabel('Measured')  \n",
    "#     plt.ylabel('Predicted') \n",
    "#     plt.title('Measured vs. Predicted Values')\n",
    "#     plt.show()\n",
    "\n",
    "#     print(f\"Fold {idx} - MSE: {mse_scores[idx]}, MAE: {mae_scores[idx]}, R2: {r2_scores[idx]}\")\n",
    "#     hp = best_hyperparameters_hyperband[idx]\n",
    "#     print(f\"Best hyperparameters for model {i+1}:\")\n",
    "#     for key in hp.values:\n",
    "#         print(f\"{key}: {hp.get(key)}\")\n",
    "#     print(\"-\" * 50)\n",
    "#     model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "# from keras.models import Model\n",
    "\n",
    "# # Kerätään ensin kaikkien mallien ominaisuusvektorit\n",
    "# X_train_features_list = []\n",
    "# X_test_features_list = []\n",
    "\n",
    "# for model in models_hyperband:\n",
    "#     feature_extractor = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "#     X_train_features = feature_extractor.predict(X_train)\n",
    "#     X_test_features = feature_extractor.predict(X_test)\n",
    "    \n",
    "#     X_train_features_list.append(X_train_features)\n",
    "#     X_test_features_list.append(X_test_features)\n",
    "\n",
    "# # Yhdistetään ominaisuusvektorit\n",
    "# X_train_combined = np.concatenate(X_train_features_list, axis=1)\n",
    "# X_test_combined = np.concatenate(X_test_features_list, axis=1)\n",
    "\n",
    "# xgb = xgboost.XGBRegressor(objective ='reg:squarederror')\n",
    "# param_space = {\n",
    "#     'n_estimators': np.arange(1, 500, 20),\n",
    "#     'max_depth': np.arange(2, 11),\n",
    "#     'learning_rate': [0.1, 0.01, 0.001],\n",
    "#     'subsample': [0.8, 0.9, 1.0],\n",
    "#     'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "#     'gamma': [0, 1, 5],\n",
    "#     'reg_alpha': [0, 0.1, 0.5],\n",
    "#     'reg_lambda': [1, 1.5, 2]\n",
    "# }\n",
    "\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     estimator=xgb,\n",
    "#     param_distributions=param_space,\n",
    "#     cv=5,\n",
    "#     n_jobs=-2,\n",
    "#     n_iter=100,\n",
    "#     verbose=2\n",
    "# )\n",
    "\n",
    "# start_time = time.time()\n",
    "# random_search.fit(X_train_combined, y_train)\n",
    "# best_model = random_search.best_estimator_\n",
    "# end_time = time.time()\n",
    "# elapsed_time = end_time - start_time\n",
    "\n",
    "# predictions = best_model.predict(X_test_combined)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse = mean_squared_error(y_test, predictions)\n",
    "# mae = mean_absolute_error(y_test, predictions)\n",
    "# r2 = r2_score(y_test, predictions)\n",
    "# print(f\"MAE: {mae}, MSE: {mse}, R2: {r2}\")\n",
    "# print(f\"Random search took {elapsed_time} seconds\")\n",
    "# print(f\"Feature shape: {X_train_combined.shape}\")\n",
    "\n",
    "# plt.figure(figsize=(20, 10)) \n",
    "# plt.scatter(y_test, predictions, edgecolors=(0, 0, 0))\n",
    "# plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "# plt.xlabel('Measured')  \n",
    "# plt.ylabel('Predicted') \n",
    "# plt.title('Measured vs. Predicted Values NN Hyperband features')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras_tuner import BayesianOptimization\n",
    "from keras import regularizers, Sequential, layers, optimizers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler \n",
    "from tensorflow import keras\n",
    "import time\n",
    "from keras import initializers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "search_time_start = time.time()\n",
    "models_bayes = []\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "r2_scores = []\n",
    "\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(X_train.shape[1],)))\n",
    "\n",
    "    # Luodaan kerroksia käyttäen Hyperband-optimoinnin hyperparametreja\n",
    "    for i in range(hp.Int('num_layers', 1, 4)):  # Vaihteluväli kerrosten määrälle\n",
    "        model.add(layers.Dense(\n",
    "            units=hp.Int(f'units_{i}', min_value=8, max_value=512, step=16),\n",
    "            activation=hp.Choice(f'activation_{i}', values=['relu', 'linear', 'selu', 'elu', 'sigmoid', 'tanh']),\n",
    "            kernel_regularizer=regularizers.l1_l2(\n",
    "                l1=hp.Float(f'l1_reg_{i}', min_value=1e-6, max_value=1e-1, sampling='log'),\n",
    "                l2=hp.Float(f'l2_reg_{i}', min_value=1e-6, max_value=1e-1, sampling='log')),\n",
    "            kernel_initializer=hp.Choice('initializer', values=['he_normal', 'glorot_uniform', 'lecun_normal', 'glorot_normal'])\n",
    "            )\n",
    "        )\n",
    "        model.add(layers.Dropout(hp.Float(f'dropout_{i}', min_value=0.0, max_value=0.5, step=0.05)))\n",
    "\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "    optimizer_choice = hp.Choice('optimizer', values=['rmsprop', 'nadam', 'adamax', 'adam'])\n",
    "    learning_rate = hp.Choice('learning_rate', values=[1.0, 1e-1, 1e-2, 1e-3, 1e-4])\n",
    "    \n",
    "    if optimizer_choice == 'rmsprop':\n",
    "        optimizer = optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'nadam':\n",
    "        optimizer = optimizers.Nadam(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'adam':\n",
    "        optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = optimizers.Adamax(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n",
    "\n",
    "# Initialize the Bayesian Optimization tuner\n",
    "\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr_callback = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.8,  # New learning rate = learning rate * factor\n",
    "    patience=5,  # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    min_lr=1e-6,  # Lower bound on the learning rate\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "terminate_on_nan = TerminateOnNaN()\n",
    "\n",
    "best_hyperparameters_bayes = []\n",
    "\n",
    "start_time = time.time()\n",
    "kf = KFold(n_splits=5)\n",
    "models_bayes = []\n",
    "round = 0\n",
    "\n",
    "fold = 0\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "\n",
    "    tuner = BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=420, \n",
    "    executions_per_trial=1,\n",
    "    directory='NN_search',\n",
    "    project_name=f'kt_bayesian_fold_{fold}',\n",
    "    # overwrite=True, # otetaan pois, niin saan vanhat tulokset mukaan\n",
    "    max_consecutive_failed_trials=10,\n",
    "    max_retries_per_trial = 0\n",
    ")\n",
    "    \n",
    "    X_train_b, X_val_b = X_train[train_index], X_train[val_index]    \n",
    "    y_train_b, y_val_b = y_train[train_index], y_train[val_index]\n",
    "    y_train_b = tf.data.Dataset.from_tensor_slices(y_train_b)\n",
    "    X_train_b = tf.data.Dataset.from_tensor_slices(X_train_b)\n",
    "    train_dataset = tf.data.Dataset.zip((X_train_b, y_train_b)).batch(64)\n",
    "    train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    tuner.search(\n",
    "        train_dataset,\n",
    "        epochs=300,\n",
    "        validation_data=(X_val_b, y_val_b),\n",
    "        callbacks=[early_stopping_callback, reduce_lr_callback, terminate_on_nan],\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "  \n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "    models_bayes.append(best_model)\n",
    "    best_hyperparameters_bayes.append(tuner.get_best_hyperparameters(num_trials=1)[0])\n",
    "\n",
    "    predictions = best_model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    \n",
    "    # Tallenna suorituskykymetriikat\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    r2_scores.append(r2)\n",
    "    fold += 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "search_time_end = time.time()\n",
    "print(f\"Bayesian search took {search_time_end - search_time_start} seconds\")\n",
    "\n",
    "# Tulosetaan kaikki tulokset alkuun\n",
    "for idx, (mae, mse, r2) in enumerate(zip(mae_scores, mse_scores, r2_scores), start=1):\n",
    "    print(f\"Model {idx} - MAE: {mae}, MSE: {mse}, R2: {r2}\")\n",
    "\n",
    "# Käydään vielä eri mallit lävitse selvyyden vuoksi\n",
    "for idx, model in enumerate(models_bayes):\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "    plt.figure(figsize=(20, 10)) \n",
    "    plt.scatter(y_test, predictions, edgecolors=(0, 0, 0))\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "    plt.xlabel('Measured')  \n",
    "    plt.ylabel('Predicted') \n",
    "    plt.title('Measured vs. Predicted Values')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Fold {idx} - MSE: {mse_scores[idx]}, MAE: {mae_scores[idx]}, R2: {r2_scores[idx]}\")\n",
    "    hp = best_hyperparameters_bayes[idx]\n",
    "    print(f\"Best hyperparameters for model {i+1}:\")\n",
    "    for key in hp.values:\n",
    "        print(f\"{key}: {hp.get(key)}\")\n",
    "    print(\"-\" * 50)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.models import Model\n",
    "\n",
    "# Kerätään ensin kaikkien mallien ominaisuusvektorit\n",
    "X_train_features_list = []\n",
    "X_test_features_list = []\n",
    "\n",
    "for model in models_bayes:\n",
    "    feature_extractor = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "    X_train_features = feature_extractor.predict(X_train)\n",
    "    X_test_features = feature_extractor.predict(X_test)\n",
    "    \n",
    "    X_train_features_list.append(X_train_features)\n",
    "    X_test_features_list.append(X_test_features)\n",
    "\n",
    "# Yhdistetään ominaisuusvektorit\n",
    "X_train_combined = np.concatenate(X_train_features_list, axis=1)\n",
    "X_test_combined = np.concatenate(X_test_features_list, axis=1)\n",
    "\n",
    "xgb = xgboost.XGBRegressor(objective ='reg:squarederror')\n",
    "param_space = {\n",
    "    'n_estimators': np.arange(1, 500, 20),\n",
    "    'max_depth': np.arange(2, 11),\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 1, 5],\n",
    "    'reg_alpha': [0, 0.1, 0.5],\n",
    "    'reg_lambda': [1, 1.5, 2]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=param_space,\n",
    "    cv=5,\n",
    "    n_jobs=-2,\n",
    "    n_iter=100,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "random_search.fit(X_train_combined, y_train)\n",
    "best_model = random_search.best_estimator_\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "predictions = best_model.predict(X_test_combined)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f\"MAE: {mae}, MSE: {mse}, R2: {r2}\")\n",
    "print(f\"Random search took {elapsed_time} seconds\")\n",
    "print(f\"Feature shape: {X_train_combined.shape}\")\n",
    "\n",
    "plt.figure(figsize=(20, 10)) \n",
    "plt.scatter(y_test, predictions, edgecolors=(0, 0, 0))\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "plt.xlabel('Measured')  \n",
    "plt.ylabel('Predicted') \n",
    "plt.title('Measured vs. Predicted Values NN Bayesian features')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "joda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
