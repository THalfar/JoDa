{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_train_filtered = pd.read_pickle('./data/df_train_filtered.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_train_filtered.drop('Hinta', axis=1)\n",
    "y = df_train_filtered['Hinta']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=df_train_filtered['Kaupunginosa'], random_state=42)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler \n",
    "\n",
    "# Skaalataan numeeriset muuttujat\n",
    "robust_scaler = RobustScaler()\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "df_train_NN = df_train_filtered.copy()\n",
    "df_train_NN[['Pituusaste', 'Leveysaste']] = minmax_scaler.fit_transform(df_train_NN[['Pituusaste', 'Leveysaste']])\n",
    "df_train_NN['Rv'] = minmax_scaler.fit_transform(df_train_NN[['Rv']])\n",
    "df_train_NN['m2'] = minmax_scaler.fit_transform(df_train_NN[['m2']])\n",
    "\n",
    "# One hot koodataan kategoriset muuttujat\n",
    "df_hot = pd.get_dummies(df_train_NN['Kaupunginosa'], prefix='Kaupunginosa').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN['kerros'], prefix='kerros').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN['max_kerros'], prefix='max_kerros').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN['Kunto'], prefix='Kunto').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN['Hissi'], prefix='Hissi').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN['Asunnon tyyppi'], prefix='Asunnon tyyppi').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN[\"Talot.\"], prefix='Talot.').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "\n",
    "df_train_NN.drop(['Kaupunginosa', 'kerros', 'max_kerros', 'Kunto', 'Hissi', 'Asunnon tyyppi', \"Talot.\"], axis=1, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muodostetaan X ja y sekä jaetaan data harjoitus- ja testijoukkoihin\n",
    "\n",
    "X = df_train_NN.drop('Hinta', axis=1)\n",
    "y = df_train_NN['Hinta']\n",
    "\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy().astype('float32')\n",
    "\n",
    "X_train_NN, X_test_NN, y_train_NN, y_test_NN = train_test_split(X, y, test_size=0.1, random_state=42, stratify=df_train_filtered['Kaupunginosa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from keras import regularizers, layers, optimizers, initializers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "import time \n",
    "from optuna.integration import TFKerasPruningCallback\n",
    "import os \n",
    "import pickle \n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "# Haun nimi\n",
    "study_name = 'rmsle5_layers_0328'\n",
    "# Montako osittelua käytettiin\n",
    "folds = 5\n",
    "# Montako epochia kullekin osittelulle\n",
    "epochs_search = 50\n",
    "# Montako satunnaista hakua kieroksella\n",
    "num_random = 100\n",
    "# Montako TPE hakua kieroksella\n",
    "num_tpe = 0\n",
    "\n",
    "# Aika sekuntteina jota hakuun käytetän\n",
    "max_search_time = 36000\n",
    "# Neuroneiden maksimimäärä \n",
    "max_units = 512\n",
    "\n",
    "def rmsle_loss(y_true, y_pred):\n",
    "    # Asetetaan suuri rangaistusarvo, jos y_pred sisältää arvon nolla tai alle\n",
    "    penalty = tf.constant(1e5, dtype=tf.float32)\n",
    "    \n",
    "    # Maski, joka on tosi, kun y_pred on > 0\n",
    "    valid_mask = tf.math.greater(y_pred, 0.0)\n",
    "    \n",
    "    # Käytä maskia valitsemaan joko oikea RMSLE laskenta tai suuri rangaistus\n",
    "    safe_y_pred = tf.where(valid_mask, y_pred, penalty)\n",
    "    \n",
    "    # Laske RMSLE vain, jos y_pred on suurempi kuin 0, muuten palauta rangaistus\n",
    "    rmsle = tf.sqrt(tf.reduce_mean(tf.square(tf.math.log1p(safe_y_pred) - tf.math.log1p(y_true))))\n",
    "    \n",
    "    # Palauta suuri rangaistus, jos y_pred sisälsi nollan tai negatiivisen arvon\n",
    "    return tf.where(tf.reduce_any(~valid_mask), penalty, rmsle)\n",
    "\n",
    "\n",
    "def create_model(trial):\n",
    "        \n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=(X_train_NN.shape[1],)))\n",
    "    \n",
    "    num_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "    max_units = 128\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        # Ehdota neuronien määrää, joka on enintään max_units\n",
    "        num_units = trial.suggest_int(f'n_units_{i}', 4, max_units)\n",
    "        dropout_rate = trial.suggest_float(f'dropout_{i}', 0.0, 0.5)\n",
    "        kernel_regularizer = regularizers.l1_l2(\n",
    "            l1= trial.suggest_float(f'l1_reg_{i}', 1e-6, 1, log=True),\n",
    "            l2= trial.suggest_float(f'l2_reg_{i}', 1e-6, 1, log=True)\n",
    "        )\n",
    "        activation = trial.suggest_categorical(f'activation_{i}', ['relu', 'elu', 'LeakyReLU', 'tanh'])\n",
    "        \n",
    "        model.add(keras.layers.Dense(num_units, activation=activation, kernel_regularizer=kernel_regularizer))\n",
    "        model.add(keras.layers.Dropout(rate=dropout_rate))\n",
    "        \n",
    "        # Päivitä max_units varmistaaksesi, että seuraavan kerroksen neuronien määrä ei ole suurempi\n",
    "        max_units = min(max_units, num_units)  \n",
    "    \n",
    "    model.add(keras.layers.Dense(1, activation='linear')) \n",
    "    \n",
    "    # Optimisaattorin ja oppimisnopeuden valinta\n",
    "    optimizer_options = ['adam', 'rmsprop', 'Nadam', 'adamax', 'Adagrad', 'Adadelta']\n",
    "    optimizer_selected = trial.suggest_categorical('optimizer', optimizer_options)\n",
    "    \n",
    "    \n",
    "    if optimizer_selected == 'adam':\n",
    "        optimizer = optimizers.Adam()\n",
    "    elif optimizer_selected == 'rmsprop':\n",
    "        optimizer = optimizers.RMSprop()\n",
    "    elif optimizer_selected == 'Nadam':\n",
    "        optimizer = optimizers.Nadam()\n",
    "    elif optimizer_selected == 'Adagrad':\n",
    "        optimizer = optimizers.Adagrad()\n",
    "    elif optimizer_selected == 'Adadelta':\n",
    "        optimizer = optimizers.Adadelta()\n",
    "    else:\n",
    "        optimizer = optimizers.Adamax()\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=rmsle_loss, metrics=['mse', 'mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    model = create_model(trial)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 128, log=True)    \n",
    "    callbacks = [TFKerasPruningCallback(trial, 'val_loss'),\n",
    "                 ReduceLROnPlateau('val_loss', patience=5, factor=0.5), \n",
    "                 TerminateOnNaN()]\n",
    "\n",
    "    history = model.fit(X_train_b, y_train_b, epochs=epochs_search, validation_data=(X_val_b, y_val_b) ,batch_size=batch_size, verbose=0, callbacks=callbacks)\n",
    "    val_loss = np.min(history.history['val_loss'])\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "total_time_start = time.time()  \n",
    "search_time_start = time.time() \n",
    "num_completed_trials = 0\n",
    "search_rounds = 0\n",
    "time_taken = 0\n",
    "while time_taken < max_search_time:\n",
    "        \n",
    "    fold = 0 \n",
    "    kf = KFold(n_splits=folds)\n",
    "    \n",
    "    time_fold_start = time.time()    \n",
    "    for train_index, val_index in kf.split(X_train_NN):\n",
    "\n",
    "        print('-------------------')\n",
    "        print(f\"Starting fold {fold} search...\")\n",
    "        X_train_b, X_val_b = X_train_NN[train_index], X_train_NN[val_index]    \n",
    "        y_train_b, y_val_b = y_train_NN[train_index], y_train_NN[val_index]\n",
    "\n",
    "        fold_name = f'{study_name}_{fold}'\n",
    "       \n",
    "        study = optuna.create_study(direction='minimize',\n",
    "                                    pruner=optuna.pruners.HyperbandPruner(min_resource=5),\n",
    "                                    study_name=fold_name,\n",
    "                                    storage=f'sqlite:///tampere_reg.db',\n",
    "                                    load_if_exists=True                                 \n",
    "                                    )\n",
    "        \n",
    "    \n",
    "        fold_time = time.time()    \n",
    "\n",
    "        fold_random = time.time()\n",
    "        # # study.sampler = optuna.samplers.RandomSampler()\n",
    "        study.sampler = optuna.samplers.QMCSampler(warn_independent_sampling = False) # TODO tämä testiin, vaikutti paljon paremmalta kuin random \n",
    "        print(f'Random search for fold {fold}...')\n",
    "        study.optimize(objective, n_trials=num_random)\n",
    "        print(f'Time taken for random search: {str(timedelta(seconds=(time.time() - fold_random)))}')\n",
    "\n",
    "        # fold_tpe = time.time()  \n",
    "        # study.sampler = optuna.samplers.TPESampler(n_startup_trials=0)\n",
    "        # print(f'TPE search for fold {fold}...')\n",
    "        # study.optimize(objective, n_trials=num_tpe)\n",
    "        # print(f'Time taken for TPE search: {str(timedelta(seconds=(time.time() - fold_tpe)))}')\n",
    "\n",
    "        num_completed_trials += num_random + num_tpe\n",
    "        print('-------------------')\n",
    "        print(f'Finished fold {fold} search.')\n",
    "        print(f\"Time taken for this fold: {str(timedelta(seconds=(time.time() - fold_time)))}\")                \n",
    "        print(f'Fold {fold} best value so far: {study.best_value}')\n",
    "        print(f'Mean time for one trial this fold: {str(timedelta(seconds=(time.time() - fold_time) / (num_random + num_tpe)))}')\n",
    "\n",
    "        fold += 1\n",
    "    search_rounds += 1\n",
    "    \n",
    "    time_taken = time.time() - search_time_start\n",
    "    \n",
    "    print(f'\\n# Completed search round: {search_rounds} #')\n",
    "    print(f'Time taken for all folds this round: {str(timedelta(seconds=(time.time() - time_fold_start)))}')\n",
    "    print(f'Total time taken for search: {str(timedelta(seconds=(time.time() - search_time_start)))}')\n",
    "    print(f'Made trials this far: {num_completed_trials}')\n",
    "    print(f\"Current mean time for one trial: {str(timedelta(seconds=(time.time() - search_time_start) / num_completed_trials))}\\n\")\n",
    "\n",
    "####\n",
    "\n",
    "####\n",
    "\n",
    "# num_tpe = 42\n",
    "# num_random = 0\n",
    "# max_search_time = 25200\n",
    "\n",
    "# search_time_start = time.time()\n",
    "    \n",
    "# while time_taken < max_search_time:\n",
    "        \n",
    "#     fold = 0 \n",
    "#     kf = KFold(n_splits=folds)\n",
    "    \n",
    "#     time_fold_start = time.time()    \n",
    "#     for train_index, val_index in kf.split(X_train_NN):\n",
    "\n",
    "#         print('-------------------')\n",
    "#         print(f\"Starting fold {fold} search...\")\n",
    "#         X_train_b, X_val_b = X_train_NN[train_index], X_train_NN[val_index]    \n",
    "#         y_train_b, y_val_b = y_train_NN[train_index], y_train_NN[val_index]\n",
    "\n",
    "#         fold_name = f'{study_name}_{fold}'\n",
    "       \n",
    "#         study = optuna.create_study(direction='minimize',\n",
    "#                                     pruner=optuna.pruners.HyperbandPruner(min_resource=20),\n",
    "#                                     study_name=fold_name,\n",
    "#                                     storage=f'sqlite:///tampere_reg.db',\n",
    "#                                     load_if_exists=True                                 \n",
    "#                                     )\n",
    "\n",
    "#         fold_time = time.time()    \n",
    "\n",
    "#         fold_tpe = time.time()  \n",
    "#         study.sampler = optuna.samplers.TPESampler(n_startup_trials=0)\n",
    "#         print(f'TPE search for fold {fold}...')\n",
    "#         study.optimize(objective, n_trials=num_tpe)\n",
    "#         print(f'Time taken for TPE search: {str(timedelta(seconds=(time.time() - fold_tpe)))}')\n",
    "\n",
    "#         num_completed_trials += num_random + num_tpe\n",
    "#         print('-------------------')\n",
    "#         print(f'Finished fold {fold} search.')\n",
    "#         print(f\"Time taken for this fold: {str(timedelta(seconds=(time.time() - fold_time)))}\")                \n",
    "#         print(f'Fold {fold} best value so far: {study.best_value}')\n",
    "#         print(f'Mean time for one trial this fold: {str(timedelta(seconds=(time.time() - fold_time) / (num_random + num_tpe)))}')\n",
    "\n",
    "#         fold += 1\n",
    "#     search_rounds += 1\n",
    "    \n",
    "#     time_taken = time.time() - search_time_start\n",
    "    \n",
    "#     print(f'\\n# Completed search round: {search_rounds} #')\n",
    "#     print(f'Time taken for all folds this round: {str(timedelta(seconds=(time.time() - time_fold_start)))}')\n",
    "#     print(f'Total time taken for search: {str(timedelta(seconds=(time.time() - search_time_start)))}')\n",
    "#     print(f'Made trials this far: {num_completed_trials}')\n",
    "#     print(f\"Current mean time for one trial: {str(timedelta(seconds=(time.time() - search_time_start) / num_completed_trials))}\\n\")\n",
    "\n",
    "# ###\n",
    "\n",
    "# print('='*20)    \n",
    "# print(f'Finished search.')    \n",
    "# print(f'Total time taken for all folds: {str(timedelta(seconds=(time.time() - search_time_start)))}')\n",
    "# print(f'Made {num_completed_trials} trials in total.')\n",
    "# print(f\"Mean time for one trial: {str(timedelta(seconds=(time.time() - search_time_start) / num_completed_trials))}\")\n",
    "# print('='*20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import optuna\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from keras import regularizers, layers, optimizers, initializers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "import time \n",
    "import os \n",
    "\n",
    "def rmsle_score(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(np.log1p(y_true+1), np.log1p(y_pred+1)))\n",
    "\n",
    "folds = 5\n",
    "# Montako epochia kullekin parhaalle sovitetaan malli\n",
    "epochs_best_fit = 500\n",
    "# Montako paras otetaan mukaan osittelusta\n",
    "num_best = 2\n",
    "# Montako kertaa kullekin parhaalle sovitetaan malli\n",
    "num_best_fits = 1\n",
    "\n",
    "best_optuna_models = []\n",
    "best_val_scores = []\n",
    "best_optuna_trials = [] \n",
    "\n",
    "kf = KFold(n_splits=folds)\n",
    "fold_num = 0\n",
    "fitting_search_start = time.time()\n",
    "\n",
    "for train_index, val_index in kf.split(X_train_NN):\n",
    "\n",
    "    best_fitting_time = time.time()\n",
    "    print(f\"Fold {fold_num} Best best trial fitting...\")\n",
    "\n",
    "    X_train_b, X_val_b = X_train_NN[train_index], X_train_NN[val_index]    \n",
    "    y_train_b, y_val_b = y_train_NN[train_index], y_train_NN[val_index]\n",
    "    \n",
    "    fold_name = f'{study_name}_{fold_num}'\n",
    "       \n",
    "    study = optuna.create_study(                                \n",
    "                                study_name=fold_name,\n",
    "                                storage=f'sqlite:///tampere_reg.db',\n",
    "                                load_if_exists=True\n",
    "                                )\n",
    "\n",
    "    valid_trials = [trial for trial in study.trials if trial.value is not None]\n",
    "    sorted_trials = sorted(valid_trials, key=lambda trial: trial.value)\n",
    "    best_trials = sorted_trials[:num_best]\n",
    "    best_val = np.inf\n",
    "    best_model = None\n",
    "\n",
    "    print('='*30)\n",
    "    print(f'Fitting best trials for fold {fold_num}...')\n",
    "    fitting_fold_best_start = time.time()\n",
    "    \n",
    "    for trial in best_trials:\n",
    "\n",
    "        for fit_num in range(num_best_fits):\n",
    "            \n",
    "            print('-'*30)\n",
    "            print(f\"Trial ID: {trial.number}, Value: {trial.value}, fit number: {fit_num}\")\n",
    "\n",
    "            checkpoint_filepath = f'./NN_search/optuna_search_checkpoint.h5'\n",
    "            model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=checkpoint_filepath,\n",
    "                save_weights_only=True,\n",
    "                monitor='val_loss',\n",
    "                mode='min',\n",
    "                save_best_only=True)\n",
    "\n",
    "            best_callback = [model_checkpoint_callback,                  \n",
    "                            ReduceLROnPlateau('val_loss', patience=10, factor=0.8), \n",
    "                            TerminateOnNaN(),\n",
    "                            EarlyStopping(monitor='val_loss', patience=100, verbose=1)\n",
    "                        ]\n",
    "\n",
    "\n",
    "            model = create_model(trial)\n",
    "            model.fit(X_train_b, y_train_b, epochs=epochs_best_fit, validation_data=(X_val_b, y_val_b), batch_size=trial.params['batch_size'], verbose=0, callbacks=best_callback)\n",
    "            model.load_weights(checkpoint_filepath)\n",
    "\n",
    "            predictions = model.predict(X_val_b, verbose=0)\n",
    "            mse = mean_squared_error(y_val_b, predictions)\n",
    "            mae = mean_absolute_error(y_val_b, predictions)\n",
    "            r2 = r2_score(y_val_b, predictions)\n",
    "            rmsle = rmsle_score(y_val_b, predictions)\n",
    "\n",
    "                        \n",
    "            print(f'MSE:{mse:.5f}\\nMAE:{mae:.5f}\\nRMSLE:{rmsle:.5f}\\nR2:{r2:.5f}')\n",
    "\n",
    "            if rmsle < best_val:\n",
    "                best_model = model\n",
    "                best_val = rmsle\n",
    "                best_trial_num = trial.number\n",
    "                best_trial = trial\n",
    "                print(f'*** New best model for fold {fold_num} is Trial {best_trial_num} with RMSLE {best_val} ***')\n",
    "                print(f'Best trial hyperparameters: {trial.params}')\n",
    "    \n",
    "    if best_model is not None:\n",
    "\n",
    "        best_optuna_models.append(best_model)\n",
    "        best_val_scores.append(best_val)\n",
    "        best_optuna_trials.append(best_trial)\n",
    "        print('*'*40)\n",
    "        print(f\"Best model for fold {fold_num} RMSLE: {best_val}\\nTrial number: {best_trial_num}\\nHyperparameters: {best_trial.params}\")\n",
    "        print(f\"Time taken for best fitting in fold {fold_num}: {str(timedelta(seconds=(time.time() - best_fitting_time)) )}\")\n",
    "        print('*'*40)\n",
    "\n",
    "    fold_num += 1\n",
    "\n",
    "print('*'*40)\n",
    "print(f'Best models fitting time total:', str(timedelta(seconds=(time.time() - fitting_search_start))))\n",
    "print(f\"Total time taken for search and fitting best models: {str(timedelta(seconds=(time.time() - total_time_start)))}\")\n",
    "print('*'*40)   \n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "for i, (model, score) in enumerate(zip(best_optuna_models, best_val_scores)):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    directory = f\"./NN_search/{study_name}_foldmodel{i}_score_{score:.4f}_{timestamp}.h5\"\n",
    "    print(f\"Saving model {i} with score {score:.4f} to {directory}\")\n",
    "    model.save(directory)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "\n",
    "# Oletetaan, että rmsle_score ja create_model funktiot ovat määritelty\n",
    "\n",
    "folds = 5\n",
    "epochs_best_fit = 500\n",
    "\n",
    "# Ladataan kaikki studyt ja etsitään globaalisti paras trial\n",
    "best_global_val = float('inf')\n",
    "best_global_trial = None\n",
    "best_optuna_models_global = []\n",
    "\n",
    "for fold_num in range(folds):\n",
    "    \n",
    "     \n",
    "    fold_name = f'{study_name}_{fold_num}'    \n",
    "    study = optuna.create_study(                                \n",
    "                                study_name=fold_name,\n",
    "                                storage=f'sqlite:///tampere_reg.db',\n",
    "                                load_if_exists=True\n",
    "                                )\n",
    "    valid_trials = [trial for trial in study.trials if trial.value is not None]\n",
    "    sorted_trials = sorted(valid_trials, key=lambda trial: trial.value)\n",
    "    best_trial = sorted_trials[0].value\n",
    "\n",
    "    if best_trial < best_global_val:\n",
    "        best_global_val = best_trial\n",
    "        best_global_trial = sorted_trials[0]\n",
    "        best_fold = fold_num\n",
    "        print(f'New best global trial value: {best_global_val:.4f} found in fold {best_fold}')\n",
    "        \n",
    "print(f'Best global trial value: {best_global_val:.4f} tahat found in fold {best_fold}')\n",
    "\n",
    "# Nyt meillä on paras trial, jota käytetään kaikkien foldien kouluttamiseen\n",
    "kf = KFold(n_splits=folds)\n",
    "fold_num = 0\n",
    "\n",
    "for train_index, val_index in kf.split(X_train_NN):\n",
    "    print(f\"Koulutetaan fold {fold_num} käyttäen parasta globaalia trialia...\")\n",
    "    \n",
    "    X_train_b, X_val_b = X_train_NN[train_index], X_train_NN[val_index]\n",
    "    y_train_b, y_val_b = y_train_NN[train_index], y_train_NN[val_index]\n",
    "\n",
    "    # Luodaan malli parhaan trialin parametreilla\n",
    "    model = create_model(best_global_trial)\n",
    "\n",
    "    checkpoint_filepath = f'./NN_search/optuna_search_checkpoint.h5'\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        save_weights_only=True,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_best_only=True)\n",
    "\n",
    "\n",
    "    best_callback = [model_checkpoint_callback,                  \n",
    "                        ReduceLROnPlateau('val_loss', patience=10, factor=0.8), \n",
    "                        TerminateOnNaN(),\n",
    "                        EarlyStopping(monitor='val_loss', patience=100, verbose=1)\n",
    "                    ]\n",
    "    \n",
    "    # Koulutetaan malli\n",
    "    model.fit(X_train_b, y_train_b, epochs=epochs_best_fit, validation_data=(X_val_b, y_val_b), batch_size=best_global_trial.params['batch_size'], verbose=0, callbacks=best_callback)\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    \n",
    "    # Tarkistetaan mallin suorituskykyä (tämä osa voi vaatia mukauttamista projektisi tarpeisiin)\n",
    "    predictions = model.predict(X_val_b)\n",
    "    rmsle = rmsle_score(y_val_b, predictions)\n",
    "    print(f\"Fold {fold_num} RMSLE: {rmsle}\")\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    directory = f\"./NN_search/{study_name}_best_foldmodel{fold_num}_score_{rmsle:.4f}_{timestamp}.h5\"\n",
    "    print(f\"Saving model {fold_num} with score {rmsle:.4f} to {directory}\")\n",
    "    model.save(directory)\n",
    "    best_optuna_models_global.append(model)\n",
    "    fold_num += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, model in enumerate(best_optuna_models_global):\n",
    "    print(f\"\\nModel {idx} Summary:\")\n",
    "    # model.summary()\n",
    "    \n",
    "    # Testaa mallia testidatalla\n",
    "    predictions = model.predict(X_test_NN, verbose = 0)\n",
    "    mse = mean_squared_error(y_test_NN, predictions)\n",
    "    mae = mean_absolute_error(y_test_NN, predictions)\n",
    "    r2 = r2_score(y_test_NN, predictions)\n",
    "    rmsle = rmsle_score(y_test_NN, predictions)\n",
    "    \n",
    "    print(f\"\\nModel {idx} Performance on Test Data:\")\n",
    "    print(f\"MSE: {mse:.3f}\")\n",
    "    print(f\"MAE: {mae:.3f}\")\n",
    "    print(f\"R2: {r2:.3f}\")\n",
    "    print(f\"RMSLE: {rmsle:.3f}\")\n",
    "    print(\"*\"*40)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Models with best global trial fitted')\n",
    "for idx, model in enumerate(best_optuna_models):\n",
    "    print(f\"\\nModel {idx} Summary:\")\n",
    "    # model.summary()\n",
    "    \n",
    "    # Testaa mallia testidatalla\n",
    "    predictions = model.predict(X_test_NN, verbose = 0)\n",
    "    mse = mean_squared_error(y_test_NN, predictions)\n",
    "    mae = mean_absolute_error(y_test_NN, predictions)\n",
    "    r2 = r2_score(y_test_NN, predictions)\n",
    "    rmsle = rmsle_score(y_test_NN, predictions)\n",
    "    \n",
    "    print(f\"\\nModel {idx} Performance on Test Data:\")\n",
    "    print(f\"MSE: {mse:.3f}\")\n",
    "    print(f\"MAE: {mae:.3f}\")\n",
    "    print(f\"R2: {r2:.3f}\")\n",
    "    print(f\"RMSLE: {rmsle:.3f}\")\n",
    "    print(\"*\"*40)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model for fold 0 from ./NN_search/rmsle5_random_2503_foldmodel0_score_0.1721_20240327-105334.h5 with score 0.1721\n",
      "Loaded best model for fold 1 from ./NN_search/rmsle5_random_2503_foldmodel1_score_0.1802_20240326-111144.h5 with score 0.1802\n",
      "Loaded best model for fold 2 from ./NN_search/rmsle5_random_2503_foldmodel2_score_0.1541_20240328-123253.h5 with score 0.1541\n",
      "Loaded best model for fold 3 from ./NN_search/rmsle5_random_2503_best_foldmodel3_score_0.1872_20240327-113608.h5 with score 0.1872\n",
      "Loaded best model for fold 4 from ./NN_search/rmsle5_random_2503_best_foldmodel4_score_0.1813_20240327-112131.h5 with score 0.1813\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "Model train feature shape: (972, 12)\n",
      "Model test feature shape: (108, 12)\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "Model train feature shape: (972, 6)\n",
      "Model test feature shape: (108, 6)\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "Model train feature shape: (972, 30)\n",
      "Model test feature shape: (108, 30)\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "Model train feature shape: (972, 21)\n",
      "Model test feature shape: (108, 21)\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "Model train feature shape: (972, 21)\n",
      "Model test feature shape: (108, 21)\n",
      "Train combined feature shape: (972, 21)\n",
      "Test combined feature shape: (108, 21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-29 12:44:49,540] A new study created in memory with name: no-name-adbd413c-8052-4113-b87f-40116ea300f7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sampling 200 trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_245790/1394335201.py:189: ExperimentalWarning: QMCSampler is experimental (supported from v3.0.0). The interface can change in the future.\n",
      "  study.sampler = optuna.samplers.QMCSampler(warn_independent_sampling = False)\n",
      "[I 2024-03-29 12:44:49,699] Trial 0 finished with value: 0.42399453142986465 and parameters: {'booster': 'gbtree', 'max_depth': 1, 'lambda': 0.1965537529748997, 'alpha': 0.0002756390047351662, 'learning_rate': 0.012434334810298186, 'subsample': 0.42955437684862663, 'colsample_bytree': 0.5020332361851008, 'gamma': 16.50014469665287, 'num_boost_round': 4, 'selector': 'None', 'Num selected features': 10}. Best is trial 0 with value: 0.42399453142986465.\n",
      "[I 2024-03-29 12:44:50,109] Trial 1 finished with value: 0.4321338957241199 and parameters: {'booster': 'gbtree', 'max_depth': 1, 'lambda': 1.0000000000000004e-06, 'alpha': 1.0000000000000004e-06, 'learning_rate': 0.01, 'subsample': 0.2, 'colsample_bytree': 0.2, 'gamma': 0.0, 'num_boost_round': 1, 'selector': 'mutual_info_regression', 'Num selected features': 1}. Best is trial 0 with value: 0.42399453142986465.\n",
      "[I 2024-03-29 12:44:50,272] Trial 2 finished with value: 0.1567520765081343 and parameters: {'booster': 'gbtree', 'max_depth': 5, 'lambda': 0.0010000000000000002, 'alpha': 0.0010000000000000002, 'learning_rate': 0.155, 'subsample': 0.6000000000000001, 'colsample_bytree': 0.6000000000000001, 'gamma': 10.0, 'num_boost_round': 11, 'selector': 'f_regression', 'Num selected features': 8}. Best is trial 2 with value: 0.1567520765081343.\n",
      "[I 2024-03-29 12:44:51,489] Trial 3 finished with value: 0.192516265631315 and parameters: {'booster': 'dart', 'max_depth': 8, 'lambda': 3.16227766016838e-05, 'alpha': 3.16227766016838e-05, 'learning_rate': 0.08249999999999999, 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 5.0, 'num_boost_round': 15, 'selector': 'None', 'Num selected features': 31}. Best is trial 2 with value: 0.1567520765081343.\n",
      "[I 2024-03-29 12:44:51,635] Trial 4 finished with value: 0.19018030479749828 and parameters: {'booster': 'dart', 'max_depth': 3, 'lambda': 0.0316227766016838, 'alpha': 0.0316227766016838, 'learning_rate': 0.22749999999999998, 'subsample': 0.4, 'colsample_bytree': 0.4, 'gamma': 15.0, 'num_boost_round': 5, 'selector': 'f_regression', 'Num selected features': 2}. Best is trial 2 with value: 0.1567520765081343.\n",
      "[I 2024-03-29 12:44:52,358] Trial 5 finished with value: 0.15834223598207833 and parameters: {'booster': 'gbtree', 'max_depth': 4, 'lambda': 0.00017782794100389232, 'alpha': 0.005623413251903492, 'learning_rate': 0.26375, 'subsample': 0.5, 'colsample_bytree': 0.30000000000000004, 'gamma': 7.5, 'num_boost_round': 18, 'selector': 'mutual_info_regression', 'Num selected features': 61}. Best is trial 2 with value: 0.1567520765081343.\n",
      "[I 2024-03-29 12:44:52,555] Trial 6 finished with value: 0.21553578174701862 and parameters: {'booster': 'gbtree', 'max_depth': 9, 'lambda': 0.1778279410038923, 'alpha': 5.623413251903493e-06, 'learning_rate': 0.11874999999999998, 'subsample': 0.9000000000000001, 'colsample_bytree': 0.7, 'gamma': 17.5, 'num_boost_round': 8, 'selector': 'f_regression', 'Num selected features': 4}. Best is trial 2 with value: 0.1567520765081343.\n",
      "[I 2024-03-29 12:44:52,981] Trial 7 finished with value: 0.2660415007198875 and parameters: {'booster': 'gbtree', 'max_depth': 7, 'lambda': 5.623413251903493e-06, 'alpha': 0.1778279410038923, 'learning_rate': 0.19125, 'subsample': 0.7, 'colsample_bytree': 0.9000000000000001, 'gamma': 2.5, 'num_boost_round': 3, 'selector': 'mutual_info_regression', 'Num selected features': 1}. Best is trial 2 with value: 0.1567520765081343.\n",
      "[I 2024-03-29 12:44:53,229] Trial 8 finished with value: 0.28962339050538366 and parameters: {'booster': 'dart', 'max_depth': 2, 'lambda': 0.005623413251903492, 'alpha': 0.00017782794100389232, 'learning_rate': 0.04625, 'subsample': 0.30000000000000004, 'colsample_bytree': 0.5, 'gamma': 12.5, 'num_boost_round': 13, 'selector': 'f_regression', 'Num selected features': 15}. Best is trial 2 with value: 0.1567520765081343.\n",
      "[I 2024-03-29 12:44:53,705] Trial 9 finished with value: 0.16843328143023722 and parameters: {'booster': 'dart', 'max_depth': 2, 'lambda': 7.498942093324561e-05, 'alpha': 0.4216965034285823, 'learning_rate': 0.136875, 'subsample': 0.65, 'colsample_bytree': 0.45, 'gamma': 8.75, 'num_boost_round': 19, 'selector': 'f_regression', 'Num selected features': 86}. Best is trial 2 with value: 0.1567520765081343.\n",
      "[I 2024-03-29 12:44:53,954] Trial 10 finished with value: 0.14662204664893166 and parameters: {'booster': 'gbtree', 'max_depth': 7, 'lambda': 0.07498942093324559, 'alpha': 0.00042169650342858235, 'learning_rate': 0.281875, 'subsample': 0.25, 'colsample_bytree': 0.8500000000000001, 'gamma': 18.75, 'num_boost_round': 9, 'selector': 'f_regression', 'Num selected features': 6}. Best is trial 10 with value: 0.14662204664893166.\n",
      "[I 2024-03-29 12:44:54,107] Trial 11 finished with value: 0.347394940001027 and parameters: {'booster': 'dart', 'max_depth': 10, 'lambda': 2.3713737056616565e-06, 'alpha': 0.013335214321633242, 'learning_rate': 0.06437499999999999, 'subsample': 0.45, 'colsample_bytree': 0.65, 'gamma': 3.75, 'num_boost_round': 4, 'selector': 'f_regression', 'Num selected features': 1}. Best is trial 10 with value: 0.14662204664893166.\n",
      "[I 2024-03-29 12:44:54,750] Trial 12 finished with value: 0.14816437516407682 and parameters: {'booster': 'gbtree', 'max_depth': 5, 'lambda': 0.002371373705661656, 'alpha': 1.3335214321633246e-05, 'learning_rate': 0.209375, 'subsample': 0.8500000000000001, 'colsample_bytree': 0.25, 'gamma': 13.75, 'num_boost_round': 14, 'selector': 'mutual_info_regression', 'Num selected features': 22}. Best is trial 10 with value: 0.14662204664893166.\n",
      "[I 2024-03-29 12:44:54,837] Trial 13 finished with value: 0.3197508121817335 and parameters: {'booster': 'gbtree', 'max_depth': 4, 'lambda': 1.3335214321633246e-05, 'alpha': 7.498942093324561e-05, 'learning_rate': 0.173125, 'subsample': 0.95, 'colsample_bytree': 0.55, 'gamma': 1.25, 'num_boost_round': 2, 'selector': 'None', 'Num selected features': 1}. Best is trial 10 with value: 0.14662204664893166.\n",
      "[I 2024-03-29 12:44:55,413] Trial 14 finished with value: 0.3284606522818866 and parameters: {'booster': 'gbtree', 'max_depth': 9, 'lambda': 0.013335214321633242, 'alpha': 0.07498942093324559, 'learning_rate': 0.028124999999999997, 'subsample': 0.55, 'colsample_bytree': 0.95, 'gamma': 11.25, 'num_boost_round': 12, 'selector': 'mutual_info_regression', 'Num selected features': 11}. Best is trial 10 with value: 0.14662204664893166.\n",
      "[I 2024-03-29 12:44:56,239] Trial 15 finished with value: 0.15566269953938988 and parameters: {'booster': 'dart', 'max_depth': 6, 'lambda': 0.00042169650342858235, 'alpha': 2.3713737056616565e-06, 'learning_rate': 0.24562499999999998, 'subsample': 0.35000000000000003, 'colsample_bytree': 0.75, 'gamma': 6.25, 'num_boost_round': 17, 'selector': 'mutual_info_regression', 'Num selected features': 43}. Best is trial 10 with value: 0.14662204664893166.\n",
      "[I 2024-03-29 12:44:56,653] Trial 16 finished with value: 0.32846709744101055 and parameters: {'booster': 'gbtree', 'max_depth': 1, 'lambda': 0.4216965034285823, 'alpha': 0.002371373705661656, 'learning_rate': 0.10062499999999999, 'subsample': 0.75, 'colsample_bytree': 0.35000000000000003, 'gamma': 16.25, 'num_boost_round': 7, 'selector': 'mutual_info_regression', 'Num selected features': 3}. Best is trial 10 with value: 0.14662204664893166.\n",
      "[I 2024-03-29 12:44:57,438] Trial 17 finished with value: 0.18274446409798992 and parameters: {'booster': 'dart', 'max_depth': 1, 'lambda': 0.0006493816315762115, 'alpha': 0.0006493816315762115, 'learning_rate': 0.2003125, 'subsample': 0.42500000000000004, 'colsample_bytree': 0.9750000000000001, 'gamma': 10.625, 'num_boost_round': 17, 'selector': 'mutual_info_regression', 'Num selected features': 7}. Best is trial 10 with value: 0.14662204664893166.\n",
      "[I 2024-03-29 12:44:57,655] Trial 18 finished with value: 0.32001039974028134 and parameters: {'booster': 'gbtree', 'max_depth': 6, 'lambda': 0.6493816315762113, 'alpha': 0.6493816315762113, 'learning_rate': 0.0553125, 'subsample': 0.825, 'colsample_bytree': 0.575, 'gamma': 0.625, 'num_boost_round': 7, 'selector': 'None', 'Num selected features': 102}. Best is trial 10 with value: 0.14662204664893166.\n",
      "[I 2024-03-29 12:44:58,158] Trial 19 finished with value: 0.26836169467268334 and parameters: {'booster': 'gbtree', 'max_depth': 9, 'lambda': 2.053525026457147e-05, 'alpha': 2.053525026457147e-05, 'learning_rate': 0.2728125, 'subsample': 0.625, 'colsample_bytree': 0.375, 'gamma': 15.625, 'num_boost_round': 2, 'selector': 'mutual_info_regression', 'Num selected features': 26}. Best is trial 10 with value: 0.14662204664893166.\n",
      "[I 2024-03-29 12:44:58,376] Trial 20 finished with value: 0.17109085132795765 and parameters: {'booster': 'gbtree', 'max_depth': 4, 'lambda': 0.020535250264571463, 'alpha': 0.020535250264571463, 'learning_rate': 0.1278125, 'subsample': 0.225, 'colsample_bytree': 0.7750000000000001, 'gamma': 5.625, 'num_boost_round': 12, 'selector': 'None', 'Num selected features': 2}. Best is trial 10 with value: 0.14662204664893166.\n",
      "[I 2024-03-29 12:44:58,576] Trial 21 finished with value: 0.2952428170066857 and parameters: {'booster': 'dart', 'max_depth': 5, 'lambda': 3.6517412725483788e-06, 'alpha': 0.11547819846894583, 'learning_rate': 0.09156249999999999, 'subsample': 0.325, 'colsample_bytree': 0.875, 'gamma': 18.125, 'num_boost_round': 5, 'selector': 'f_regression', 'Num selected features': 13}. Best is trial 10 with value: 0.14662204664893166.\n",
      "[I 2024-03-29 12:44:59,099] Trial 22 finished with value: 0.15093360709731235 and parameters: {'booster': 'gbtree', 'max_depth': 10, 'lambda': 0.003651741272548378, 'alpha': 0.00011547819846894585, 'learning_rate': 0.23656249999999998, 'subsample': 0.7250000000000001, 'colsample_bytree': 0.47500000000000003, 'gamma': 8.125, 'num_boost_round': 15, 'selector': 'mutual_info_regression', 'Num selected features': 1}. Best is trial 10 with value: 0.14662204664893166.\n",
      "[I 2024-03-29 12:44:59,651] Trial 23 finished with value: 0.317429553059475 and parameters: {'booster': 'dart', 'max_depth': 8, 'lambda': 0.00011547819846894585, 'alpha': 0.003651741272548378, 'learning_rate': 0.0190625, 'subsample': 0.925, 'colsample_bytree': 0.275, 'gamma': 13.125, 'num_boost_round': 20, 'selector': 'f_regression', 'Num selected features': 3}. Best is trial 10 with value: 0.14662204664893166.\n",
      "[I 2024-03-29 12:45:00,064] Trial 24 finished with value: 0.16915589155850536 and parameters: {'booster': 'dart', 'max_depth': 3, 'lambda': 0.11547819846894583, 'alpha': 3.6517412725483788e-06, 'learning_rate': 0.1640625, 'subsample': 0.525, 'colsample_bytree': 0.675, 'gamma': 3.125, 'num_boost_round': 10, 'selector': 'None', 'Num selected features': 52}. Best is trial 10 with value: 0.14662204664893166.\n",
      "[I 2024-03-29 12:45:00,180] Trial 25 finished with value: 0.23579127356094293 and parameters: {'booster': 'dart', 'max_depth': 2, 'lambda': 8.659643233600657e-06, 'alpha': 0.0015399265260594922, 'learning_rate': 0.25468749999999996, 'subsample': 0.875, 'colsample_bytree': 0.7250000000000001, 'gamma': 19.375, 'num_boost_round': 4, 'selector': 'None', 'Num selected features': 9}. Best is trial 10 with value: 0.14662204664893166.\n",
      "[I 2024-03-29 12:45:00,710] Trial 26 finished with value: 0.17448219498650386 and parameters: {'booster': 'gbtree', 'max_depth': 7, 'lambda': 0.008659643233600654, 'alpha': 1.5399265260594928e-06, 'learning_rate': 0.1096875, 'subsample': 0.47500000000000003, 'colsample_bytree': 0.325, 'gamma': 9.375, 'num_boost_round': 14, 'selector': 'None', 'Num selected features': 1}. Best is trial 10 with value: 0.14662204664893166.\n",
      "[I 2024-03-29 12:45:01,263] Trial 27 finished with value: 0.14580569053321707 and parameters: {'booster': 'gbtree', 'max_depth': 10, 'lambda': 0.0002738419634264362, 'alpha': 0.04869675251658632, 'learning_rate': 0.1821875, 'subsample': 0.275, 'colsample_bytree': 0.525, 'gamma': 14.375, 'num_boost_round': 19, 'selector': 'mutual_info_regression', 'Num selected features': 2}. Best is trial 27 with value: 0.14580569053321707.\n",
      "[I 2024-03-29 12:45:01,610] Trial 28 finished with value: 0.330819402245599 and parameters: {'booster': 'gbtree', 'max_depth': 5, 'lambda': 0.27384196342643613, 'alpha': 4.8696752516586326e-05, 'learning_rate': 0.0371875, 'subsample': 0.675, 'colsample_bytree': 0.925, 'gamma': 4.375, 'num_boost_round': 9, 'selector': 'None', 'Num selected features': 37}. Best is trial 27 with value: 0.14580569053321707.\n",
      "[I 2024-03-29 12:45:02,833] Trial 29 finished with value: 0.19649621764293582 and parameters: {'booster': 'gbtree', 'max_depth': 3, 'lambda': 4.8696752516586326e-05, 'alpha': 8.659643233600657e-06, 'learning_rate': 0.07343749999999999, 'subsample': 0.7750000000000001, 'colsample_bytree': 0.625, 'gamma': 11.875, 'num_boost_round': 16, 'selector': 'mutual_info_regression', 'Num selected features': 5}. Best is trial 27 with value: 0.14580569053321707.\n",
      "[I 2024-03-29 12:45:03,647] Trial 30 finished with value: 0.180687653684933 and parameters: {'booster': 'gbtree', 'max_depth': 8, 'lambda': 0.04869675251658632, 'alpha': 0.008659643233600654, 'learning_rate': 0.2184375, 'subsample': 0.375, 'colsample_bytree': 0.225, 'gamma': 1.875, 'num_boost_round': 6, 'selector': 'None', 'Num selected features': 73}. Best is trial 27 with value: 0.14580569053321707.\n",
      "[I 2024-03-29 12:45:03,764] Trial 31 finished with value: 0.37758732023455266 and parameters: {'booster': 'gbtree', 'max_depth': 6, 'lambda': 1.5399265260594928e-06, 'alpha': 0.0002738419634264362, 'learning_rate': 0.1459375, 'subsample': 0.575, 'colsample_bytree': 0.42500000000000004, 'gamma': 16.875, 'num_boost_round': 1, 'selector': 'None', 'Num selected features': 18}. Best is trial 27 with value: 0.14580569053321707.\n",
      "[I 2024-03-29 12:45:04,379] Trial 32 finished with value: 0.20639648727661344 and parameters: {'booster': 'dart', 'max_depth': 1, 'lambda': 0.0015399265260594922, 'alpha': 0.27384196342643613, 'learning_rate': 0.2909375, 'subsample': 0.9750000000000001, 'colsample_bytree': 0.825, 'gamma': 6.875, 'num_boost_round': 11, 'selector': 'None', 'Num selected features': 1}. Best is trial 27 with value: 0.14580569053321707.\n",
      "[I 2024-03-29 12:45:05,101] Trial 33 finished with value: 0.21844393200230847 and parameters: {'booster': 'gbtree', 'max_depth': 1, 'lambda': 3.9241897584845336e-05, 'alpha': 0.016548170999431802, 'learning_rate': 0.16859375, 'subsample': 0.3125, 'colsample_bytree': 0.9375, 'gamma': 15.9375, 'num_boost_round': 14, 'selector': 'mutual_info_regression', 'Num selected features': 112}. Best is trial 27 with value: 0.14580569053321707.\n",
      "[I 2024-03-29 12:45:05,530] Trial 34 finished with value: 0.39945294464194964 and parameters: {'booster': 'gbtree', 'max_depth': 6, 'lambda': 0.03924189758484533, 'alpha': 1.6548170999431806e-05, 'learning_rate': 0.023593749999999997, 'subsample': 0.7125000000000001, 'colsample_bytree': 0.5375000000000001, 'gamma': 5.9375, 'num_boost_round': 4, 'selector': 'mutual_info_regression', 'Num selected features': 7}. Best is trial 27 with value: 0.14580569053321707.\n",
      "[I 2024-03-29 12:45:06,037] Trial 35 finished with value: 0.14867521043103862 and parameters: {'booster': 'gbtree', 'max_depth': 8, 'lambda': 1.240937760751719e-06, 'alpha': 0.5232991146814943, 'learning_rate': 0.24109375, 'subsample': 0.9125000000000001, 'colsample_bytree': 0.3375, 'gamma': 10.9375, 'num_boost_round': 9, 'selector': 'mutual_info_regression', 'Num selected features': 2}. Best is trial 27 with value: 0.14580569053321707.\n",
      "[I 2024-03-29 12:45:06,422] Trial 36 finished with value: 0.1607796225973192 and parameters: {'booster': 'dart', 'max_depth': 3, 'lambda': 0.0012409377607517198, 'alpha': 0.0005232991146814948, 'learning_rate': 0.09609374999999999, 'subsample': 0.5125, 'colsample_bytree': 0.7375, 'gamma': 0.9375, 'num_boost_round': 19, 'selector': 'f_regression', 'Num selected features': 28}. Best is trial 27 with value: 0.14580569053321707.\n",
      "[I 2024-03-29 12:45:06,562] Trial 37 finished with value: 0.23183538665653045 and parameters: {'booster': 'dart', 'max_depth': 5, 'lambda': 6.97830584859866e-06, 'alpha': 2.9427271762092804e-06, 'learning_rate': 0.13234374999999998, 'subsample': 0.41250000000000003, 'colsample_bytree': 0.8375000000000001, 'gamma': 13.4375, 'num_boost_round': 6, 'selector': 'f_regression', 'Num selected features': 1}. Best is trial 27 with value: 0.14580569053321707.\n",
      "[I 2024-03-29 12:45:08,109] Trial 38 finished with value: 0.16308601887191695 and parameters: {'booster': 'dart', 'max_depth': 10, 'lambda': 0.006978305848598658, 'alpha': 0.0029427271762092824, 'learning_rate': 0.27734375, 'subsample': 0.8125, 'colsample_bytree': 0.4375, 'gamma': 3.4375, 'num_boost_round': 16, 'selector': 'mutual_info_regression', 'Num selected features': 14}. Best is trial 27 with value: 0.14580569053321707.\n",
      "[I 2024-03-29 12:45:08,822] Trial 39 finished with value: 0.2617808410022363 and parameters: {'booster': 'dart', 'max_depth': 7, 'lambda': 0.00022067340690845884, 'alpha': 9.305720409296985e-05, 'learning_rate': 0.05984375, 'subsample': 0.6125, 'colsample_bytree': 0.23750000000000002, 'gamma': 18.4375, 'num_boost_round': 11, 'selector': 'mutual_info_regression', 'Num selected features': 56}. Best is trial 27 with value: 0.14580569053321707.\n",
      "[I 2024-03-29 12:45:09,221] Trial 40 finished with value: 0.3675473243300462 and parameters: {'booster': 'dart', 'max_depth': 2, 'lambda': 0.2206734069084588, 'alpha': 0.09305720409296982, 'learning_rate': 0.20484375, 'subsample': 0.21250000000000002, 'colsample_bytree': 0.6375, 'gamma': 8.4375, 'num_boost_round': 1, 'selector': 'mutual_info_regression', 'Num selected features': 4}. Best is trial 27 with value: 0.14580569053321707.\n",
      "[I 2024-03-29 12:45:09,393] Trial 41 finished with value: 0.1573215756277846 and parameters: {'booster': 'dart', 'max_depth': 3, 'lambda': 2.9427271762092804e-06, 'alpha': 3.9241897584845336e-05, 'learning_rate': 0.29546875, 'subsample': 0.7625, 'colsample_bytree': 0.7875000000000001, 'gamma': 14.6875, 'num_boost_round': 8, 'selector': 'None', 'Num selected features': 1}. Best is trial 27 with value: 0.14580569053321707.\n",
      "[I 2024-03-29 12:45:10,647] Trial 42 finished with value: 0.1516221330169985 and parameters: {'booster': 'dart', 'max_depth': 8, 'lambda': 0.0029427271762092824, 'alpha': 0.03924189758484533, 'learning_rate': 0.15046875, 'subsample': 0.36250000000000004, 'colsample_bytree': 0.3875, 'gamma': 4.6875, 'num_boost_round': 18, 'selector': 'None', 'Num selected features': 10}. Best is trial 27 with value: 0.14580569053321707.\n",
      "[I 2024-03-29 12:45:11,798] Trial 43 finished with value: 0.16529923208437783 and parameters: {'booster': 'gbtree', 'max_depth': 10, 'lambda': 9.305720409296985e-05, 'alpha': 1.240937760751719e-06, 'learning_rate': 0.22296875, 'subsample': 0.5625, 'colsample_bytree': 0.5875, 'gamma': 19.6875, 'num_boost_round': 13, 'selector': 'None', 'Num selected features': 40}. Best is trial 27 with value: 0.14580569053321707.\n",
      "[I 2024-03-29 12:45:11,914] Trial 44 finished with value: 0.3555073186813134 and parameters: {'booster': 'gbtree', 'max_depth': 5, 'lambda': 0.09305720409296982, 'alpha': 0.0012409377607517198, 'learning_rate': 0.07796874999999999, 'subsample': 0.9625000000000001, 'colsample_bytree': 0.9875, 'gamma': 9.6875, 'num_boost_round': 3, 'selector': 'None', 'Num selected features': 3}. Best is trial 27 with value: 0.14580569053321707.\n",
      "[I 2024-03-29 12:45:12,240] Trial 45 finished with value: 0.26847448170107735 and parameters: {'booster': 'gbtree', 'max_depth': 4, 'lambda': 0.0005232991146814948, 'alpha': 0.2206734069084588, 'learning_rate': 0.04171875, 'subsample': 0.8625, 'colsample_bytree': 0.6875, 'gamma': 17.1875, 'num_boost_round': 15, 'selector': 'f_regression', 'Num selected features': 79}. Best is trial 27 with value: 0.14580569053321707.\n",
      "[I 2024-03-29 12:45:12,700] Trial 46 finished with value: 0.2153886306479496 and parameters: {'booster': 'gbtree', 'max_depth': 9, 'lambda': 0.5232991146814943, 'alpha': 0.00022067340690845884, 'learning_rate': 0.18671875, 'subsample': 0.4625, 'colsample_bytree': 0.28750000000000003, 'gamma': 7.1875, 'num_boost_round': 5, 'selector': 'mutual_info_regression', 'Num selected features': 5}. Best is trial 27 with value: 0.14580569053321707.\n",
      "[I 2024-03-29 12:45:13,768] Trial 47 finished with value: 0.1926798468670669 and parameters: {'booster': 'dart', 'max_depth': 7, 'lambda': 1.6548170999431806e-05, 'alpha': 0.006978305848598658, 'learning_rate': 0.11421875, 'subsample': 0.2625, 'colsample_bytree': 0.48750000000000004, 'gamma': 12.1875, 'num_boost_round': 10, 'selector': 'mutual_info_regression', 'Num selected features': 1}. Best is trial 27 with value: 0.14580569053321707.\n",
      "[I 2024-03-29 12:45:14,243] Trial 48 finished with value: 0.15900639481904272 and parameters: {'booster': 'gbtree', 'max_depth': 2, 'lambda': 0.016548170999431802, 'alpha': 6.97830584859866e-06, 'learning_rate': 0.25921875, 'subsample': 0.6625000000000001, 'colsample_bytree': 0.8875, 'gamma': 2.1875, 'num_boost_round': 20, 'selector': 'mutual_info_regression', 'Num selected features': 20}. Best is trial 27 with value: 0.14580569053321707.\n",
      "[I 2024-03-29 12:45:14,371] Trial 49 finished with value: 0.3497975743178478 and parameters: {'booster': 'gbtree', 'max_depth': 1, 'lambda': 2.5482967479793498e-05, 'alpha': 0.06042963902381334, 'learning_rate': 0.05078125, 'subsample': 0.5375000000000001, 'colsample_bytree': 0.2625, 'gamma': 5.3125, 'num_boost_round': 10, 'selector': 'f_regression', 'Num selected features': 8}. Best is trial 27 with value: 0.14580569053321707.\n",
      "[I 2024-03-29 12:45:15,223] Trial 50 finished with value: 0.15900053899775343 and parameters: {'booster': 'gbtree', 'max_depth': 6, 'lambda': 0.025482967479793492, 'alpha': 6.042963902381336e-05, 'learning_rate': 0.19578125, 'subsample': 0.9375, 'colsample_bytree': 0.6625000000000001, 'gamma': 15.3125, 'num_boost_round': 20, 'selector': 'None', 'Num selected features': 1}. Best is trial 27 with value: 0.14580569053321707.\n",
      "[I 2024-03-29 12:45:15,836] Trial 51 finished with value: 0.15661441298783904 and parameters: {'booster': 'dart', 'max_depth': 9, 'lambda': 0.000805842187761482, 'alpha': 0.001910952974970441, 'learning_rate': 0.12328124999999998, 'subsample': 0.7375, 'colsample_bytree': 0.8625, 'gamma': 0.3125, 'num_boost_round': 15, 'selector': 'mutual_info_regression', 'Num selected features': 2}. Best is trial 27 with value: 0.14580569053321707.\n",
      "[I 2024-03-29 12:45:16,058] Trial 52 finished with value: 0.17601216961418503 and parameters: {'booster': 'dart', 'max_depth': 4, 'lambda': 0.8058421877614825, 'alpha': 1.910952974970443e-06, 'learning_rate': 0.26828124999999997, 'subsample': 0.3375, 'colsample_bytree': 0.4625, 'gamma': 10.3125, 'num_boost_round': 5, 'selector': 'f_regression', 'Num selected features': 34}. Best is trial 27 with value: 0.14580569053321707.\n",
      "[I 2024-03-29 12:45:16,397] Trial 53 finished with value: 0.1597444808004677 and parameters: {'booster': 'dart', 'max_depth': 5, 'lambda': 0.00014330125702369644, 'alpha': 0.000339820832894256, 'learning_rate': 0.23203125, 'subsample': 0.23750000000000002, 'colsample_bytree': 0.36250000000000004, 'gamma': 2.8125, 'num_boost_round': 12, 'selector': 'None', 'Num selected features': 4}. Best is trial 27 with value: 0.14580569053321707.\n",
      "[I 2024-03-29 12:45:16,970] Trial 54 finished with value: 0.3729319204088913 and parameters: {'booster': 'gbtree', 'max_depth': 10, 'lambda': 0.14330125702369642, 'alpha': 0.33982083289425624, 'learning_rate': 0.08703124999999999, 'subsample': 0.6375, 'colsample_bytree': 0.7625, 'gamma': 12.8125, 'num_boost_round': 2, 'selector': 'mutual_info_regression', 'Num selected features': 67}. Best is trial 27 with value: 0.14580569053321707.\n",
      "[I 2024-03-29 12:45:17,618] Trial 55 finished with value: 0.1994327395827658 and parameters: {'booster': 'dart', 'max_depth': 8, 'lambda': 4.531583637600824e-06, 'alpha': 1.074607828321319e-05, 'learning_rate': 0.15953124999999999, 'subsample': 0.8375000000000001, 'colsample_bytree': 0.9625000000000001, 'gamma': 7.8125, 'num_boost_round': 7, 'selector': 'mutual_info_regression', 'Num selected features': 17}. Best is trial 27 with value: 0.14580569053321707.\n",
      "[I 2024-03-29 12:45:18,092] Trial 56 finished with value: 0.354969859982854 and parameters: {'booster': 'dart', 'max_depth': 3, 'lambda': 0.0045315836376008225, 'alpha': 0.010746078283213186, 'learning_rate': 0.014531249999999999, 'subsample': 0.4375, 'colsample_bytree': 0.5625, 'gamma': 17.8125, 'num_boost_round': 17, 'selector': 'None', 'Num selected features': 1}. Best is trial 27 with value: 0.14580569053321707.\n",
      "[I 2024-03-29 12:45:18,267] Trial 57 finished with value: 0.220250838097322 and parameters: {'booster': 'dart', 'max_depth': 2, 'lambda': 0.000339820832894256, 'alpha': 2.5482967479793498e-05, 'learning_rate': 0.10515624999999999, 'subsample': 0.9875, 'colsample_bytree': 0.41250000000000003, 'gamma': 4.0625, 'num_boost_round': 11, 'selector': 'f_regression', 'Num selected features': 6}. Best is trial 27 with value: 0.14580569053321707.\n",
      "[I 2024-03-29 12:45:18,716] Trial 58 finished with value: 0.3458586574043073 and parameters: {'booster': 'gbtree', 'max_depth': 7, 'lambda': 0.33982083289425624, 'alpha': 0.025482967479793492, 'learning_rate': 0.25015624999999997, 'subsample': 0.5875, 'colsample_bytree': 0.8125, 'gamma': 14.0625, 'num_boost_round': 1, 'selector': 'mutual_info_regression', 'Num selected features': 94}. Best is trial 27 with value: 0.14580569053321707.\n",
      "[I 2024-03-29 12:45:19,101] Trial 59 finished with value: 0.36661057064749514 and parameters: {'booster': 'gbtree', 'max_depth': 9, 'lambda': 1.074607828321319e-05, 'alpha': 0.000805842187761482, 'learning_rate': 0.03265625, 'subsample': 0.3875, 'colsample_bytree': 0.6125, 'gamma': 9.0625, 'num_boost_round': 6, 'selector': 'f_regression', 'Num selected features': 24}. Best is trial 27 with value: 0.14580569053321707.\n",
      "[I 2024-03-29 12:45:19,351] Trial 60 finished with value: 0.13770124838983366 and parameters: {'booster': 'dart', 'max_depth': 4, 'lambda': 0.010746078283213186, 'alpha': 0.8058421877614825, 'learning_rate': 0.17765625, 'subsample': 0.7875000000000001, 'colsample_bytree': 0.21250000000000002, 'gamma': 19.0625, 'num_boost_round': 16, 'selector': 'f_regression', 'Num selected features': 2}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:19,525] Trial 61 finished with value: 0.16919661947190862 and parameters: {'booster': 'dart', 'max_depth': 3, 'lambda': 1.910952974970443e-06, 'alpha': 0.0045315836376008225, 'learning_rate': 0.21390625, 'subsample': 0.6875, 'colsample_bytree': 0.5125, 'gamma': 6.5625, 'num_boost_round': 8, 'selector': 'None', 'Num selected features': 12}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:20,167] Trial 62 finished with value: 0.18742614265058827 and parameters: {'booster': 'gbtree', 'max_depth': 8, 'lambda': 0.001910952974970441, 'alpha': 4.531583637600824e-06, 'learning_rate': 0.06890624999999999, 'subsample': 0.28750000000000003, 'colsample_bytree': 0.9125000000000001, 'gamma': 16.5625, 'num_boost_round': 18, 'selector': 'mutual_info_regression', 'Num selected features': 1}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:20,442] Trial 63 finished with value: 0.14073264219074239 and parameters: {'booster': 'dart', 'max_depth': 6, 'lambda': 6.042963902381336e-05, 'alpha': 0.14330125702369642, 'learning_rate': 0.28640625, 'subsample': 0.48750000000000004, 'colsample_bytree': 0.7125000000000001, 'gamma': 1.5625, 'num_boost_round': 13, 'selector': 'f_regression', 'Num selected features': 3}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:20,881] Trial 64 finished with value: 0.36528506338932176 and parameters: {'booster': 'dart', 'max_depth': 1, 'lambda': 0.06042963902381334, 'alpha': 0.00014330125702369644, 'learning_rate': 0.14140625, 'subsample': 0.8875, 'colsample_bytree': 0.3125, 'gamma': 11.5625, 'num_boost_round': 3, 'selector': 'mutual_info_regression', 'Num selected features': 47}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:21,508] Trial 65 finished with value: 0.27679434729525126 and parameters: {'booster': 'dart', 'max_depth': 1, 'lambda': 0.0002458244068920199, 'alpha': 0.08353625469578259, 'learning_rate': 0.25242187499999996, 'subsample': 0.71875, 'colsample_bytree': 0.55625, 'gamma': 13.28125, 'num_boost_round': 5, 'selector': 'mutual_info_regression', 'Num selected features': 29}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:22,105] Trial 66 finished with value: 0.16295483961623053 and parameters: {'booster': 'gbtree', 'max_depth': 6, 'lambda': 0.24582440689201987, 'alpha': 8.35362546957826e-05, 'learning_rate': 0.10742187499999999, 'subsample': 0.31875000000000003, 'colsample_bytree': 0.95625, 'gamma': 3.28125, 'num_boost_round': 15, 'selector': 'mutual_info_regression', 'Num selected features': 2}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:24,981] Trial 67 finished with value: 0.1591783662513669 and parameters: {'booster': 'dart', 'max_depth': 8, 'lambda': 7.773650302387765e-06, 'alpha': 0.0026416483203860917, 'learning_rate': 0.179921875, 'subsample': 0.51875, 'colsample_bytree': 0.7562500000000001, 'gamma': 18.28125, 'num_boost_round': 20, 'selector': 'None', 'Num selected features': 7}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:25,747] Trial 68 finished with value: 0.32790546277816934 and parameters: {'booster': 'gbtree', 'max_depth': 3, 'lambda': 0.007773650302387762, 'alpha': 2.6416483203860926e-06, 'learning_rate': 0.034921875, 'subsample': 0.91875, 'colsample_bytree': 0.35625, 'gamma': 8.28125, 'num_boost_round': 10, 'selector': 'f_regression', 'Num selected features': 116}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:26,580] Trial 69 finished with value: 0.18889111890838833 and parameters: {'booster': 'dart', 'max_depth': 4, 'lambda': 1.382372227357901e-06, 'alpha': 0.0004697588816706491, 'learning_rate': 0.071171875, 'subsample': 0.8187500000000001, 'colsample_bytree': 0.45625000000000004, 'gamma': 15.78125, 'num_boost_round': 17, 'selector': 'mutual_info_regression', 'Num selected features': 4}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:27,297] Trial 70 finished with value: 0.17941795473757577 and parameters: {'booster': 'dart', 'max_depth': 9, 'lambda': 0.0013823722273579005, 'alpha': 0.469758881670649, 'learning_rate': 0.21617187499999999, 'subsample': 0.41875, 'colsample_bytree': 0.85625, 'gamma': 5.78125, 'num_boost_round': 7, 'selector': 'mutual_info_regression', 'Num selected features': 59}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:27,429] Trial 71 finished with value: 0.3346746181397298 and parameters: {'booster': 'dart', 'max_depth': 7, 'lambda': 4.3714448126110935e-05, 'alpha': 1.485508017172775e-05, 'learning_rate': 0.143671875, 'subsample': 0.21875, 'colsample_bytree': 0.65625, 'gamma': 10.78125, 'num_boost_round': 2, 'selector': 'None', 'Num selected features': 15}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:27,903] Trial 72 finished with value: 0.1440168708721945 and parameters: {'booster': 'dart', 'max_depth': 2, 'lambda': 0.04371444812611092, 'alpha': 0.014855080171727746, 'learning_rate': 0.28867187499999997, 'subsample': 0.61875, 'colsample_bytree': 0.25625000000000003, 'gamma': 0.78125, 'num_boost_round': 12, 'selector': 'mutual_info_regression', 'Num selected features': 1}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:28,338] Trial 73 finished with value: 0.1399892016811283 and parameters: {'booster': 'gbtree', 'max_depth': 3, 'lambda': 1.843422992409112e-05, 'alpha': 6.2643353665688555e-06, 'learning_rate': 0.19804687499999998, 'subsample': 0.36875, 'colsample_bytree': 0.30625, 'gamma': 17.03125, 'num_boost_round': 16, 'selector': 'mutual_info_regression', 'Num selected features': 3}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:28,779] Trial 74 finished with value: 0.3331870933557789 and parameters: {'booster': 'gbtree', 'max_depth': 8, 'lambda': 0.018434229924091116, 'alpha': 0.006264335366568854, 'learning_rate': 0.053046875, 'subsample': 0.76875, 'colsample_bytree': 0.70625, 'gamma': 7.03125, 'num_boost_round': 6, 'selector': 'None', 'Num selected features': 42}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:29,216] Trial 75 finished with value: 0.3369060799579341 and parameters: {'booster': 'gbtree', 'max_depth': 10, 'lambda': 0.0005829415347136078, 'alpha': 0.00019809567785503384, 'learning_rate': 0.27054687499999996, 'subsample': 0.96875, 'colsample_bytree': 0.90625, 'gamma': 12.03125, 'num_boost_round': 1, 'selector': 'mutual_info_regression', 'Num selected features': 11}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:29,685] Trial 76 finished with value: 0.17584752830145975 and parameters: {'booster': 'dart', 'max_depth': 5, 'lambda': 0.5829415347136077, 'alpha': 0.1980956778550338, 'learning_rate': 0.125546875, 'subsample': 0.5687500000000001, 'colsample_bytree': 0.5062500000000001, 'gamma': 2.03125, 'num_boost_round': 11, 'selector': 'mutual_info_regression', 'Num selected features': 1}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:29,871] Trial 77 finished with value: 0.3450930108065414 and parameters: {'booster': 'gbtree', 'max_depth': 4, 'lambda': 0.00010366329284376988, 'alpha': 0.0011139738599948022, 'learning_rate': 0.08929687499999998, 'subsample': 0.46875, 'colsample_bytree': 0.20625000000000002, 'gamma': 14.53125, 'num_boost_round': 3, 'selector': 'None', 'Num selected features': 21}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:30,102] Trial 78 finished with value: 0.14765197327561852 and parameters: {'booster': 'gbtree', 'max_depth': 9, 'lambda': 0.10366329284376985, 'alpha': 1.1139738599948025e-06, 'learning_rate': 0.234296875, 'subsample': 0.8687500000000001, 'colsample_bytree': 0.60625, 'gamma': 4.53125, 'num_boost_round': 13, 'selector': 'f_regression', 'Num selected features': 1}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:30,561] Trial 79 finished with value: 0.3367843322512095 and parameters: {'booster': 'dart', 'max_depth': 6, 'lambda': 3.2781211513934614e-06, 'alpha': 0.035226946514731, 'learning_rate': 0.016796875, 'subsample': 0.66875, 'colsample_bytree': 0.8062500000000001, 'gamma': 19.53125, 'num_boost_round': 18, 'selector': 'f_regression', 'Num selected features': 5}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:30,737] Trial 80 finished with value: 0.2762349016951529 and parameters: {'booster': 'gbtree', 'max_depth': 1, 'lambda': 0.003278121151393461, 'alpha': 3.522694651473101e-05, 'learning_rate': 0.161796875, 'subsample': 0.26875000000000004, 'colsample_bytree': 0.40625, 'gamma': 9.53125, 'num_boost_round': 8, 'selector': 'f_regression', 'Num selected features': 83}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:30,967] Trial 81 finished with value: 0.15335277172143877 and parameters: {'booster': 'gbtree', 'max_depth': 2, 'lambda': 5.048065716667475e-06, 'alpha': 0.00964661619911199, 'learning_rate': 0.152734375, 'subsample': 0.9437500000000001, 'colsample_bytree': 0.6312500000000001, 'gamma': 2.65625, 'num_boost_round': 19, 'selector': 'f_regression', 'Num selected features': 35}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:31,138] Trial 82 finished with value: 0.1474814272843345 and parameters: {'booster': 'gbtree', 'max_depth': 7, 'lambda': 0.005048065716667474, 'alpha': 9.646616199111991e-06, 'learning_rate': 0.297734375, 'subsample': 0.54375, 'colsample_bytree': 0.23125, 'gamma': 12.65625, 'num_boost_round': 9, 'selector': 'f_regression', 'Num selected features': 2}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:31,286] Trial 83 finished with value: 0.32815363918474283 and parameters: {'booster': 'dart', 'max_depth': 9, 'lambda': 0.00015963385442879435, 'alpha': 0.3050527890267024, 'learning_rate': 0.080234375, 'subsample': 0.34375, 'colsample_bytree': 0.43125, 'gamma': 7.65625, 'num_boost_round': 4, 'selector': 'f_regression', 'Num selected features': 1}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:31,743] Trial 84 finished with value: 0.13839305044461322 and parameters: {'booster': 'gbtree', 'max_depth': 4, 'lambda': 0.1596338544287943, 'alpha': 0.00030505278902670253, 'learning_rate': 0.225234375, 'subsample': 0.7437500000000001, 'colsample_bytree': 0.83125, 'gamma': 17.65625, 'num_boost_round': 14, 'selector': 'mutual_info_regression', 'Num selected features': 9}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:31,849] Trial 85 finished with value: 0.3104718377642092 and parameters: {'booster': 'dart', 'max_depth': 5, 'lambda': 0.0008976871324473148, 'alpha': 1.7154378963428789e-06, 'learning_rate': 0.188984375, 'subsample': 0.64375, 'colsample_bytree': 0.73125, 'gamma': 5.15625, 'num_boost_round': 2, 'selector': 'None', 'Num selected features': 1}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:32,349] Trial 86 finished with value: 0.29300037857509353 and parameters: {'booster': 'gbtree', 'max_depth': 10, 'lambda': 0.8976871324473146, 'alpha': 0.0017154378963428786, 'learning_rate': 0.043984375, 'subsample': 0.24375000000000002, 'colsample_bytree': 0.33125000000000004, 'gamma': 15.15625, 'num_boost_round': 12, 'selector': 'f_regression', 'Num selected features': 18}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:33,542] Trial 87 finished with value: 0.16096202290345143 and parameters: {'booster': 'dart', 'max_depth': 8, 'lambda': 2.838735964758757e-05, 'alpha': 5.424690937011326e-05, 'learning_rate': 0.261484375, 'subsample': 0.44375000000000003, 'colsample_bytree': 0.53125, 'gamma': 0.15625, 'num_boost_round': 17, 'selector': 'None', 'Num selected features': 70}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:33,714] Trial 88 finished with value: 0.24000185707131924 and parameters: {'booster': 'dart', 'max_depth': 3, 'lambda': 0.028387359647587564, 'alpha': 0.05424690937011324, 'learning_rate': 0.11648437499999999, 'subsample': 0.84375, 'colsample_bytree': 0.9312500000000001, 'gamma': 10.15625, 'num_boost_round': 7, 'selector': 'None', 'Num selected features': 4}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:34,223] Trial 89 finished with value: 0.40929987664183 and parameters: {'booster': 'gbtree', 'max_depth': 2, 'lambda': 6.731703824144988e-05, 'alpha': 0.00012863969449369744, 'learning_rate': 0.025859374999999997, 'subsample': 0.59375, 'colsample_bytree': 0.8812500000000001, 'gamma': 6.40625, 'num_boost_round': 3, 'selector': 'mutual_info_regression', 'Num selected features': 2}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:35,396] Trial 90 finished with value: 0.1677592284250641 and parameters: {'booster': 'gbtree', 'max_depth': 7, 'lambda': 0.06731703824144986, 'alpha': 0.12863969449369742, 'learning_rate': 0.170859375, 'subsample': 0.9937500000000001, 'colsample_bytree': 0.48125, 'gamma': 16.40625, 'num_boost_round': 13, 'selector': 'None', 'Num selected features': 25}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:41,800] Trial 91 finished with value: 0.17130308664730526 and parameters: {'booster': 'gbtree', 'max_depth': 10, 'lambda': 2.1287516617963746e-06, 'alpha': 4.067944321083047e-06, 'learning_rate': 0.09835937499999999, 'subsample': 0.79375, 'colsample_bytree': 0.28125, 'gamma': 1.40625, 'num_boost_round': 18, 'selector': 'mutual_info_regression', 'Num selected features': 98}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:42,270] Trial 92 finished with value: 0.15195502048231763 and parameters: {'booster': 'dart', 'max_depth': 5, 'lambda': 0.002128751661796374, 'alpha': 0.004067944321083046, 'learning_rate': 0.243359375, 'subsample': 0.39375000000000004, 'colsample_bytree': 0.68125, 'gamma': 11.40625, 'num_boost_round': 8, 'selector': 'mutual_info_regression', 'Num selected features': 6}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:42,781] Trial 93 finished with value: 0.1531209851940468 and parameters: {'booster': 'dart', 'max_depth': 4, 'lambda': 1.197085030495731e-05, 'alpha': 0.7233941627366745, 'learning_rate': 0.279609375, 'subsample': 0.29375, 'colsample_bytree': 0.98125, 'gamma': 3.90625, 'num_boost_round': 20, 'selector': 'f_regression', 'Num selected features': 49}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:43,356] Trial 94 finished with value: 0.17686272658850516 and parameters: {'booster': 'dart', 'max_depth': 9, 'lambda': 0.011970850304957308, 'alpha': 0.0007233941627366746, 'learning_rate': 0.134609375, 'subsample': 0.6937500000000001, 'colsample_bytree': 0.58125, 'gamma': 13.90625, 'num_boost_round': 10, 'selector': 'mutual_info_regression', 'Num selected features': 3}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:43,561] Trial 95 finished with value: 0.19830083006011762 and parameters: {'booster': 'gbtree', 'max_depth': 6, 'lambda': 0.0003785515249258633, 'alpha': 0.02287573200318395, 'learning_rate': 0.20710937499999998, 'subsample': 0.89375, 'colsample_bytree': 0.38125000000000003, 'gamma': 8.90625, 'num_boost_round': 5, 'selector': 'f_regression', 'Num selected features': 1}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:44,060] Trial 96 finished with value: 0.3088835640288026 and parameters: {'booster': 'gbtree', 'max_depth': 1, 'lambda': 0.3785515249258632, 'alpha': 2.2875732003183956e-05, 'learning_rate': 0.062109375, 'subsample': 0.49375, 'colsample_bytree': 0.78125, 'gamma': 18.90625, 'num_boost_round': 15, 'selector': 'mutual_info_regression', 'Num selected features': 12}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:44,557] Trial 97 finished with value: 0.2637813192899318 and parameters: {'booster': 'dart', 'max_depth': 1, 'lambda': 6.2643353665688555e-06, 'alpha': 0.0002458244068920199, 'learning_rate': 0.11195312499999999, 'subsample': 0.60625, 'colsample_bytree': 0.6937500000000001, 'gamma': 7.96875, 'num_boost_round': 13, 'selector': 'mutual_info_regression', 'Num selected features': 2}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:45,001] Trial 98 finished with value: 0.2316000545707615 and parameters: {'booster': 'gbtree', 'max_depth': 6, 'lambda': 0.006264335366568854, 'alpha': 0.24582440689201987, 'learning_rate': 0.256953125, 'subsample': 0.20625000000000002, 'colsample_bytree': 0.29375, 'gamma': 17.96875, 'num_boost_round': 3, 'selector': 'mutual_info_regression', 'Num selected features': 32}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:45,609] Trial 99 finished with value: 0.33139474292515236 and parameters: {'booster': 'dart', 'max_depth': 8, 'lambda': 0.00019809567785503384, 'alpha': 7.773650302387765e-06, 'learning_rate': 0.039453125, 'subsample': 0.40625, 'colsample_bytree': 0.49375, 'gamma': 2.96875, 'num_boost_round': 8, 'selector': 'mutual_info_regression', 'Num selected features': 8}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:45,873] Trial 100 finished with value: 0.15489836877221352 and parameters: {'booster': 'gbtree', 'max_depth': 3, 'lambda': 0.1980956778550338, 'alpha': 0.007773650302387762, 'learning_rate': 0.184453125, 'subsample': 0.8062500000000001, 'colsample_bytree': 0.89375, 'gamma': 12.96875, 'num_boost_round': 18, 'selector': 'None', 'Num selected features': 1}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:46,074] Trial 101 finished with value: 0.15234258183875837 and parameters: {'booster': 'gbtree', 'max_depth': 5, 'lambda': 3.522694651473101e-05, 'alpha': 0.04371444812611092, 'learning_rate': 0.220703125, 'subsample': 0.90625, 'colsample_bytree': 0.79375, 'gamma': 0.46875, 'num_boost_round': 10, 'selector': 'f_regression', 'Num selected features': 16}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:46,522] Trial 102 finished with value: 0.17420049638682683 and parameters: {'booster': 'dart', 'max_depth': 10, 'lambda': 0.035226946514731, 'alpha': 4.3714448126110935e-05, 'learning_rate': 0.075703125, 'subsample': 0.5062500000000001, 'colsample_bytree': 0.39375000000000004, 'gamma': 10.46875, 'num_boost_round': 20, 'selector': 'f_regression', 'Num selected features': 1}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:46,990] Trial 103 finished with value: 0.16254712808659313 and parameters: {'booster': 'gbtree', 'max_depth': 7, 'lambda': 1.1139738599948025e-06, 'alpha': 0.0013823722273579005, 'learning_rate': 0.293203125, 'subsample': 0.30625, 'colsample_bytree': 0.59375, 'gamma': 5.46875, 'num_boost_round': 15, 'selector': 'None', 'Num selected features': 4}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:47,470] Trial 104 finished with value: 0.2669101610022428 and parameters: {'booster': 'dart', 'max_depth': 2, 'lambda': 0.0011139738599948022, 'alpha': 1.382372227357901e-06, 'learning_rate': 0.148203125, 'subsample': 0.70625, 'colsample_bytree': 0.9937500000000001, 'gamma': 15.46875, 'num_boost_round': 5, 'selector': 'mutual_info_regression', 'Num selected features': 64}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:47,640] Trial 105 finished with value: 0.2873796974149386 and parameters: {'booster': 'gbtree', 'max_depth': 3, 'lambda': 0.0004697588816706491, 'alpha': 0.003278121151393461, 'learning_rate': 0.057578125, 'subsample': 0.25625000000000003, 'colsample_bytree': 0.84375, 'gamma': 1.71875, 'num_boost_round': 9, 'selector': 'None', 'Num selected features': 23}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:48,152] Trial 106 finished with value: 0.14505487368886819 and parameters: {'booster': 'dart', 'max_depth': 8, 'lambda': 0.469758881670649, 'alpha': 3.2781211513934614e-06, 'learning_rate': 0.202578125, 'subsample': 0.65625, 'colsample_bytree': 0.44375000000000003, 'gamma': 11.71875, 'num_boost_round': 19, 'selector': 'f_regression', 'Num selected features': 1}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:48,571] Trial 107 finished with value: 0.1572913106810087 and parameters: {'booster': 'dart', 'max_depth': 10, 'lambda': 1.485508017172775e-05, 'alpha': 0.10366329284376985, 'learning_rate': 0.130078125, 'subsample': 0.85625, 'colsample_bytree': 0.24375000000000002, 'gamma': 6.71875, 'num_boost_round': 14, 'selector': 'f_regression', 'Num selected features': 6}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:49,065] Trial 108 finished with value: 0.1954031833134091 and parameters: {'booster': 'dart', 'max_depth': 5, 'lambda': 0.014855080171727746, 'alpha': 0.00010366329284376988, 'learning_rate': 0.275078125, 'subsample': 0.45625000000000004, 'colsample_bytree': 0.64375, 'gamma': 16.71875, 'num_boost_round': 4, 'selector': 'mutual_info_regression', 'Num selected features': 90}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:49,311] Trial 109 finished with value: 0.15361531117664423 and parameters: {'booster': 'gbtree', 'max_depth': 4, 'lambda': 2.6416483203860926e-06, 'alpha': 1.843422992409112e-05, 'learning_rate': 0.238828125, 'subsample': 0.55625, 'colsample_bytree': 0.9437500000000001, 'gamma': 9.21875, 'num_boost_round': 11, 'selector': 'None', 'Num selected features': 3}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:49,779] Trial 110 finished with value: 0.3977272720412264 and parameters: {'booster': 'gbtree', 'max_depth': 9, 'lambda': 0.0026416483203860917, 'alpha': 0.018434229924091116, 'learning_rate': 0.09382812499999998, 'subsample': 0.95625, 'colsample_bytree': 0.54375, 'gamma': 19.21875, 'num_boost_round': 1, 'selector': 'mutual_info_regression', 'Num selected features': 45}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:50,263] Trial 111 finished with value: 0.20550228069630383 and parameters: {'booster': 'gbtree', 'max_depth': 7, 'lambda': 8.35362546957826e-05, 'alpha': 0.0005829415347136078, 'learning_rate': 0.166328125, 'subsample': 0.7562500000000001, 'colsample_bytree': 0.34375, 'gamma': 4.21875, 'num_boost_round': 6, 'selector': 'mutual_info_regression', 'Num selected features': 11}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:50,617] Trial 112 finished with value: 0.3403759284699562 and parameters: {'booster': 'dart', 'max_depth': 2, 'lambda': 0.08353625469578259, 'alpha': 0.5829415347136077, 'learning_rate': 0.021328125, 'subsample': 0.35625, 'colsample_bytree': 0.7437500000000001, 'gamma': 14.21875, 'num_boost_round': 16, 'selector': 'f_regression', 'Num selected features': 1}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:50,727] Trial 113 finished with value: 0.2534735618049671 and parameters: {'booster': 'gbtree', 'max_depth': 1, 'lambda': 0.00012863969449369744, 'alpha': 5.048065716667475e-06, 'learning_rate': 0.284140625, 'subsample': 0.83125, 'colsample_bytree': 0.51875, 'gamma': 18.59375, 'num_boost_round': 6, 'selector': 'None', 'Num selected features': 2}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:51,304] Trial 114 finished with value: 0.14956406700518016 and parameters: {'booster': 'gbtree', 'max_depth': 6, 'lambda': 0.12863969449369742, 'alpha': 0.005048065716667474, 'learning_rate': 0.139140625, 'subsample': 0.43125, 'colsample_bytree': 0.91875, 'gamma': 8.59375, 'num_boost_round': 16, 'selector': 'mutual_info_regression', 'Num selected features': 27}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:52,148] Trial 115 finished with value: 0.16267530386617027 and parameters: {'booster': 'dart', 'max_depth': 9, 'lambda': 4.067944321083047e-06, 'alpha': 0.00015963385442879435, 'learning_rate': 0.211640625, 'subsample': 0.23125, 'colsample_bytree': 0.71875, 'gamma': 13.59375, 'num_boost_round': 11, 'selector': 'None', 'Num selected features': 107}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:52,224] Trial 116 finished with value: 0.40832889924370813 and parameters: {'booster': 'dart', 'max_depth': 4, 'lambda': 0.004067944321083046, 'alpha': 0.1596338544287943, 'learning_rate': 0.066640625, 'subsample': 0.6312500000000001, 'colsample_bytree': 0.31875000000000003, 'gamma': 3.59375, 'num_boost_round': 1, 'selector': 'None', 'Num selected features': 7}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:52,827] Trial 117 finished with value: 0.31516022649236064 and parameters: {'booster': 'gbtree', 'max_depth': 5, 'lambda': 2.2875732003183956e-05, 'alpha': 0.028387359647587564, 'learning_rate': 0.030390624999999998, 'subsample': 0.73125, 'colsample_bytree': 0.41875, 'gamma': 11.09375, 'num_boost_round': 13, 'selector': 'mutual_info_regression', 'Num selected features': 54}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:52,991] Trial 118 finished with value: 0.27927493912205426 and parameters: {'booster': 'gbtree', 'max_depth': 10, 'lambda': 0.02287573200318395, 'alpha': 2.838735964758757e-05, 'learning_rate': 0.175390625, 'subsample': 0.33125000000000004, 'colsample_bytree': 0.8187500000000001, 'gamma': 1.09375, 'num_boost_round': 3, 'selector': 'f_regression', 'Num selected features': 3}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:53,493] Trial 119 finished with value: 0.2343292920511621 and parameters: {'booster': 'dart', 'max_depth': 7, 'lambda': 0.0007233941627366746, 'alpha': 0.8976871324473146, 'learning_rate': 0.10289062499999999, 'subsample': 0.53125, 'colsample_bytree': 0.61875, 'gamma': 16.09375, 'num_boost_round': 8, 'selector': 'None', 'Num selected features': 1}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:54,338] Trial 120 finished with value: 0.14449736151081777 and parameters: {'booster': 'dart', 'max_depth': 2, 'lambda': 0.7233941627366745, 'alpha': 0.0008976871324473148, 'learning_rate': 0.247890625, 'subsample': 0.9312500000000001, 'colsample_bytree': 0.21875, 'gamma': 6.09375, 'num_boost_round': 18, 'selector': 'mutual_info_regression', 'Num selected features': 14}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:54,876] Trial 121 finished with value: 0.1701682838523277 and parameters: {'booster': 'gbtree', 'max_depth': 2, 'lambda': 1.7154378963428789e-06, 'alpha': 0.3785515249258632, 'learning_rate': 0.157265625, 'subsample': 0.48125, 'colsample_bytree': 0.36875, 'gamma': 12.34375, 'num_boost_round': 14, 'selector': 'mutual_info_regression', 'Num selected features': 38}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:55,019] Trial 122 finished with value: 0.4156347995609703 and parameters: {'booster': 'gbtree', 'max_depth': 7, 'lambda': 0.0017154378963428786, 'alpha': 0.0003785515249258633, 'learning_rate': 0.012265625, 'subsample': 0.8812500000000001, 'colsample_bytree': 0.76875, 'gamma': 2.34375, 'num_boost_round': 4, 'selector': 'f_regression', 'Num selected features': 2}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:55,396] Trial 123 finished with value: 0.15218878949282982 and parameters: {'booster': 'dart', 'max_depth': 9, 'lambda': 5.424690937011326e-05, 'alpha': 0.011970850304957308, 'learning_rate': 0.229765625, 'subsample': 0.68125, 'colsample_bytree': 0.96875, 'gamma': 17.34375, 'num_boost_round': 9, 'selector': 'f_regression', 'Num selected features': 1}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:56,202] Trial 124 finished with value: 0.16410035375756643 and parameters: {'booster': 'dart', 'max_depth': 4, 'lambda': 0.05424690937011324, 'alpha': 1.197085030495731e-05, 'learning_rate': 0.08476562499999998, 'subsample': 0.28125, 'colsample_bytree': 0.5687500000000001, 'gamma': 7.34375, 'num_boost_round': 19, 'selector': 'mutual_info_regression', 'Num selected features': 10}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:56,620] Trial 125 finished with value: 0.22882319546986749 and parameters: {'booster': 'gbtree', 'max_depth': 3, 'lambda': 0.00030505278902670253, 'alpha': 6.731703824144988e-05, 'learning_rate': 0.12101562499999999, 'subsample': 0.38125000000000003, 'colsample_bytree': 0.26875000000000004, 'gamma': 19.84375, 'num_boost_round': 7, 'selector': 'mutual_info_regression', 'Num selected features': 1}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:57,608] Trial 126 finished with value: 0.1610317318417836 and parameters: {'booster': 'gbtree', 'max_depth': 8, 'lambda': 0.3050527890267024, 'alpha': 0.06731703824144986, 'learning_rate': 0.266015625, 'subsample': 0.78125, 'colsample_bytree': 0.66875, 'gamma': 9.84375, 'num_boost_round': 17, 'selector': 'None', 'Num selected features': 19}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:58,624] Trial 127 finished with value: 0.277361820610852 and parameters: {'booster': 'dart', 'max_depth': 6, 'lambda': 9.646616199111991e-06, 'alpha': 2.1287516617963746e-06, 'learning_rate': 0.048515625, 'subsample': 0.98125, 'colsample_bytree': 0.8687500000000001, 'gamma': 14.84375, 'num_boost_round': 12, 'selector': 'mutual_info_regression', 'Num selected features': 76}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:59,031] Trial 128 finished with value: 0.36613335563439137 and parameters: {'booster': 'dart', 'max_depth': 1, 'lambda': 0.00964661619911199, 'alpha': 0.002128751661796374, 'learning_rate': 0.193515625, 'subsample': 0.58125, 'colsample_bytree': 0.46875, 'gamma': 4.84375, 'num_boost_round': 2, 'selector': 'mutual_info_regression', 'Num selected features': 5}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:45:59,231] Trial 129 finished with value: 0.37730392469812307 and parameters: {'booster': 'dart', 'max_depth': 1, 'lambda': 9.821718891880379e-05, 'alpha': 5.725487884358382e-05, 'learning_rate': 0.0655078125, 'subsample': 0.921875, 'colsample_bytree': 0.7406250000000001, 'gamma': 17.265625, 'num_boost_round': 5, 'selector': 'None', 'Num selected features': 1}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:00,150] Trial 130 finished with value: 0.1451943387119002 and parameters: {'booster': 'dart', 'max_depth': 6, 'lambda': 0.09821718891880377, 'alpha': 0.05725487884358381, 'learning_rate': 0.2105078125, 'subsample': 0.5218750000000001, 'colsample_bytree': 0.340625, 'gamma': 7.265625, 'num_boost_round': 15, 'selector': 'mutual_info_regression', 'Num selected features': 14}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:01,254] Trial 131 finished with value: 0.15466275725781892 and parameters: {'booster': 'gbtree', 'max_depth': 8, 'lambda': 3.1059002236247057e-06, 'alpha': 1.8105582430271232e-06, 'learning_rate': 0.1380078125, 'subsample': 0.321875, 'colsample_bytree': 0.540625, 'gamma': 12.265625, 'num_boost_round': 20, 'selector': 'None', 'Num selected features': 55}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:01,435] Trial 132 finished with value: 0.13792661445893448 and parameters: {'booster': 'gbtree', 'max_depth': 3, 'lambda': 0.0031059002236247047, 'alpha': 0.0018105582430271226, 'learning_rate': 0.2830078125, 'subsample': 0.721875, 'colsample_bytree': 0.940625, 'gamma': 2.265625, 'num_boost_round': 10, 'selector': 'f_regression', 'Num selected features': 4}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:02,302] Trial 133 finished with value: 0.1560525247629317 and parameters: {'booster': 'dart', 'max_depth': 4, 'lambda': 1.7465760476621183e-05, 'alpha': 0.321967844425138, 'learning_rate': 0.24675781249999998, 'subsample': 0.621875, 'colsample_bytree': 0.640625, 'gamma': 14.765625, 'num_boost_round': 18, 'selector': 'mutual_info_regression', 'Num selected features': 109}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:02,493] Trial 134 finished with value: 0.2318601659350726 and parameters: {'booster': 'gbtree', 'max_depth': 9, 'lambda': 0.01746576047662118, 'alpha': 0.00032196784442513804, 'learning_rate': 0.10175781249999999, 'subsample': 0.22187500000000002, 'colsample_bytree': 0.240625, 'gamma': 4.765625, 'num_boost_round': 8, 'selector': 'f_regression', 'Num selected features': 7}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:02,736] Trial 135 finished with value: 0.28072864940078346 and parameters: {'booster': 'gbtree', 'max_depth': 7, 'lambda': 0.0005523158417307099, 'alpha': 0.010181517217181822, 'learning_rate': 0.1742578125, 'subsample': 0.421875, 'colsample_bytree': 0.44062500000000004, 'gamma': 19.765625, 'num_boost_round': 3, 'selector': 'None', 'Num selected features': 2}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:03,381] Trial 136 finished with value: 0.33259213283315747 and parameters: {'booster': 'dart', 'max_depth': 2, 'lambda': 0.5523158417307098, 'alpha': 1.0181517217181825e-05, 'learning_rate': 0.0292578125, 'subsample': 0.8218750000000001, 'colsample_bytree': 0.840625, 'gamma': 9.765625, 'num_boost_round': 13, 'selector': 'mutual_info_regression', 'Num selected features': 28}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:03,799] Trial 137 finished with value: 0.2052908815347993 and parameters: {'booster': 'dart', 'max_depth': 2, 'lambda': 1.30974726430059e-06, 'alpha': 0.024144182212566402, 'learning_rate': 0.08363281249999999, 'subsample': 0.571875, 'colsample_bytree': 0.9906250000000001, 'gamma': 13.515625, 'num_boost_round': 17, 'selector': 'None', 'Num selected features': 77}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:04,203] Trial 138 finished with value: 0.1704829695677569 and parameters: {'booster': 'dart', 'max_depth': 7, 'lambda': 0.0013097472643005899, 'alpha': 2.4144182212566407e-05, 'learning_rate': 0.2286328125, 'subsample': 0.971875, 'colsample_bytree': 0.590625, 'gamma': 3.515625, 'num_boost_round': 7, 'selector': 'None', 'Num selected features': 5}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:04,327] Trial 139 finished with value: 0.4257404351447701 and parameters: {'booster': 'gbtree', 'max_depth': 10, 'lambda': 4.1417845143644053e-05, 'alpha': 0.7635060803383348, 'learning_rate': 0.0111328125, 'subsample': 0.7718750000000001, 'colsample_bytree': 0.390625, 'gamma': 18.515625, 'num_boost_round': 2, 'selector': 'f_regression', 'Num selected features': 1}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:04,592] Trial 140 finished with value: 0.15721931715694004 and parameters: {'booster': 'dart', 'max_depth': 5, 'lambda': 0.04141784514364404, 'alpha': 0.0007635060803383349, 'learning_rate': 0.1561328125, 'subsample': 0.371875, 'colsample_bytree': 0.7906250000000001, 'gamma': 8.515625, 'num_boost_round': 12, 'selector': 'f_regression', 'Num selected features': 20}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:05,004] Trial 141 finished with value: 0.23125419633492134 and parameters: {'booster': 'gbtree', 'max_depth': 4, 'lambda': 0.0002329096592460546, 'alpha': 4.2935102100834845e-06, 'learning_rate': 0.1923828125, 'subsample': 0.27187500000000003, 'colsample_bytree': 0.890625, 'gamma': 16.015625, 'num_boost_round': 4, 'selector': 'mutual_info_regression', 'Num selected features': 1}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:05,681] Trial 142 finished with value: 0.26156949076241265 and parameters: {'booster': 'dart', 'max_depth': 9, 'lambda': 0.23290965924605456, 'alpha': 0.004293510210083484, 'learning_rate': 0.047382812499999996, 'subsample': 0.671875, 'colsample_bytree': 0.49062500000000003, 'gamma': 6.015625, 'num_boost_round': 14, 'selector': 'mutual_info_regression', 'Num selected features': 10}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:06,518] Trial 143 finished with value: 0.1625358277068756 and parameters: {'booster': 'dart', 'max_depth': 6, 'lambda': 7.365250122712282e-06, 'alpha': 0.00013577271421051846, 'learning_rate': 0.2648828125, 'subsample': 0.871875, 'colsample_bytree': 0.290625, 'gamma': 11.015625, 'num_boost_round': 19, 'selector': 'mutual_info_regression', 'Num selected features': 39}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:06,672] Trial 144 finished with value: 0.2914500560043713 and parameters: {'booster': 'gbtree', 'max_depth': 1, 'lambda': 0.00736525012271228, 'alpha': 0.13577271421051842, 'learning_rate': 0.11988281249999999, 'subsample': 0.47187500000000004, 'colsample_bytree': 0.690625, 'gamma': 1.015625, 'num_boost_round': 9, 'selector': 'f_regression', 'Num selected features': 2}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:07,136] Trial 145 finished with value: 0.13832093936316092 and parameters: {'booster': 'gbtree', 'max_depth': 2, 'lambda': 1.1341944035027571e-05, 'alpha': 1.5678788438269714e-05, 'learning_rate': 0.1833203125, 'subsample': 0.746875, 'colsample_bytree': 0.465625, 'gamma': 6.640625, 'num_boost_round': 19, 'selector': 'mutual_info_regression', 'Num selected features': 4}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:07,532] Trial 146 finished with value: 0.32490874938244285 and parameters: {'booster': 'dart', 'max_depth': 7, 'lambda': 0.011341944035027567, 'alpha': 0.01567878843826971, 'learning_rate': 0.038320312499999995, 'subsample': 0.34687500000000004, 'colsample_bytree': 0.8656250000000001, 'gamma': 16.640625, 'num_boost_round': 9, 'selector': 'f_regression', 'Num selected features': 65}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:08,051] Trial 147 finished with value: 0.19927322934842934 and parameters: {'booster': 'dart', 'max_depth': 9, 'lambda': 0.0003586637624484769, 'alpha': 0.0004958068241684658, 'learning_rate': 0.2558203125, 'subsample': 0.546875, 'colsample_bytree': 0.665625, 'gamma': 1.640625, 'num_boost_round': 4, 'selector': 'mutual_info_regression', 'Num selected features': 17}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:08,486] Trial 148 finished with value: 0.16416138737113878 and parameters: {'booster': 'gbtree', 'max_depth': 4, 'lambda': 0.3586637624484768, 'alpha': 0.4958068241684657, 'learning_rate': 0.11082031249999999, 'subsample': 0.9468750000000001, 'colsample_bytree': 0.265625, 'gamma': 11.640625, 'num_boost_round': 14, 'selector': 'mutual_info_regression', 'Num selected features': 1}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:08,897] Trial 149 finished with value: 0.3781694682687272 and parameters: {'booster': 'dart', 'max_depth': 5, 'lambda': 6.378043838892179e-05, 'alpha': 0.0027881266654131345, 'learning_rate': 0.1470703125, 'subsample': 0.846875, 'colsample_bytree': 0.565625, 'gamma': 4.140625, 'num_boost_round': 1, 'selector': 'mutual_info_regression', 'Num selected features': 8}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:09,707] Trial 150 finished with value: 0.16777592243143685 and parameters: {'booster': 'gbtree', 'max_depth': 10, 'lambda': 0.06378043838892178, 'alpha': 2.788126665413135e-06, 'learning_rate': 0.2920703125, 'subsample': 0.446875, 'colsample_bytree': 0.965625, 'gamma': 14.140625, 'num_boost_round': 11, 'selector': 'None', 'Num selected features': 1}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:10,216] Trial 151 finished with value: 0.19089594375668323 and parameters: {'booster': 'gbtree', 'max_depth': 8, 'lambda': 2.016914554730331e-06, 'alpha': 0.08816830667755711, 'learning_rate': 0.07457031249999999, 'subsample': 0.246875, 'colsample_bytree': 0.765625, 'gamma': 9.140625, 'num_boost_round': 16, 'selector': 'mutual_info_regression', 'Num selected features': 2}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:10,655] Trial 152 finished with value: 0.18274847286932094 and parameters: {'booster': 'gbtree', 'max_depth': 3, 'lambda': 0.0020169145547303305, 'alpha': 8.816830667755714e-05, 'learning_rate': 0.2195703125, 'subsample': 0.6468750000000001, 'colsample_bytree': 0.36562500000000003, 'gamma': 19.140625, 'num_boost_round': 6, 'selector': 'mutual_info_regression', 'Num selected features': 33}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:11,070] Trial 153 finished with value: 0.28551298389935803 and parameters: {'booster': 'gbtree', 'max_depth': 2, 'lambda': 0.0008505258154439963, 'alpha': 0.037180266639144754, 'learning_rate': 0.2739453125, 'subsample': 0.39687500000000003, 'colsample_bytree': 0.215625, 'gamma': 2.890625, 'num_boost_round': 2, 'selector': 'mutual_info_regression', 'Num selected features': 12}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:11,532] Trial 154 finished with value: 0.168602941553529 and parameters: {'booster': 'gbtree', 'max_depth': 7, 'lambda': 0.8505258154439961, 'alpha': 3.7180266639144764e-05, 'learning_rate': 0.1289453125, 'subsample': 0.796875, 'colsample_bytree': 0.6156250000000001, 'gamma': 12.890625, 'num_boost_round': 12, 'selector': 'mutual_info_regression', 'Num selected features': 1}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:12,162] Trial 155 finished with value: 0.15828563400190446 and parameters: {'booster': 'gbtree', 'max_depth': 10, 'lambda': 2.6895987855750437e-05, 'alpha': 0.0011757432659207114, 'learning_rate': 0.2014453125, 'subsample': 0.996875, 'colsample_bytree': 0.815625, 'gamma': 7.890625, 'num_boost_round': 17, 'selector': 'mutual_info_regression', 'Num selected features': 3}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:12,384] Trial 156 finished with value: 0.3141528247001542 and parameters: {'booster': 'gbtree', 'max_depth': 5, 'lambda': 0.02689598785575043, 'alpha': 1.1757432659207118e-06, 'learning_rate': 0.0564453125, 'subsample': 0.596875, 'colsample_bytree': 0.415625, 'gamma': 17.890625, 'num_boost_round': 7, 'selector': 'f_regression', 'Num selected features': 46}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:12,638] Trial 157 finished with value: 0.31570036521498335 and parameters: {'booster': 'gbtree', 'max_depth': 3, 'lambda': 4.782858141653792e-06, 'alpha': 0.00020908000412787192, 'learning_rate': 0.0201953125, 'subsample': 0.496875, 'colsample_bytree': 0.31562500000000004, 'gamma': 5.390625, 'num_boost_round': 20, 'selector': 'None', 'Num selected features': 6}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:13,858] Trial 158 finished with value: 0.1726425413191684 and parameters: {'booster': 'dart', 'max_depth': 8, 'lambda': 0.004782858141653791, 'alpha': 0.20908000412787187, 'learning_rate': 0.1651953125, 'subsample': 0.8968750000000001, 'colsample_bytree': 0.715625, 'gamma': 15.390625, 'num_boost_round': 10, 'selector': 'mutual_info_regression', 'Num selected features': 92}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:14,349] Trial 159 finished with value: 0.2942157790399559 and parameters: {'booster': 'dart', 'max_depth': 6, 'lambda': 0.00015124725453106235, 'alpha': 6.61169026241482e-06, 'learning_rate': 0.09269531249999999, 'subsample': 0.696875, 'colsample_bytree': 0.9156250000000001, 'gamma': 0.390625, 'num_boost_round': 5, 'selector': 'mutual_info_regression', 'Num selected features': 23}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:14,674] Trial 160 finished with value: 0.19029414687670282 and parameters: {'booster': 'dart', 'max_depth': 1, 'lambda': 0.1512472545310623, 'alpha': 0.006611690262414818, 'learning_rate': 0.2376953125, 'subsample': 0.296875, 'colsample_bytree': 0.515625, 'gamma': 10.390625, 'num_boost_round': 15, 'selector': 'None', 'Num selected features': 1}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:14,813] Trial 161 finished with value: 0.21668895492118034 and parameters: {'booster': 'gbtree', 'max_depth': 1, 'lambda': 2.5028654311746106e-06, 'alpha': 0.9474635256553747, 'learning_rate': 0.2241015625, 'subsample': 0.809375, 'colsample_bytree': 0.403125, 'gamma': 1.328125, 'num_boost_round': 12, 'selector': 'None', 'Num selected features': 71}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:14,984] Trial 162 finished with value: 0.3752730157642788 and parameters: {'booster': 'dart', 'max_depth': 6, 'lambda': 0.0025028654311746077, 'alpha': 0.0009474635256553758, 'learning_rate': 0.07910156249999999, 'subsample': 0.40937500000000004, 'colsample_bytree': 0.8031250000000001, 'gamma': 11.328125, 'num_boost_round': 2, 'selector': 'f_regression', 'Num selected features': 5}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:15,228] Trial 163 finished with value: 0.160810630662176 and parameters: {'booster': 'gbtree', 'max_depth': 9, 'lambda': 7.914755439411168e-05, 'alpha': 0.029961427410043623, 'learning_rate': 0.29660156249999997, 'subsample': 0.209375, 'colsample_bytree': 0.603125, 'gamma': 6.328125, 'num_boost_round': 7, 'selector': 'f_regression', 'Num selected features': 1}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:15,941] Trial 164 finished with value: 0.15279947465174057 and parameters: {'booster': 'gbtree', 'max_depth': 4, 'lambda': 0.07914755439411167, 'alpha': 2.996142741004363e-05, 'learning_rate': 0.1516015625, 'subsample': 0.609375, 'colsample_bytree': 0.203125, 'gamma': 16.328125, 'num_boost_round': 17, 'selector': 'None', 'Num selected features': 18}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:16,190] Trial 165 finished with value: 0.20926993113091247 and parameters: {'booster': 'gbtree', 'max_depth': 5, 'lambda': 0.0004450794062355996, 'alpha': 0.00016848548794358382, 'learning_rate': 0.11535156249999999, 'subsample': 0.7093750000000001, 'colsample_bytree': 0.503125, 'gamma': 8.828125, 'num_boost_round': 9, 'selector': 'None', 'Num selected features': 1}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:17,369] Trial 166 finished with value: 0.1529956395179097 and parameters: {'booster': 'dart', 'max_depth': 10, 'lambda': 0.4450794062355999, 'alpha': 0.16848548794358378, 'learning_rate': 0.26035156249999997, 'subsample': 0.309375, 'colsample_bytree': 0.903125, 'gamma': 18.828125, 'num_boost_round': 19, 'selector': 'None', 'Num selected features': 9}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:18,401] Trial 167 finished with value: 0.27219024739648906 and parameters: {'booster': 'dart', 'max_depth': 7, 'lambda': 1.407464663339845e-05, 'alpha': 5.3279789458656395e-06, 'learning_rate': 0.0428515625, 'subsample': 0.509375, 'colsample_bytree': 0.703125, 'gamma': 3.828125, 'num_boost_round': 14, 'selector': 'mutual_info_regression', 'Num selected features': 36}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:18,874] Trial 168 finished with value: 0.26024325077817706 and parameters: {'booster': 'gbtree', 'max_depth': 2, 'lambda': 0.014074646633398446, 'alpha': 0.005327978945865638, 'learning_rate': 0.18785156249999999, 'subsample': 0.909375, 'colsample_bytree': 0.30312500000000003, 'gamma': 13.828125, 'num_boost_round': 4, 'selector': 'mutual_info_regression', 'Num selected features': 2}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:19,025] Trial 169 finished with value: 0.16863969379095067 and parameters: {'booster': 'gbtree', 'max_depth': 3, 'lambda': 3.337624694292042e-05, 'alpha': 2.246790091812644e-06, 'learning_rate': 0.2422265625, 'subsample': 0.45937500000000003, 'colsample_bytree': 0.25312500000000004, 'gamma': 7.578125, 'num_boost_round': 8, 'selector': 'None', 'Num selected features': 1}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:20,357] Trial 170 finished with value: 0.16827630890975262 and parameters: {'booster': 'dart', 'max_depth': 8, 'lambda': 0.033376246942920414, 'alpha': 0.0022467900918126454, 'learning_rate': 0.09722656249999999, 'subsample': 0.859375, 'colsample_bytree': 0.653125, 'gamma': 17.578125, 'num_boost_round': 18, 'selector': 'None', 'Num selected features': 13}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:21,905] Trial 171 finished with value: 0.16031666472161965 and parameters: {'booster': 'dart', 'max_depth': 10, 'lambda': 1.0554496008786042e-06, 'alpha': 7.104974114426784e-05, 'learning_rate': 0.1697265625, 'subsample': 0.659375, 'colsample_bytree': 0.8531250000000001, 'gamma': 2.578125, 'num_boost_round': 13, 'selector': 'None', 'Num selected features': 50}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:22,036] Trial 172 finished with value: 0.4062406481832518 and parameters: {'booster': 'gbtree', 'max_depth': 5, 'lambda': 0.001055449600878603, 'alpha': 0.07104974114426783, 'learning_rate': 0.0247265625, 'subsample': 0.259375, 'colsample_bytree': 0.453125, 'gamma': 12.578125, 'num_boost_round': 3, 'selector': 'None', 'Num selected features': 3}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:22,701] Trial 173 finished with value: 0.25869016448855586 and parameters: {'booster': 'dart', 'max_depth': 4, 'lambda': 5.9352292722969924e-06, 'alpha': 0.012634629176544678, 'learning_rate': 0.0609765625, 'subsample': 0.359375, 'colsample_bytree': 0.353125, 'gamma': 0.078125, 'num_boost_round': 11, 'selector': 'mutual_info_regression', 'Num selected features': 100}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:22,882] Trial 174 finished with value: 0.3560008903468413 and parameters: {'booster': 'dart', 'max_depth': 9, 'lambda': 0.005935229272296992, 'alpha': 1.2634629176544682e-05, 'learning_rate': 0.2059765625, 'subsample': 0.7593750000000001, 'colsample_bytree': 0.753125, 'gamma': 10.078125, 'num_boost_round': 1, 'selector': 'f_regression', 'Num selected features': 6}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:23,396] Trial 175 finished with value: 0.23148284021140703 and parameters: {'booster': 'dart', 'max_depth': 7, 'lambda': 0.00018768842935762206, 'alpha': 0.39954205589498837, 'learning_rate': 0.1334765625, 'subsample': 0.9593750000000001, 'colsample_bytree': 0.953125, 'gamma': 5.078125, 'num_boost_round': 6, 'selector': 'mutual_info_regression', 'Num selected features': 2}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:23,717] Trial 176 finished with value: 0.14740090446703205 and parameters: {'booster': 'dart', 'max_depth': 2, 'lambda': 0.187688429357622, 'alpha': 0.00039954205589498885, 'learning_rate': 0.27847656249999997, 'subsample': 0.559375, 'colsample_bytree': 0.5531250000000001, 'gamma': 15.078125, 'num_boost_round': 16, 'selector': 'f_regression', 'Num selected features': 25}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:23,841] Trial 177 finished with value: 0.39489582030900483 and parameters: {'booster': 'gbtree', 'max_depth': 1, 'lambda': 0.00028902639100224475, 'alpha': 0.0014590242156305613, 'learning_rate': 0.0337890625, 'subsample': 0.634375, 'colsample_bytree': 0.778125, 'gamma': 11.953125, 'num_boost_round': 6, 'selector': 'f_regression', 'Num selected features': 15}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:24,164] Trial 178 finished with value: 0.14404729424079338 and parameters: {'booster': 'dart', 'max_depth': 6, 'lambda': 0.2890263910022452, 'alpha': 1.4590242156305603e-06, 'learning_rate': 0.17878906249999998, 'subsample': 0.234375, 'colsample_bytree': 0.37812500000000004, 'gamma': 1.953125, 'num_boost_round': 16, 'selector': 'f_regression', 'Num selected features': 1}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:24,621] Trial 179 finished with value: 0.19368451277867998 and parameters: {'booster': 'gbtree', 'max_depth': 9, 'lambda': 9.139816994654899e-06, 'alpha': 0.04613839682733212, 'learning_rate': 0.10628906249999999, 'subsample': 0.434375, 'colsample_bytree': 0.578125, 'gamma': 16.953125, 'num_boost_round': 11, 'selector': 'f_regression', 'Num selected features': 4}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:24,700] Trial 180 finished with value: 0.34159836593826265 and parameters: {'booster': 'gbtree', 'max_depth': 4, 'lambda': 0.009139816994654913, 'alpha': 4.613839682733221e-05, 'learning_rate': 0.2512890625, 'subsample': 0.8343750000000001, 'colsample_bytree': 0.9781250000000001, 'gamma': 6.953125, 'num_boost_round': 1, 'selector': 'None', 'Num selected features': 60}. Best is trial 60 with value: 0.13770124838983366.\n",
      "[I 2024-03-29 12:46:25,061] Trial 181 finished with value: 0.13652148752963436 and parameters: {'booster': 'gbtree', 'max_depth': 5, 'lambda': 1.6253148373118664e-06, 'alpha': 8.204696109025002e-06, 'learning_rate': 0.2875390625, 'subsample': 0.934375, 'colsample_bytree': 0.6781250000000001, 'gamma': 19.453125, 'num_boost_round': 14, 'selector': 'f_regression', 'Num selected features': 8}. Best is trial 181 with value: 0.13652148752963436.\n",
      "[I 2024-03-29 12:46:25,887] Trial 182 finished with value: 0.2715934509775449 and parameters: {'booster': 'gbtree', 'max_depth': 10, 'lambda': 0.0016253148373118645, 'alpha': 0.008204696109024986, 'learning_rate': 0.1425390625, 'subsample': 0.534375, 'colsample_bytree': 0.278125, 'gamma': 9.453125, 'num_boost_round': 4, 'selector': 'f_regression', 'Num selected features': 119}. Best is trial 181 with value: 0.13652148752963436.\n",
      "[I 2024-03-29 12:46:26,378] Trial 183 finished with value: 0.16450312957393737 and parameters: {'booster': 'dart', 'max_depth': 8, 'lambda': 5.139696800771509e-05, 'alpha': 0.0002594552721404019, 'learning_rate': 0.2150390625, 'subsample': 0.334375, 'colsample_bytree': 0.478125, 'gamma': 14.453125, 'num_boost_round': 9, 'selector': 'f_regression', 'Num selected features': 30}. Best is trial 181 with value: 0.13652148752963436.\n",
      "[I 2024-03-29 12:46:27,161] Trial 184 finished with value: 0.19115147682385886 and parameters: {'booster': 'gbtree', 'max_depth': 3, 'lambda': 0.05139696800771517, 'alpha': 0.25945527214040137, 'learning_rate': 0.0700390625, 'subsample': 0.734375, 'colsample_bytree': 0.878125, 'gamma': 4.453125, 'num_boost_round': 19, 'selector': 'None', 'Num selected features': 2}. Best is trial 181 with value: 0.13652148752963436.\n",
      "[I 2024-03-29 12:46:27,528] Trial 185 finished with value: 0.17209834000153473 and parameters: {'booster': 'dart', 'max_depth': 2, 'lambda': 2.167392169568416e-05, 'alpha': 0.0006152654101490375, 'learning_rate': 0.12441406249999999, 'subsample': 0.28437500000000004, 'colsample_bytree': 0.9281250000000001, 'gamma': 18.203125, 'num_boost_round': 15, 'selector': 'f_regression', 'Num selected features': 5}. Best is trial 181 with value: 0.13652148752963436.\n",
      "[I 2024-03-29 12:46:28,248] Trial 186 finished with value: 0.18861164926381738 and parameters: {'booster': 'dart', 'max_depth': 7, 'lambda': 0.021673921695684193, 'alpha': 0.6152654101490368, 'learning_rate': 0.2694140625, 'subsample': 0.684375, 'colsample_bytree': 0.528125, 'gamma': 8.203125, 'num_boost_round': 5, 'selector': 'mutual_info_regression', 'Num selected features': 84}. Best is trial 181 with value: 0.13652148752963436.\n",
      "[I 2024-03-29 12:46:29,432] Trial 187 finished with value: 0.2874581170410465 and parameters: {'booster': 'dart', 'max_depth': 10, 'lambda': 0.0006853895838650083, 'alpha': 1.9456400615886386e-05, 'learning_rate': 0.0519140625, 'subsample': 0.8843750000000001, 'colsample_bytree': 0.328125, 'gamma': 13.203125, 'num_boost_round': 10, 'selector': 'mutual_info_regression', 'Num selected features': 21}. Best is trial 181 with value: 0.13652148752963436.\n",
      "[I 2024-03-29 12:46:30,092] Trial 188 finished with value: 0.1395435225313924 and parameters: {'booster': 'dart', 'max_depth': 5, 'lambda': 0.6853895838650087, 'alpha': 0.019456400615886348, 'learning_rate': 0.1969140625, 'subsample': 0.484375, 'colsample_bytree': 0.7281250000000001, 'gamma': 3.203125, 'num_boost_round': 20, 'selector': 'mutual_info_regression', 'Num selected features': 1}. Best is trial 181 with value: 0.13652148752963436.\n",
      "[I 2024-03-29 12:46:30,319] Trial 189 finished with value: 0.18625103866452372 and parameters: {'booster': 'dart', 'max_depth': 3, 'lambda': 0.00012188141848422887, 'alpha': 0.10941138105771851, 'learning_rate': 0.1606640625, 'subsample': 0.5843750000000001, 'colsample_bytree': 0.828125, 'gamma': 10.703125, 'num_boost_round': 8, 'selector': 'f_regression', 'Num selected features': 11}. Best is trial 181 with value: 0.13652148752963436.\n",
      "[I 2024-03-29 12:46:30,753] Trial 190 finished with value: 0.3437146116904012 and parameters: {'booster': 'gbtree', 'max_depth': 8, 'lambda': 0.12188141848422905, 'alpha': 0.00010941138105771873, 'learning_rate': 0.0156640625, 'subsample': 0.984375, 'colsample_bytree': 0.42812500000000003, 'gamma': 0.703125, 'num_boost_round': 18, 'selector': 'f_regression', 'Num selected features': 1}. Best is trial 181 with value: 0.13652148752963436.\n",
      "[I 2024-03-29 12:46:31,307] Trial 191 finished with value: 0.14173391778896494 and parameters: {'booster': 'gbtree', 'max_depth': 6, 'lambda': 3.854228868623111e-06, 'alpha': 0.003459891660869931, 'learning_rate': 0.2331640625, 'subsample': 0.784375, 'colsample_bytree': 0.22812500000000002, 'gamma': 15.703125, 'num_boost_round': 13, 'selector': 'mutual_info_regression', 'Num selected features': 3}. Best is trial 181 with value: 0.13652148752963436.\n",
      "[I 2024-03-29 12:46:31,373] Trial 192 finished with value: 0.3838065064935231 and parameters: {'booster': 'dart', 'max_depth': 1, 'lambda': 0.0038542288686231095, 'alpha': 3.4598916608699316e-06, 'learning_rate': 0.08816406249999999, 'subsample': 0.384375, 'colsample_bytree': 0.628125, 'gamma': 5.703125, 'num_boost_round': 3, 'selector': 'None', 'Num selected features': 42}. Best is trial 181 with value: 0.13652148752963436.\n",
      "[I 2024-03-29 12:46:31,496] Trial 193 finished with value: 0.3727210556652329 and parameters: {'booster': 'gbtree', 'max_depth': 1, 'lambda': 1.5678788438269714e-05, 'alpha': 0.0038542288686231095, 'learning_rate': 0.2716796875, 'subsample': 0.403125, 'colsample_bytree': 0.8968750000000001, 'gamma': 9.296875, 'num_boost_round': 1, 'selector': 'f_regression', 'Num selected features': 17}. Best is trial 181 with value: 0.13652148752963436.\n",
      "[I 2024-03-29 12:46:31,680] Trial 194 finished with value: 0.1747090214217651 and parameters: {'booster': 'gbtree', 'max_depth': 6, 'lambda': 0.01567878843826971, 'alpha': 3.854228868623111e-06, 'learning_rate': 0.1266796875, 'subsample': 0.8031250000000001, 'colsample_bytree': 0.496875, 'gamma': 19.296875, 'num_boost_round': 11, 'selector': 'f_regression', 'Num selected features': 1}. Best is trial 181 with value: 0.13652148752963436.\n",
      "[I 2024-03-29 12:46:33,141] Trial 195 finished with value: 0.1575077503178166 and parameters: {'booster': 'dart', 'max_depth': 8, 'lambda': 0.0004958068241684658, 'alpha': 0.12188141848422905, 'learning_rate': 0.1991796875, 'subsample': 0.603125, 'colsample_bytree': 0.296875, 'gamma': 4.296875, 'num_boost_round': 16, 'selector': 'None', 'Num selected features': 4}. Best is trial 181 with value: 0.13652148752963436.\n",
      "[I 2024-03-29 12:46:33,599] Trial 196 finished with value: 0.33498243780684916 and parameters: {'booster': 'dart', 'max_depth': 3, 'lambda': 0.4958068241684657, 'alpha': 0.00012188141848422887, 'learning_rate': 0.0541796875, 'subsample': 0.203125, 'colsample_bytree': 0.696875, 'gamma': 14.296875, 'num_boost_round': 6, 'selector': 'f_regression', 'Num selected features': 68}. Best is trial 181 with value: 0.13652148752963436.\n",
      "[I 2024-03-29 12:46:34,161] Trial 197 finished with value: 0.3255539213828197 and parameters: {'booster': 'gbtree', 'max_depth': 4, 'lambda': 8.816830667755714e-05, 'alpha': 2.167392169568416e-05, 'learning_rate': 0.0179296875, 'subsample': 0.30312500000000003, 'colsample_bytree': 0.996875, 'gamma': 1.796875, 'num_boost_round': 19, 'selector': 'mutual_info_regression', 'Num selected features': 2}. Best is trial 181 with value: 0.13652148752963436.\n",
      "[I 2024-03-29 12:46:34,948] Trial 198 finished with value: 0.18036622529082588 and parameters: {'booster': 'gbtree', 'max_depth': 9, 'lambda': 0.08816830667755711, 'alpha': 0.021673921695684193, 'learning_rate': 0.1629296875, 'subsample': 0.703125, 'colsample_bytree': 0.596875, 'gamma': 11.796875, 'num_boost_round': 9, 'selector': 'mutual_info_regression', 'Num selected features': 34}. Best is trial 181 with value: 0.13652148752963436.\n",
      "[I 2024-03-29 12:46:35,102] Trial 199 finished with value: 0.31829054530564244 and parameters: {'booster': 'dart', 'max_depth': 7, 'lambda': 2.788126665413135e-06, 'alpha': 0.0006853895838650083, 'learning_rate': 0.0904296875, 'subsample': 0.903125, 'colsample_bytree': 0.39687500000000003, 'gamma': 6.796875, 'num_boost_round': 4, 'selector': 'f_regression', 'Num selected features': 9}. Best is trial 181 with value: 0.13652148752963436.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPE sampling 14 trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-29 12:46:35,634] Trial 200 finished with value: 0.13945988903466883 and parameters: {'booster': 'gbtree', 'max_depth': 6, 'lambda': 0.2080845553592563, 'alpha': 8.722890975081501e-06, 'learning_rate': 0.2849341230114831, 'subsample': 0.9310522589068744, 'colsample_bytree': 0.2268821817105117, 'gamma': 15.087874227893701, 'num_boost_round': 14, 'selector': 'f_regression', 'Num selected features': 7}. Best is trial 181 with value: 0.13652148752963436.\n",
      "[I 2024-03-29 12:46:36,032] Trial 201 finished with value: 0.13939839767698095 and parameters: {'booster': 'gbtree', 'max_depth': 6, 'lambda': 0.20549251105520297, 'alpha': 1.021435565465476e-06, 'learning_rate': 0.28559315988217615, 'subsample': 0.9311300015806357, 'colsample_bytree': 0.22544194222132707, 'gamma': 15.128740148306383, 'num_boost_round': 14, 'selector': 'f_regression', 'Num selected features': 7}. Best is trial 181 with value: 0.13652148752963436.\n",
      "[I 2024-03-29 12:46:36,244] Trial 202 finished with value: 0.1389546330271571 and parameters: {'booster': 'gbtree', 'max_depth': 6, 'lambda': 0.1982406714169045, 'alpha': 8.533632371791178e-06, 'learning_rate': 0.2846771620364398, 'subsample': 0.9336966931697073, 'colsample_bytree': 0.2295116924499714, 'gamma': 15.292489385656204, 'num_boost_round': 13, 'selector': 'f_regression', 'Num selected features': 7}. Best is trial 181 with value: 0.13652148752963436.\n",
      "[I 2024-03-29 12:46:36,774] Trial 203 finished with value: 0.13765500563024333 and parameters: {'booster': 'gbtree', 'max_depth': 6, 'lambda': 0.17257984021295636, 'alpha': 8.555224721174725e-06, 'learning_rate': 0.28589234905722427, 'subsample': 0.9284075526758893, 'colsample_bytree': 0.22554592741934198, 'gamma': 15.705256796308749, 'num_boost_round': 14, 'selector': 'f_regression', 'Num selected features': 7}. Best is trial 181 with value: 0.13652148752963436.\n",
      "[I 2024-03-29 12:46:37,033] Trial 204 finished with value: 0.1384786550188135 and parameters: {'booster': 'gbtree', 'max_depth': 6, 'lambda': 0.17273060690412556, 'alpha': 7.537543768045051e-06, 'learning_rate': 0.2837459511147228, 'subsample': 0.9373947429059242, 'colsample_bytree': 0.22479028536365725, 'gamma': 15.021104304136838, 'num_boost_round': 14, 'selector': 'f_regression', 'Num selected features': 8}. Best is trial 181 with value: 0.13652148752963436.\n",
      "[I 2024-03-29 12:46:37,265] Trial 205 finished with value: 0.13783912172654092 and parameters: {'booster': 'gbtree', 'max_depth': 6, 'lambda': 0.27128034257894734, 'alpha': 7.038998830200187e-06, 'learning_rate': 0.2838332186405893, 'subsample': 0.9303126647290069, 'colsample_bytree': 0.22618051986282614, 'gamma': 15.701407823505326, 'num_boost_round': 14, 'selector': 'f_regression', 'Num selected features': 7}. Best is trial 181 with value: 0.13652148752963436.\n",
      "[I 2024-03-29 12:46:37,865] Trial 206 finished with value: 0.1385956162339695 and parameters: {'booster': 'gbtree', 'max_depth': 6, 'lambda': 0.26899364913476354, 'alpha': 8.645636473434325e-06, 'learning_rate': 0.28598638897037804, 'subsample': 0.9351424600598679, 'colsample_bytree': 0.22899392475094227, 'gamma': 15.438004068663203, 'num_boost_round': 14, 'selector': 'f_regression', 'Num selected features': 7}. Best is trial 181 with value: 0.13652148752963436.\n",
      "[I 2024-03-29 12:46:38,156] Trial 207 finished with value: 0.13782977651733472 and parameters: {'booster': 'gbtree', 'max_depth': 6, 'lambda': 0.27895664330513603, 'alpha': 8.068753368191515e-06, 'learning_rate': 0.28597303120800677, 'subsample': 0.9376626874166798, 'colsample_bytree': 0.22181490640370216, 'gamma': 15.669097730481015, 'num_boost_round': 14, 'selector': 'f_regression', 'Num selected features': 7}. Best is trial 181 with value: 0.13652148752963436.\n",
      "[I 2024-03-29 12:46:38,615] Trial 208 finished with value: 0.13711506447112337 and parameters: {'booster': 'gbtree', 'max_depth': 6, 'lambda': 0.26730152506418237, 'alpha': 8.542784251822812e-06, 'learning_rate': 0.28707609168026643, 'subsample': 0.9263555979810499, 'colsample_bytree': 0.2233377474006385, 'gamma': 15.705336061974476, 'num_boost_round': 14, 'selector': 'f_regression', 'Num selected features': 7}. Best is trial 181 with value: 0.13652148752963436.\n",
      "[I 2024-03-29 12:46:38,841] Trial 209 finished with value: 0.1384911439117282 and parameters: {'booster': 'gbtree', 'max_depth': 6, 'lambda': 0.23540354384090345, 'alpha': 8.501251575847557e-06, 'learning_rate': 0.2860682441319969, 'subsample': 0.9304904179562141, 'colsample_bytree': 0.22575817986188032, 'gamma': 15.667657374508039, 'num_boost_round': 14, 'selector': 'f_regression', 'Num selected features': 7}. Best is trial 181 with value: 0.13652148752963436.\n",
      "[I 2024-03-29 12:46:39,077] Trial 210 finished with value: 0.13820152482782957 and parameters: {'booster': 'gbtree', 'max_depth': 6, 'lambda': 0.2858014712430646, 'alpha': 7.91159348925858e-06, 'learning_rate': 0.2843624806550758, 'subsample': 0.933560381828423, 'colsample_bytree': 0.22606247603531066, 'gamma': 15.846136924550283, 'num_boost_round': 14, 'selector': 'f_regression', 'Num selected features': 7}. Best is trial 181 with value: 0.13652148752963436.\n",
      "[I 2024-03-29 12:46:39,360] Trial 211 finished with value: 0.1375121724668115 and parameters: {'booster': 'gbtree', 'max_depth': 6, 'lambda': 0.23729705133540116, 'alpha': 8.951676392153222e-06, 'learning_rate': 0.2860681988553049, 'subsample': 0.9270829837128127, 'colsample_bytree': 0.23013107503414346, 'gamma': 15.677625779976426, 'num_boost_round': 14, 'selector': 'f_regression', 'Num selected features': 7}. Best is trial 181 with value: 0.13652148752963436.\n",
      "[I 2024-03-29 12:46:39,590] Trial 212 finished with value: 0.13820141360933905 and parameters: {'booster': 'gbtree', 'max_depth': 6, 'lambda': 0.1706814025857674, 'alpha': 7.609183977040103e-06, 'learning_rate': 0.2878908998325043, 'subsample': 0.9356656411091716, 'colsample_bytree': 0.2288110985106836, 'gamma': 15.727086347118654, 'num_boost_round': 14, 'selector': 'f_regression', 'Num selected features': 7}. Best is trial 181 with value: 0.13652148752963436.\n",
      "[I 2024-03-29 12:46:39,953] Trial 213 finished with value: 0.13743482398692136 and parameters: {'booster': 'gbtree', 'max_depth': 6, 'lambda': 0.17323950616659914, 'alpha': 9.071011077334278e-06, 'learning_rate': 0.2856269530888535, 'subsample': 0.9324200191583448, 'colsample_bytree': 0.226628807169228, 'gamma': 16.000339473230234, 'num_boost_round': 14, 'selector': 'f_regression', 'Num selected features': 7}. Best is trial 181 with value: 0.13652148752963436.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for XGBoost optimization: 0:01:51.911680\n",
      "Time taken for one trial: 0:00:00.522953\n",
      "Best val: 0.13652148752963436\n",
      "Best params: {'booster': 'gbtree', 'max_depth': 5, 'lambda': 1.6253148373118664e-06, 'alpha': 8.204696109025002e-06, 'learning_rate': 0.2875390625, 'subsample': 0.934375, 'colsample_bytree': 0.6781250000000001, 'gamma': 19.453125, 'num_boost_round': 14, 'selector': 'f_regression', 'Num selected features': 8}\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[12:46:39] /home/conda/feedstock_root/build_artifacts/xgboost-split_1705650282415/work/src/learner.cc:724: Invalid parameter \"Num selected features\" contains whitespace.\nStack trace:\n  [bt] (0) /home/tobias/miniconda3/envs/tf/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x71) [0x7fd6a8a54cf1]\n  [bt] (1) /home/tobias/miniconda3/envs/tf/lib/libxgboost.so(xgboost::LearnerConfiguration::ValidateParameters()+0xaea) [0x7fd6a8d63b7a]\n  [bt] (2) /home/tobias/miniconda3/envs/tf/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x8bf) [0x7fd6a8d65e2f]\n  [bt] (3) /home/tobias/miniconda3/envs/tf/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x70) [0x7fd6a8d67390]\n  [bt] (4) /home/tobias/miniconda3/envs/tf/lib/libxgboost.so(XGBoosterUpdateOneIter+0x74) [0x7fd6a8a37834]\n  [bt] (5) /home/tobias/miniconda3/envs/tf/lib/python3.9/lib-dynload/../../libffi.so.8(+0xa052) [0x7fdc072fd052]\n  [bt] (6) /home/tobias/miniconda3/envs/tf/lib/python3.9/lib-dynload/../../libffi.so.8(+0x8925) [0x7fdc072fb925]\n  [bt] (7) /home/tobias/miniconda3/envs/tf/lib/python3.9/lib-dynload/../../libffi.so.8(ffi_call+0xde) [0x7fdc072fc06e]\n  [bt] (8) /home/tobias/miniconda3/envs/tf/lib/python3.9/lib-dynload/_ctypes.cpython-39-x86_64-linux-gnu.so(+0x91e0) [0x7fdc070241e0]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 205\u001b[0m\n\u001b[1;32m    203\u001b[0m X_train_combined_selected, X_test_combined_selected, selected_features_names \u001b[38;5;241m=\u001b[39m select_features(X_train_combined, X_test_combined, y_train, combined_feature_names, study\u001b[38;5;241m.\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselector\u001b[39m\u001b[38;5;124m'\u001b[39m], study\u001b[38;5;241m.\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNum selected features\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    204\u001b[0m dtrain \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mDMatrix(X_train_combined_selected, label\u001b[38;5;241m=\u001b[39my_train, feature_names\u001b[38;5;241m=\u001b[39mselected_features_names)\n\u001b[0;32m--> 205\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43mxgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_boost_round\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSelected features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mselected_features_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# Testataan mallia koulutusdatalla jotta voidaan arvioida overfittingia\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/xgboost/core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/xgboost/core.py:2050\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2047\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2050\u001b[0m     \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2051\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2056\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/xgboost/core.py:282\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \n\u001b[1;32m    273\u001b[0m \u001b[38;5;124;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;03m    return value from API calls\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[38;5;241m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [12:46:39] /home/conda/feedstock_root/build_artifacts/xgboost-split_1705650282415/work/src/learner.cc:724: Invalid parameter \"Num selected features\" contains whitespace.\nStack trace:\n  [bt] (0) /home/tobias/miniconda3/envs/tf/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x71) [0x7fd6a8a54cf1]\n  [bt] (1) /home/tobias/miniconda3/envs/tf/lib/libxgboost.so(xgboost::LearnerConfiguration::ValidateParameters()+0xaea) [0x7fd6a8d63b7a]\n  [bt] (2) /home/tobias/miniconda3/envs/tf/lib/libxgboost.so(xgboost::LearnerConfiguration::Configure()+0x8bf) [0x7fd6a8d65e2f]\n  [bt] (3) /home/tobias/miniconda3/envs/tf/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, std::shared_ptr<xgboost::DMatrix>)+0x70) [0x7fd6a8d67390]\n  [bt] (4) /home/tobias/miniconda3/envs/tf/lib/libxgboost.so(XGBoosterUpdateOneIter+0x74) [0x7fd6a8a37834]\n  [bt] (5) /home/tobias/miniconda3/envs/tf/lib/python3.9/lib-dynload/../../libffi.so.8(+0xa052) [0x7fdc072fd052]\n  [bt] (6) /home/tobias/miniconda3/envs/tf/lib/python3.9/lib-dynload/../../libffi.so.8(+0x8925) [0x7fdc072fb925]\n  [bt] (7) /home/tobias/miniconda3/envs/tf/lib/python3.9/lib-dynload/../../libffi.so.8(ffi_call+0xde) [0x7fdc072fc06e]\n  [bt] (8) /home/tobias/miniconda3/envs/tf/lib/python3.9/lib-dynload/_ctypes.cpython-39-x86_64-linux-gnu.so(+0x91e0) [0x7fdc070241e0]\n\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "import time\n",
    "from tensorflow.keras.models import Model\n",
    "import glob\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "\n",
    "\n",
    "# study_name = 'rmsle5_layers_0328'\n",
    "study_name = 'rmsle5_random_2503'\n",
    "\n",
    "num_random = 100\n",
    "num_tpe = 14\n",
    "\n",
    "time_started_xgb = time.time()\n",
    "\n",
    "X_train_features_list = []\n",
    "X_test_features_list = []\n",
    "features_names_list = []\n",
    "\n",
    "def rmsle_loss(y_true, y_pred):\n",
    "    penalty = tf.constant(1e5, dtype=tf.float32)\n",
    "    valid_mask = tf.math.greater(y_pred, 0.0)\n",
    "    safe_y_pred = tf.where(valid_mask, y_pred, penalty)\n",
    "    rmsle = tf.sqrt(tf.reduce_mean(tf.square(tf.math.log1p(safe_y_pred) - tf.math.log1p(y_true))))\n",
    "    return tf.where(tf.reduce_any(~valid_mask), penalty, rmsle)\n",
    "custom_objects = {\"rmsle_loss\": rmsle_loss}\n",
    "\n",
    "model_best_vals = []\n",
    "best_optuna_models = []\n",
    "\n",
    "folds = 5\n",
    "# for fold_num in [1]:\n",
    "for fold_num in range(folds): # TODO testiä parhailla malleilla\n",
    "    patterns = [\n",
    "        f\"./NN_search/{study_name}_best_foldmodel{fold_num}_score_*.h5\",\n",
    "        f\"./NN_search/{study_name}_foldmodel{fold_num}_score_*.h5\"\n",
    "    ]\n",
    "    \n",
    "    best_score = float('inf')\n",
    "    best_model_file = None\n",
    "    for pattern in patterns:\n",
    "        model_files = glob.glob(pattern)\n",
    "        for model_file in model_files:\n",
    "            score_part = model_file.split('_score_')[1]\n",
    "            score = float(score_part.split('_')[0])\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_model_file = model_file\n",
    "                    \n",
    "    model_best_vals.append(best_score)    \n",
    "    # Lataa parhaan mallin tiedosto\n",
    "    if best_model_file:\n",
    "        best_model = load_model(best_model_file, custom_objects=custom_objects)\n",
    "        best_optuna_models.append(best_model)\n",
    "        print(f\"Loaded best model for fold {fold_num} from {best_model_file} with score {best_score:.4f}\")\n",
    "    else:\n",
    "        print(f\"No model files found for fold {fold_num} matching pattern {pattern}\")\n",
    "\n",
    "\n",
    "# best_models_per_fold-listas\n",
    "\n",
    "original_feature_names = list(X_train.columns) \n",
    "\n",
    "for idx, model in enumerate(best_optuna_models):\n",
    "    feature_extractor = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "    X_train_features = feature_extractor.predict(X_train_NN)\n",
    "    X_test_features = feature_extractor.predict(X_test_NN)\n",
    "    \n",
    "    X_train_features_list.append(X_train_features)\n",
    "    X_test_features_list.append(X_test_features)\n",
    "\n",
    "    print(f'Model train feature shape: {X_train_features.shape}')\n",
    "    print(f'Model test feature shape: {X_test_features.shape}')\n",
    "\n",
    "    num_features = X_train_features.shape[1]\n",
    "    model_feature_names = [f\"model_{idx}_feature_{feature_idx}\" for feature_idx in range(num_features)]\n",
    "    features_names_list.extend(model_feature_names)\n",
    "\n",
    "combined_feature_names = original_feature_names + features_names_list\n",
    "print(f'Train combined feature shape: {X_train_features.shape}')\n",
    "print(f'Test combined feature shape: {X_test_features.shape}')\n",
    "\n",
    "# Yhdistetään ominaisuusvektorit\n",
    "X_train_combined = np.concatenate(X_train_features_list, axis=1)\n",
    "X_test_combined = np.concatenate(X_test_features_list, axis=1)\n",
    "\n",
    "X_train_combined = np.concatenate([X_train_combined, X_train], axis=1)   \n",
    "X_test_combined = np.concatenate([X_test_combined, X_test], axis=1)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "\n",
    "## Ei valintaa \n",
    "\n",
    "# X_train_combined_selected = X_train_combined\n",
    "# X_test_combined_selected = X_test_combined\n",
    "# selected_features_names = combined_feature_names\n",
    "\n",
    "\n",
    "def select_features(X_train_combined, X_test_combined, y_train, combined_feature_names, method, max_feature):\n",
    "    \n",
    "    if method == 'f_regression':\n",
    "        method_function = f_regression\n",
    "    elif method == 'mutual_info_regression':\n",
    "        method_function = mutual_info_regression\n",
    "    else:\n",
    "        method_function = None\n",
    "\n",
    "    if method_function is not None:\n",
    "        selector = SelectKBest(method_function, k=max_feature)\n",
    "        X_train_combined_selected = selector.fit_transform(X_train_combined, y_train)\n",
    "        X_test_combined_selected = selector.transform(X_test_combined)\n",
    "        selected_indices = selector.get_support(indices=True)\n",
    "        selected_features_names = np.array(combined_feature_names)[selected_indices]\n",
    "        selected_features_names = selected_features_names.tolist()\n",
    "        \n",
    "        return X_train_combined_selected, X_test_combined_selected, selected_features_names\n",
    "    else:\n",
    "        return X_train_combined, X_test_combined, combined_feature_names\n",
    "\n",
    "\n",
    "def rmsle(y_true, y_pred):\n",
    "    if np.any(y_pred <= 0):\n",
    "        return 1e6\n",
    "    return np.sqrt(np.mean(np.square(np.log1p(y_pred) - np.log1p(y_true))))\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Määritetään parametrit, jotka optimoidaan\n",
    "    param = {\n",
    "        # 'device' : 'cuda',\n",
    "        # \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"dart\"]),\n",
    "        \"booster\": 'dart',\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"dart\"]),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-6, 1.0, log = True),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-6, 1.0, log = True),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-2, 0.3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.2, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 20)\n",
    "    }\n",
    "\n",
    "    num_boost_round = trial.suggest_int('num_boost_round', 1, 20)  \n",
    "    selector = trial.suggest_categorical('selector', choices = ['f_regression', 'mutual_info_regression', 'None'])\n",
    "\n",
    "    num_selected = trial.suggest_int('num_selected_features', 1, X_train_combined.shape[1], log = True)\n",
    "    X_train_combined_selected, _ , selected_features_names = select_features(X_train_combined, X_test_combined, y_train, combined_feature_names, selector, num_selected)\n",
    "    \n",
    "    kf = KFold(n_splits=5)\n",
    "    rmsle_scores = []\n",
    "\n",
    "    dtrain_full = xgb.DMatrix(X_train_combined_selected, label=y_train, feature_names=selected_features_names)\n",
    "\n",
    "    for train_index, val_index in kf.split(X_train_combined_selected):\n",
    "        \n",
    "        dtrain = dtrain_full.slice(train_index)\n",
    "        dval = dtrain_full.slice(val_index)\n",
    "\n",
    "        evals_result = {}\n",
    "        bst = xgb.train(param, dtrain, num_boost_round=num_boost_round, evals=[(dval, 'val')], evals_result=evals_result, verbose_eval=False, early_stopping_rounds=1000)\n",
    "        best_iteration = bst.best_iteration\n",
    "        preds = bst.predict(dval, iteration_range=(0, best_iteration + 1))\n",
    "        y_true = y_train[val_index]\n",
    "        # loss = r2_score(y_true, preds)\n",
    "        loss = rmsle(y_true, preds)\n",
    "        rmsle_scores.append(loss)\n",
    "\n",
    "    average_rmsle = np.mean(rmsle_scores)\n",
    "    return average_rmsle\n",
    "\n",
    "# study = optuna.create_study(direction='maximize', \n",
    "#                             storage='sqlite:///tampere_reg.db', \n",
    "#                             study_name='0326_xgb_comb_R2', # TODO muuta nimeä tarvittaessa\n",
    "#                             load_if_exists=False) \n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "study.sampler = optuna.samplers.QMCSampler(warn_independent_sampling = False)\n",
    "print(f'Random sampling {num_random} trials...')\n",
    "study.optimize(objective, n_trials=num_random)\n",
    "study.sampler = optuna.samplers.TPESampler()\n",
    "print(f'TPE sampling {num_tpe} trials...')\n",
    "study.optimize(objective, n_trials=num_tpe)\n",
    "\n",
    "print(f'Time taken for XGBoost optimization: {str(timedelta(seconds=(time.time() - time_started_xgb)))}')\n",
    "print(f'Time taken for one trial: {str(timedelta(seconds=(time.time() - time_started_xgb) / (num_random + num_tpe)))}')\n",
    "\n",
    "# Parhaiden parametrien tulostus ja mallin koulutus\n",
    "print(f\"Best val: {study.best_trial.value}\")\n",
    "print(f'Best params: {study.best_params}')\n",
    "\n",
    "X_train_combined_selected, X_test_combined_selected, selected_features_names = select_features(X_train_combined, X_test_combined, y_train, combined_feature_names, study.best_params['selector'], study.best_params['num_selected_features'])\n",
    "dtrain = xgb.DMatrix(X_train_combined_selected, label=y_train, feature_names=selected_features_names)\n",
    "best_model = xgb.train(study.best_params, dtrain, num_boost_round=study.best_params['num_boost_round'])\n",
    "print(f'Selected features: {selected_features_names}')\n",
    "\n",
    "# Testataan mallia koulutusdatalla jotta voidaan arvioida overfittingia\n",
    "pred_train = best_model.predict(dtrain)\n",
    "mae_train = mean_absolute_error(y_train, pred_train)\n",
    "mse_train = mean_squared_error(y_train, pred_train)\n",
    "r2_train = r2_score(y_train, pred_train)\n",
    "rmsle_train = rmsle(y_train, pred_train)\n",
    "print(f\"Train MAE: {mae_train}, Train MSE: {mse_train}, Train R2: {r2_train}, Train RMSLE: {rmsle_train}\")\n",
    "\n",
    "dtest = xgb.DMatrix(X_test_combined_selected, label=y_test, feature_names=selected_features_names)\n",
    "predictions = best_model.predict(dtest)\n",
    "\n",
    "# Visualisoidaan ennustettuja arvoja verrattuna todellisiin arvoihin\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(y_test, predictions, edgecolors=(0, 0, 0))\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "plt.xlabel('Measured')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Measured vs. Predicted Values')\n",
    "plt.show()\n",
    "\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "rmsle_val = rmsle(y_test, predictions)\n",
    "print(f\"MAE: {mae}, MSE: {mse}, R2: {r2}, RMSLE: {rmsle_val}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 30))\n",
    "xgb.plot_importance(best_model, importance_type='weight', ax=ax)\n",
    "ax.set_title('Feature Importance by Weight', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Asetetaan toisen kuvaajan koko\n",
    "fig, ax = plt.subplots(figsize=(20, 30))\n",
    "xgb.plot_importance(best_model, importance_type='gain', ax=ax)\n",
    "ax.set_title('Feature Importance by Gain', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Asetetaan kolmannen kuvaajan koko\n",
    "fig, ax = plt.subplots(figsize=(20, 30))\n",
    "xgb.plot_importance(best_model, importance_type='cover', ax=ax)\n",
    "ax.set_title('Feature Importance by Cover', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Testataan vielä minkälaisia tuloksia vain NN mallien ennustuksilla ### \n",
    "\n",
    "predictions_train = []\n",
    "\n",
    "for idx, model in enumerate(best_optuna_models):\n",
    "    pred = model.predict(X_train_NN, verbose=0).flatten()\n",
    "    predictions_train.append(pred)\n",
    "\n",
    "predictions_test = []\n",
    "\n",
    "for idx, model in enumerate(best_optuna_models):\n",
    "    pred = model.predict(X_test_NN, verbose=0).flatten()\n",
    "    predictions_test.append(pred)\n",
    "\n",
    "### Keskiarvo \n",
    "print('Keskiarvo ')\n",
    "predictions_mean = np.mean(predictions_test, axis=0)\n",
    "mse = mean_squared_error(y_test, predictions_mean)\n",
    "mae = mean_absolute_error(y_test, predictions_mean)\n",
    "r2 = r2_score(y_test, predictions_mean)\n",
    "rmsle_val = rmsle(y_test, predictions_mean)  \n",
    "print(f\"MAE: {mae}, MSE: {mse}, R2: {r2}, RMSLE: {rmsle_val}\\n\\n\")\n",
    "\n",
    "### Painotettu keskiarvo\n",
    "print('Painotettu keskiarvo käänteisillä')\n",
    "best_rmsle = float('inf')\n",
    "for pot in range (1,20):\n",
    "    weights = [1 / x**pot for x in model_best_vals]\n",
    "    w_sum = sum(weights)\n",
    "    weights = [x / w_sum for x in weights]\n",
    "    weighted_predictions = np.average(predictions_test, axis=0, weights=weights)\n",
    "    mse = mean_squared_error(y_test, weighted_predictions)\n",
    "    mae = mean_absolute_error(y_test, weighted_predictions)\n",
    "    r2 = r2_score(y_test, weighted_predictions)\n",
    "    rmsle_val = rmsle(y_test, weighted_predictions)  # Oletetaan että sinulla on rmsle funktio määritelty\n",
    "    if rmsle_val < best_rmsle:\n",
    "        best_rmsle = rmsle_val\n",
    "        bestpot = pot\n",
    "print(f'Paras arvo löytyi potenssilla {bestpot} arvolla {best_rmsle}')\n",
    "        \n",
    "\n",
    "### Lineaarinen regressio \n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Oletetaan, että `predictions` on lista, joka sisältää kunkin mallin ennusteet testidatasetille\n",
    "X_meta_train = np.stack(predictions_train, axis=1)\n",
    "X_meta_test = np.stack(predictions_test, axis=1)\n",
    "# Koulutetaan meta-malli\n",
    "meta_model = LinearRegression()\n",
    "meta_model.fit(X_meta_train, y_train)\n",
    "\n",
    "# Käytetään meta-mallia ennustamaan\n",
    "linear_predictions = meta_model.predict(X_meta_test)\n",
    "\n",
    "print('Linear meta')\n",
    "mse = mean_squared_error(y_test, linear_predictions)\n",
    "mae = mean_absolute_error(y_test, linear_predictions)\n",
    "r2 = r2_score(y_test, linear_predictions)\n",
    "rmsle_val = rmsle(y_test, linear_predictions)  # Oletetaan että sinulla on rmsle funktio määritelty\n",
    "print(f\"MAE: {mae}, MSE: {mse}, R2: {r2}, RMSLE: {rmsle_val}\\n\\n\")\n",
    "\n",
    "### XGBoost \n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "time_xgb = time.time()\n",
    "\n",
    "NN_names = [f'NN_{i}' for i in range(len(best_optuna_models))]\n",
    "\n",
    "X_train_XGB = np.column_stack(predictions_train)\n",
    "X_test_XGB = np.column_stack(predictions_test)\n",
    "\n",
    "def objective(trial):\n",
    "    # XGBoostin parametrit, jotka optimoidaan\n",
    "    param = {        \n",
    "        'objective': 'reg:absoluteerror',      \n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"dart\"]),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-6, 1.0, log = True),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-6, 1.0, log = True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 6),\n",
    "        \"learning_rate\": trial.suggest_float(\"eta\", 1e-2, 1.0),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 10)\n",
    "    }\n",
    "\n",
    "    num_boost_round = trial.suggest_int('num_boost_round', 1, 142)  \n",
    "\n",
    "    kf = KFold(n_splits=5)\n",
    "    rmsle_scores = []\n",
    "\n",
    "    dtrain_full = xgb.DMatrix(X_train_XGB, label=y_train, feature_names=NN_names)\n",
    "\n",
    "    for train_index, val_index in kf.split(X_train_XGB):\n",
    "        dtrain = dtrain_full.slice(train_index)\n",
    "        dval = dtrain_full.slice(val_index)\n",
    "\n",
    "        evals_result = {}\n",
    "        bst = xgb.train(param, dtrain, num_boost_round=num_boost_round, evals=[(dval, 'val')], evals_result=evals_result, verbose_eval=False, early_stopping_rounds=200)\n",
    "        \n",
    "        best_iteration = bst.best_iteration\n",
    "        preds = bst.predict(dval, iteration_range=(0, best_iteration + 1))\n",
    "        y_true = y_train[val_index]\n",
    "        loss = r2_score(y_true, preds)\n",
    "        # loss = rmsle(y_true, preds)\n",
    "        rmsle_scores.append(loss)\n",
    "        \n",
    "    return np.mean(rmsle_scores)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.sampler = optuna.samplers.QMCSampler(warn_independent_sampling = False)\n",
    "study.optimize(objective, n_trials=42)\n",
    "print(f'Random sampling trials...')\n",
    "study.sampler = optuna.samplers.TPESampler()\n",
    "print(f'TPE sampling trials...')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(f\"RMSLE: {trial.value}\")\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))\n",
    "\n",
    "# Koulutetaan paras malli uudelleen koko datasetillä\n",
    "best_params = trial.params\n",
    "dtrain = xgb.DMatrix(X_train_XGB, label=y_train, feature_names= NN_names)\n",
    "final_model = xgb.train(best_params, dtrain)\n",
    "\n",
    "dtest = xgb.DMatrix(X_test_XGB, label=y_test, feature_names=NN_names)\n",
    "\n",
    "predictions_XGB = final_model.predict(dtest)\n",
    "mse = mean_squared_error(y_test, predictions_XGB)\n",
    "mae = mean_absolute_error(y_test, predictions_XGB)\n",
    "r2 = r2_score(y_test, predictions_XGB)\n",
    "rmsle_val = rmsle(y_test, predictions_XGB)  \n",
    "\n",
    "print(f\"Parhaan mallin tulokset testidatalla:\")\n",
    "print(f\"MAE: {mae}, MSE: {mse}, R2: {r2}, RMSLE: {rmsle_val}\")\n",
    "print(f'Time taken for XGBoost optimization: {str(timedelta(seconds=(time.time() - time_xgb)))}')\n",
    "\n",
    "# Visualisoidaan ennustettuja arvoja verrattuna todellisiin arvoihin\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(y_test, predictions_XGB, edgecolors=(0, 0, 0))\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "plt.xlabel('Measured')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Measured vs. Predicted Values')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 30))\n",
    "xgb.plot_importance(final_model, importance_type='weight', ax=ax)\n",
    "ax.set_title('Feature Importance by Weight', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Asetetaan toisen kuvaajan koko\n",
    "fig, ax = plt.subplots(figsize=(20, 30))\n",
    "xgb.plot_importance(final_model, importance_type='gain', ax=ax)\n",
    "ax.set_title('Feature Importance by Gain', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Asetetaan kolmannen kuvaajan koko\n",
    "fig, ax = plt.subplots(figsize=(20, 30))\n",
    "xgb.plot_importance(final_model, importance_type='cover', ax=ax)\n",
    "ax.set_title('Feature Importance by Cover', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "joda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
