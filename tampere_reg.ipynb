{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_train_filtered = pd.read_pickle('./data/df_train_filtered.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_train_filtered.drop('Hinta', axis=1)\n",
    "y = df_train_filtered['Hinta']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=df_train_filtered['Kaupunginosa'], random_state=42)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score,  mean_absolute_error\n",
    "\n",
    "def rmsle_score(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(np.log1p(y_true+1), np.log1p(y_pred+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "predictions = lr.predict(X_test)\n",
    "\n",
    "r2 = r2_score(y_test, predictions)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print(f\"Mean squared error: {mse:.2f}\\nMean absolute error: {mae:.2f}\\nR²-arvo: {r2:.2f}\\nRMSLE: {rmsle_score(y_test, predictions):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "ax.scatter(y_test, predictions, edgecolors=(0, 0, 0))\n",
    "ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Measured')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.title('Measured vs. Predicted Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def rmsle(y_true, y_pred):\n",
    "\n",
    "    if np.any(y_pred <= 0):\n",
    "        return 1e6\n",
    "\n",
    "    return np.sqrt(np.mean(np.square(np.log1p(y_pred) - np.log1p(y_true))))\n",
    "\n",
    "rmsle_scorer = make_scorer(rmsle, greater_is_better=False)\n",
    "\n",
    " \n",
    "virhe_mallit = [xgboost.XGBRegressor(objective='reg:absoluteerror'), xgboost.XGBRegressor(objective='reg:squarederror')]\n",
    "virhe_nimi = ['neg_mean_absolute_error', 'neg_mean_squared_error']\n",
    "\n",
    "\n",
    "\n",
    "for idx, malli in enumerate(virhe_mallit):\n",
    "    time_start = time.time()\n",
    "    param_space = {\n",
    "        'n_estimators': np.arange(1, 500, 10),\n",
    "        'max_depth': np.arange(3, 11),\n",
    "        'learning_rate': [0.1, 0.01, 0.001],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "        'gamma': [0, 1, 5],\n",
    "        'reg_alpha': [0, 0.1, 0.5],\n",
    "        'reg_lambda': [1, 1.5, 2]\n",
    "    }\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=malli,\n",
    "        param_distributions=param_space,\n",
    "        cv=5,\n",
    "        n_jobs=-2,\n",
    "        n_iter= 1,\n",
    "        verbose=1,\n",
    "        scoring=rmsle_scorer\n",
    "    )\n",
    "\n",
    "\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    best_index = random_search.best_index_\n",
    "    cv_results = random_search.cv_results_\n",
    "    cv_splits = random_search.cv\n",
    "    best_scores = [cv_results[f'split{i}_test_score'][best_index] for i in range(cv_splits)]\n",
    "\n",
    "\n",
    "    print(f\"With error: {virhe_nimi[idx]}\")\n",
    "    for i, score in enumerate(best_scores):\n",
    "        print(f\"Ositus {i}: {-score}\")\n",
    "\n",
    "    best_model = random_search.best_estimator_\n",
    "    predictions = best_model.predict(X_test)\n",
    "\n",
    "    time_end = time.time()\n",
    "\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    rmsle = rmsle_score(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)    \n",
    "    print(f\"Mean squared error: {mse:.2f}\\nMean absolute error: {mae:.2f}\\nRMSLE: {rmsle:.4f}\\nParhaan mallin R²-arvo: {r2:.4f}\")\n",
    "    print(f\"Time taken: {str(timedelta(seconds=(time_end - time_start)))}\")\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.scatter(y_test, predictions, edgecolors=(0, 0, 0))\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "    plt.xlabel('Measured')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.title(f'{virhe_nimi[idx]} Measured vs. Predicted Values')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler \n",
    "\n",
    "# Skaalataan numeeriset muuttujat\n",
    "robust_scaler = RobustScaler()\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "df_train_NN = df_train_filtered.copy()\n",
    "df_train_NN[['Pituusaste', 'Leveysaste']] = minmax_scaler.fit_transform(df_train_NN[['Pituusaste', 'Leveysaste']])\n",
    "df_train_NN['Rv'] = minmax_scaler.fit_transform(df_train_NN[['Rv']])\n",
    "df_train_NN['m2'] = minmax_scaler.fit_transform(df_train_NN[['m2']])\n",
    "\n",
    "# One hot koodataan kategoriset muuttujat\n",
    "df_hot = pd.get_dummies(df_train_NN['Kaupunginosa'], prefix='Kaupunginosa').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN['kerros'], prefix='kerros').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN['max_kerros'], prefix='max_kerros').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN['Kunto'], prefix='Kunto').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN['Hissi'], prefix='Hissi').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN['Asunnon tyyppi'], prefix='Asunnon tyyppi').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN[\"Talot.\"], prefix='Talot.').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "\n",
    "df_train_NN.drop(['Kaupunginosa', 'kerros', 'max_kerros', 'Kunto', 'Hissi', 'Asunnon tyyppi', \"Talot.\"], axis=1, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muodostetaan X ja y sekä jaetaan data harjoitus- ja testijoukkoihin\n",
    "\n",
    "X = df_train_NN.drop('Hinta', axis=1)\n",
    "y = df_train_NN['Hinta']\n",
    "\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy().astype('float32')\n",
    "\n",
    "X_train_NN, X_test_NN, y_train_NN, y_test_NN = train_test_split(X, y, test_size=0.1, random_state=42, stratify=df_train_filtered['Kaupunginosa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from keras import regularizers, layers, optimizers, initializers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "import time \n",
    "from optuna.integration import TFKerasPruningCallback\n",
    "import os \n",
    "import pickle \n",
    "\n",
    "\n",
    "# Haun nimi\n",
    "study_name = 'tampere_rmsle5'\n",
    "# Montako osittelua käytettiin\n",
    "folds = 5\n",
    "# Montako epochia kullekin osittelulle\n",
    "epochs_search = 100\n",
    "# Montako satunnaista hakua kieroksella\n",
    "num_random = 42\n",
    "# Montako TPE hakua kieroksella\n",
    "num_tpe = 42\n",
    "\n",
    "# Aika sekuntteina jota hakuun käytetän\n",
    "max_search_time = 3600\n",
    "# Neuroneiden maksimimäärä \n",
    "max_units = 512\n",
    "\n",
    "def rmsle_loss(y_true, y_pred):\n",
    "    # Asetetaan suuri rangaistusarvo, jos y_pred sisältää arvon nolla tai alle\n",
    "    penalty = tf.constant(1e5, dtype=tf.float32)\n",
    "    \n",
    "    # Maski, joka on tosi, kun y_pred on > 0\n",
    "    valid_mask = tf.math.greater(y_pred, 0.0)\n",
    "    \n",
    "    # Käytä maskia valitsemaan joko oikea RMSLE laskenta tai suuri rangaistus\n",
    "    safe_y_pred = tf.where(valid_mask, y_pred, penalty)\n",
    "    \n",
    "    # Laske RMSLE vain, jos y_pred on suurempi kuin 0, muuten palauta rangaistus\n",
    "    rmsle = tf.sqrt(tf.reduce_mean(tf.square(tf.math.log1p(safe_y_pred) - tf.math.log1p(y_true))))\n",
    "    \n",
    "    # Palauta suuri rangaistus, jos y_pred sisälsi nollan tai negatiivisen arvon\n",
    "    return tf.where(tf.reduce_any(~valid_mask), penalty, rmsle)\n",
    "\n",
    "\n",
    "def create_model(trial):\n",
    "        \n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=(X_train_NN.shape[1],)))\n",
    "    \n",
    "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "    max_units_previous_layer = max_units\n",
    "    \n",
    "    for i in range(n_layers):\n",
    "        num_units = trial.suggest_int(f'n_units_{i}', 4, min(max_units_previous_layer, 512))\n",
    "        max_units_previous_layer = num_units\n",
    "        \n",
    "        dropout_rate = trial.suggest_float(f'dropout_{i}', 0.0, 0.5)\n",
    "        kernel_regularizer=regularizers.l1_l2(\n",
    "            l1= trial.suggest_float(f'l1_reg_{i}', 1e-6, 1, log=True),\n",
    "            l2= trial.suggest_float(f'l2_reg_{i}', 1e-6, 1, log=True)\n",
    "        )\n",
    "\n",
    "        activation = trial.suggest_categorical(f'activation_{i}', ['relu', 'tanh', 'selu', 'linear', 'sigmoid', 'elu'])\n",
    "        \n",
    "        model.add(keras.layers.Dense(num_units, activation=activation, kernel_regularizer=kernel_regularizer))    \n",
    "        model.add(keras.layers.Dropout(rate=dropout_rate))\n",
    "    \n",
    "    num_last = trial.suggest_int('n_units_last', 1, 16)\n",
    "    dropout_last = trial.suggest_float('dropout_last', 0.0, 0.5)\n",
    "    activation_last = trial.suggest_categorical('activation_last', ['relu', 'tanh', 'selu', 'linear', 'sigmoid', 'elu'])\n",
    "    kernel_regularizer_last = regularizers.l1_l2( \n",
    "        l1= trial.suggest_float('l1_reg_last', 1e-6, 1, log=True),\n",
    "        l2= trial.suggest_float('l2_reg_last', 1e-6, 1, log=True)\n",
    "    )\n",
    "    model.add(keras.layers.Dense(num_last, activation=activation_last, kernel_regularizer=kernel_regularizer_last))        \n",
    "    model.add(keras.layers.Dropout(rate=dropout_last))\n",
    "    model.add(keras.layers.Dense(1, activation='linear')) \n",
    "    \n",
    "    # Optimisaattorin ja oppimisnopeuden valinta\n",
    "    optimizer_options = ['adam', 'rmsprop', 'Nadam', 'adamax', 'Adagrad', 'Adadelta']\n",
    "    optimizer_selected = trial.suggest_categorical('optimizer', optimizer_options)\n",
    "    learning_rate = trial.suggest_float('lr', 1e-4, 1.0, log=True)\n",
    "    \n",
    "    if optimizer_selected == 'adam':\n",
    "        optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_selected == 'rmsprop':\n",
    "        optimizer = optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer_selected == 'Nadam':\n",
    "        optimizer = optimizers.Nadam(learning_rate=learning_rate)\n",
    "    elif optimizer_selected == 'Adagrad':\n",
    "        optimizer = optimizers.Adagrad(learning_rate=learning_rate)\n",
    "    elif optimizer_selected == 'Adadelta':\n",
    "        optimizer = optimizers.Adadelta(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = optimizers.Adamax(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=rmsle_loss, metrics=['mse', 'mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    model = create_model(trial)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 128, log=True)    \n",
    "    callbacks = [TFKerasPruningCallback(trial, 'val_loss'),\n",
    "                 ReduceLROnPlateau('val_loss', patience=5, factor=0.7), \n",
    "                 TerminateOnNaN()]\n",
    "\n",
    "    history = model.fit(X_train_b, y_train_b, epochs=epochs_search, validation_data=(X_val_b, y_val_b) ,batch_size=batch_size, verbose=0, callbacks=callbacks)\n",
    "    val_loss = np.min(history.history['val_loss'])\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "total_time_start = time.time()  \n",
    "search_time_start = time.time() \n",
    "num_completed_trials = 0\n",
    "search_rounds = 0\n",
    "time_taken = 0\n",
    "while time_taken < max_search_time:\n",
    "        \n",
    "    fold = 0\n",
    "    kf = KFold(n_splits=folds)\n",
    "    \n",
    "    time_fold_start = time.time()    \n",
    "    for train_index, val_index in kf.split(X_train_NN):\n",
    "\n",
    "        print('-------------------')\n",
    "        print(f\"Starting fold {fold} search...\")\n",
    "        X_train_b, X_val_b = X_train_NN[train_index], X_train_NN[val_index]    \n",
    "        y_train_b, y_val_b = y_train_NN[train_index], y_train_NN[val_index]\n",
    "\n",
    "        fold_name = f'{study_name}_{fold}'\n",
    "       \n",
    "        study = optuna.create_study(direction='minimize',\n",
    "                                    pruner=optuna.pruners.HyperbandPruner(min_resource=5),\n",
    "                                    study_name=fold_name,\n",
    "                                    storage=f'sqlite:///tampere_reg.db',\n",
    "                                    load_if_exists=True                                 \n",
    "                                    )\n",
    "\n",
    "        fold_time = time.time()    \n",
    "\n",
    "        fold_random = time.time()\n",
    "        study.sampler = optuna.samplers.RandomSampler()\n",
    "        print(f'Random search for fold {fold}...')\n",
    "        study.optimize(objective, n_trials=num_random)\n",
    "        print(f'Time taken for random search: {str(timedelta(seconds=(time.time() - fold_random)))}')\n",
    "\n",
    "        fold_tpe = time.time()  \n",
    "        study.sampler = optuna.samplers.TPESampler(n_startup_trials=0, n_ei_candidates=30, consider_endpoints=True)\n",
    "        print(f'TPE search for fold {fold}...')\n",
    "        study.optimize(objective, n_trials=num_tpe)\n",
    "        print(f'Time taken for TPE search: {str(timedelta(seconds=(time.time() - fold_tpe)))}')\n",
    "\n",
    "        num_completed_trials += num_random + num_tpe\n",
    "        print('-------------------')\n",
    "        print(f'Finished fold {fold} search.')\n",
    "        print(f\"Time taken for this fold: {str(timedelta(seconds=(time.time() - fold_time)))}\")                \n",
    "        print(f'Fold {fold} best value so far: {study.best_value}')\n",
    "        print(f'Mean time for one trial this fold: {str(timedelta(seconds=(time.time() - fold_time) / (num_random + num_tpe)))}')\n",
    "\n",
    "        fold += 1\n",
    "    search_rounds += 1\n",
    "    \n",
    "    time_taken = time.time() - search_time_start\n",
    "    \n",
    "    print(f'\\n# Completed search round: {search_rounds} #')\n",
    "    print(f'Time taken for all folds this round: {str(timedelta(seconds=(time.time() - time_fold_start)))}')\n",
    "    print(f'Total time taken for search: {str(timedelta(seconds=(time.time() - search_time_start)))}')\n",
    "    print(f'Made trials this far: {num_completed_trials}')\n",
    "    print(f\"Current mean time for one trial: {str(timedelta(seconds=(time.time() - search_time_start) / num_completed_trials))}\\n\")\n",
    "    \n",
    "\n",
    "print('='*20)    \n",
    "print(f'Finished search.')    \n",
    "print(f'Total time taken for all folds: {str(timedelta(seconds=(time.time() - search_time_start)))}')\n",
    "print(f'Made {num_completed_trials} trials in total.')\n",
    "print(f\"Mean time for one trial: {str(timedelta(seconds=(time.time() - search_time_start) / num_completed_trials))}\")\n",
    "print('='*20)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Montako epochia kullekin parhaalle sovitetaan malli\n",
    "epochs_best_fit = 500\n",
    "# Montako paras otetaan mukaan osittelusta\n",
    "num_best = 3\n",
    "# Montako kertaa kullekin parhaalle sovitetaan malli\n",
    "num_best_fits = 1\n",
    "\n",
    "best_optuna_models = []\n",
    "best_val_scores = []\n",
    "best_optuna_trials = [] \n",
    "\n",
    "kf = KFold(n_splits=folds)\n",
    "fold_num = 0\n",
    "fitting_search_start = time.time()\n",
    "\n",
    "for train_index, val_index in kf.split(X_train_NN):\n",
    "\n",
    "    best_fitting_time = time.time()\n",
    "    print(f\"Fold {fold_num} Best best trial fitting...\")\n",
    "\n",
    "    X_train_b, X_val_b = X_train_NN[train_index], X_train_NN[val_index]    \n",
    "    y_train_b, y_val_b = y_train_NN[train_index], y_train_NN[val_index]\n",
    "    \n",
    "    fold_name = f'{study_name}_{fold_num}'\n",
    "       \n",
    "    study = optuna.create_study(                                \n",
    "                                study_name=fold_name,\n",
    "                                storage=f'sqlite:///tampere_reg.db',\n",
    "                                load_if_exists=True\n",
    "                                )\n",
    "\n",
    "    sorted_trials = sorted(study.trials, key=lambda trial: trial.value)\n",
    "    best_trials = sorted_trials[:num_best]\n",
    "    best_val = np.inf\n",
    "    best_model = None\n",
    "\n",
    "    print('='*30)\n",
    "    print(f'Fitting best trials for fold {fold_num}...')\n",
    "    fitting_fold_best_start = time.time()\n",
    "    \n",
    "    for trial in best_trials:\n",
    "\n",
    "        print(f'Trial val_loss:{trial.value}\\nhyperparameters: {trial.params}')\n",
    "\n",
    "        for fit_num in range(num_best_fits):\n",
    "            \n",
    "            print('-'*30)\n",
    "            print(f\"Trial ID: {trial.number}, Value: {trial.value}, fit number: {fit_num}\")\n",
    "\n",
    "            checkpoint_filepath = f'./NN_search/optuna_search_checkpoint.h5'\n",
    "            model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=checkpoint_filepath,\n",
    "                save_weights_only=True,\n",
    "                monitor='val_loss',\n",
    "                mode='min',\n",
    "                save_best_only=True)\n",
    "\n",
    "            best_callback = [model_checkpoint_callback,                  \n",
    "                            ReduceLROnPlateau('val_loss', patience=10, factor=0.8, verbose=1), \n",
    "                            TerminateOnNaN(),\n",
    "                            EarlyStopping(monitor='val_loss', patience=100, verbose=1)\n",
    "                        ]\n",
    "\n",
    "\n",
    "            model = create_model(trial)\n",
    "            model.fit(X_train_b, y_train_b, epochs=epochs_best_fit, validation_data=(X_val_b, y_val_b), batch_size=trial.params['batch_size'], verbose=0, callbacks=best_callback)\n",
    "            model.load_weights(checkpoint_filepath)\n",
    "\n",
    "            predictions = model.predict(X_val_b, verbose=0)\n",
    "            mse = mean_squared_error(y_val_b, predictions)\n",
    "            mae = mean_absolute_error(y_val_b, predictions)\n",
    "            r2 = r2_score(y_val_b, predictions)\n",
    "            rmsle = rmsle_score(y_val_b, predictions)\n",
    "\n",
    "                        \n",
    "            print(f'MSE:{mse:.5f}\\nMAE:{mae:.5f}\\nRMSLE:{rmsle:.5f}\\nR2:{r2:.5f}')\n",
    "\n",
    "            if rmsle < best_val:\n",
    "                best_model = model\n",
    "                best_val = rmsle\n",
    "                best_trial_num = trial.number\n",
    "                best_trial = trial\n",
    "                print(f'*** New best model for fold {fold_num} is Trial {best_trial_num} with RMSLE {best_val} ***')\n",
    "                print(f'Best trial hyperparameters: {trial.params}')\n",
    "    \n",
    "    if best_model is not None:\n",
    "\n",
    "        best_optuna_models.append(best_model)\n",
    "        best_val_scores.append(best_val)\n",
    "        best_optuna_trials.append(best_trial)\n",
    "        print('='*30)\n",
    "        print(f\"Best model for fold {fold_num} RMSLE: {best_val}\\nTrial number: {best_trial_num}\\nHyperparameters: {best_trial.params}\")\n",
    "        print(f\"Time taken for best fitting in fold {fold_num}: {str(timedelta(seconds=(time.time() - best_fitting_time)) )}\")\n",
    "        print('='*30)\n",
    "\n",
    "    fold_num += 1\n",
    "\n",
    "print('='*30)\n",
    "print(f'Best models fitting time total:', str(timedelta(seconds=(time.time() - fitting_search_start))))\n",
    "print(f\"Total time taken for search and fitting best models: {str(timedelta(seconds=(time.time() - total_time_start)))}\")\n",
    "print('='*30)   \n",
    "\n",
    "\n",
    "# Tallennetaan parhaat verkot vielä myöhempää käyttöä varten\n",
    "from datetime import datetime\n",
    "for i, (model, score) in enumerate(zip(best_optuna_models, best_val_scores)):\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    filename = f\"./NN_search/{study_name}_foldmodel{i}_score_{score:.4f}_{timestamp}.pkl\"\n",
    "    print(f\"Saving model {i} with score {score:.4f} to {filename}\")\n",
    "    \n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, model in enumerate(best_optuna_models):\n",
    "    print(f\"\\nModel {idx} Summary:\")\n",
    "    # model.summary()\n",
    "    \n",
    "    # Testaa mallia testidatalla\n",
    "    predictions = model.predict(X_test_NN, verbose = 0)\n",
    "    mse = mean_squared_error(y_test_NN, predictions)\n",
    "    mae = mean_absolute_error(y_test_NN, predictions)\n",
    "    r2 = r2_score(y_test_NN, predictions)\n",
    "    rmsle = rmsle_score(y_test_NN, predictions)\n",
    "    \n",
    "    print(f\"\\nModel {idx+1} Performance on Test Data:\")\n",
    "    print(f\"MSE: {mse:.3f}\")\n",
    "    print(f\"MAE: {mae:.3f}\")\n",
    "    print(f\"R2: {r2:.3f}\")\n",
    "    print(f\"RMSLE: {rmsle:.3f}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.visualization import plot_optimization_history\n",
    "from optuna.visualization import plot_slice\n",
    "from optuna.visualization import plot_param_importances\n",
    "from optuna.visualization import plot_contour\n",
    "from optuna.visualization import plot_edf\n",
    "from optuna.visualization import plot_parallel_coordinate\n",
    "from optuna.visualization import plot_intermediate_values\n",
    "\n",
    "for fold in range(folds):\n",
    "     \n",
    "    fold_name = f'{study_name}_{fold}'    \n",
    "    study = optuna.load_study(study_name=fold_name, storage='sqlite:///tampere_reg.db')\n",
    "\n",
    "    plot_optimization_history(study).show()\n",
    "    \n",
    "    # plot_slice(study).show()\n",
    "    # plot_param_importances(study).show()\n",
    "    # plot_contour(study).show()\n",
    "    # plot_edf(study).show()\n",
    "    # plot_parallel_coordinate(study).show()\n",
    "    # plot_intermediate_values(study).show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "import time\n",
    "\n",
    "# Kerätään ensin kaikkien mallien ominaisuusvektorit\n",
    "X_train_features_list = []\n",
    "X_test_features_list = []\n",
    "features_names_list = []\n",
    "\n",
    "original_feature_names = list(X_train.columns) \n",
    "\n",
    "for idx, model in enumerate(best_optuna_models):\n",
    "    feature_extractor = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "    X_train_features = feature_extractor.predict(X_train_NN)\n",
    "    X_test_features = feature_extractor.predict(X_test_NN)\n",
    "    \n",
    "    X_train_features_list.append(X_train_features)\n",
    "    X_test_features_list.append(X_test_features)\n",
    "\n",
    "    num_features = X_train_features.shape[1]\n",
    "    model_feature_names = [f\"model_{idx}_feature_{feature_idx}\" for feature_idx in range(num_features)]\n",
    "    features_names_list.extend(model_feature_names)\n",
    "\n",
    "combined_feature_names = original_feature_names + features_names_list\n",
    "\n",
    "# Yhdistetään ominaisuusvektorit\n",
    "X_train_combined = np.concatenate(X_train_features_list, axis=1)\n",
    "X_test_combined = np.concatenate(X_test_features_list, axis=1)\n",
    "\n",
    "X_train_combined = np.concatenate([X_train_combined, X_train], axis=1)   \n",
    "X_test_combined = np.concatenate([X_test_combined, X_test], axis=1)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "\n",
    "def rmsle(y_true, y_pred):\n",
    "    if np.any(y_pred <= 0):\n",
    "        return 1e6\n",
    "    return np.sqrt(np.mean(np.square(np.log1p(y_pred) - np.log1p(y_true))))\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def objective(trial):\n",
    "    # Määritetään parametrit, jotka optimoidaan\n",
    "    param = {\n",
    "        'objective': trial.suggest_categorical('objective', ['reg:squarederror', 'reg:absoluteerror']),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 10),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1, 10),\n",
    "        'nthread': -2\n",
    "    }\n",
    "    num_boost_round = trial.suggest_int('num_boost_round', 1, 1000, log=True)\n",
    "\n",
    "    # K-fold cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    rmsle_scores = []\n",
    "\n",
    "    for train_index, val_index in kf.split(X_train_combined):\n",
    "        X_train_k, X_val_k = X_train_combined[train_index], X_train_combined[val_index]\n",
    "        y_train_k, y_val_k = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_train_k, label=y_train_k, feature_names=combined_feature_names)\n",
    "        dval = xgb.DMatrix(X_val_k, label=y_val_k, feature_names=combined_feature_names)\n",
    "\n",
    "        evals_result = {}\n",
    "        bst = xgb.train(param, dtrain, num_boost_round=num_boost_round, evals=[(dval, 'val')], early_stopping_rounds=10, evals_result=evals_result, verbose_eval=False)\n",
    "        # Käytä paras iteraatio määrä laskemaan ennusteet ja RMSLE\n",
    "        best_iteration = bst.best_iteration\n",
    "        preds = bst.predict(dval, iteration_range=(0, best_iteration + 1))\n",
    "        loss = rmsle(y_val_k, preds)\n",
    "        rmsle_scores.append(loss)\n",
    "\n",
    "    average_rmsle = np.mean(rmsle_scores)\n",
    "    return average_rmsle\n",
    "\n",
    "study = optuna.create_study(direction='minimize', storage='sqlite:///tampere_reg.db', study_name='tampere_xgb_combined_test_2', load_if_exists=True)\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Parhaiden parametrien tulostus ja mallin koulutus\n",
    "print(f\"Best trial: {study.best_trial.params}\")\n",
    "best_params = study.best_trial.params\n",
    "dtrain = xgb.DMatrix(X_train_combined, label=y_train, feature_names=combined_feature_names)\n",
    "best_model = xgb.train(best_params, dtrain, num_boost_round=best_params['num_boost_round'])\n",
    "\n",
    "# Ennustukset ja evaluointi testidatalla\n",
    "dtest = xgb.DMatrix(X_test_combined, label=y_test, feature_names=combined_feature_names)\n",
    "predictions = best_model.predict(dtest)\n",
    "\n",
    "# Visualisoidaan ennustettuja arvoja verrattuna todellisiin arvoihin\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(y_test, predictions, edgecolors=(0, 0, 0))\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "plt.xlabel('Measured')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Measured vs. Predicted Values')\n",
    "plt.show()\n",
    "\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "rmsle_val = rmsle(y_test, predictions)\n",
    "print(f\"MAE: {mae}, MSE: {mse}, R2: {r2}, RMSLE: {rmsle_val}\")\n",
    "\n",
    "# Feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "xgb.plot_importance(best_model, importance_type='weight', max_num_features=10)\n",
    "plt.title('Feature Importance by Weight')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "xgb.plot_importance(best_model, importance_type='gain', max_num_features=10)\n",
    "plt.title('Feature Importance by Gain')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "xgb.plot_importance(best_model, importance_type='cover', max_num_features=10)\n",
    "plt.title('Feature Importance by Cover')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "joda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
