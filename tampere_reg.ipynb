{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import seaborn as sns\n",
    "\n",
    "hakemisto = './data/'\n",
    "\n",
    "sisalto = os.listdir(hakemisto)\n",
    "print(sisalto)\n",
    "\n",
    "# Ladataan data \n",
    "df_train = pd.read_csv('./data/Tampere_BNB_training_listing.csv')\n",
    "df_test = pd.read_csv('./data/Tampere_BNB_testing_listing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puuttuvat_arvot = df_train.isnull().sum()\n",
    "puuttuvat_arvot = puuttuvat_arvot[puuttuvat_arvot > 0]\n",
    "print(f\"Puuttuvat arvot:\\n {puuttuvat_arvot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puuttuvat_arvot_prosentteina = (puuttuvat_arvot / len(df_train)) * 100\n",
    "print(f\"Puuttuvat arvot prosentteina:\\n {puuttuvat_arvot_prosentteina}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puuttuvat_arvot_prosentteina = puuttuvat_arvot_prosentteina.sort_values() # Järjestellään kasvavaan järjestykseen\n",
    "\n",
    "# Visualisointi \n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 15))  # Säädä kokoa tarpeen mukaan\n",
    "sns.barplot(x=puuttuvat_arvot_prosentteina, y=puuttuvat_arvot_prosentteina.index)\n",
    "plt.title('Puuttuvien arvojen prosenttiosuus sarakkeittain')\n",
    "plt.xlabel('Prosenttiosuus (%)')\n",
    "plt.ylabel('Sarakkeet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muutetaan Rv sarakkeen tyyppi kokonaisluvuksi\n",
    "df_train['Rv'] = df_train['Rv'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Rv'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Rv'].value_counts().sort_index().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Kunto'] = df_train['Kunto'].fillna('Ei tietoa')\n",
    "df_train['Kunto'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kunto_mappaus = {\n",
    "    'Ei tietoa': 0,\n",
    "    'huono': 1,\n",
    "    'tyyd.': 2,\n",
    "    'hyvä': 3,\n",
    "}\n",
    "df_train['Kunto'] = df_train['Kunto'].map(kunto_mappaus)\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Kunto'].value_counts()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Hissi'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Hissi'] = df_train['Hissi'].astype('category')\n",
    "df_train['Hissi'] = df_train['Hissi'].cat.codes\n",
    "df_train['Hissi'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_row', None) \n",
    "pd.set_option('display.max_columns', None) \n",
    "df_train['Kaupunginosa'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Kaupunginosa'] = df_train['Kaupunginosa'].astype('category')   \n",
    "df_train['Kaupunginosa'].value_counts().plot(kind='bar') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raja_arvo = 0.01\n",
    "maarat = df_train['Kaupunginosa'].value_counts(normalize=True)\n",
    "pienet_ryhmat = maarat[maarat < raja_arvo].index\n",
    "df_train['Kaupunginosa'] = df_train['Kaupunginosa'].replace(pienet_ryhmat, 'Muu')\n",
    "df_train['Kaupunginosa'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Kaupunginosa'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Kaupunginosa'] = df_train['Kaupunginosa'].astype('category')\n",
    "df_train['Kaupunginosa'] = df_train['Kaupunginosa'].cat.codes\n",
    "df_train['Kaupunginosa'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Asunnon tyyppi'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tyyppimat = {\n",
    "    'Yksiö' : 1,\n",
    "    'Kaksi huonetta' : 2,\n",
    "    'Kolme huonetta' : 3,\n",
    "    'Neljä huonetta tai enemmän' : 4\n",
    "}\n",
    "df_train['Asunnon tyyppi'] = df_train['Asunnon tyyppi'].map(tyyppimat)\n",
    "df_train['Asunnon tyyppi'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Huoneisto'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Huoneisto'] = df_train['Huoneisto'].str.lower()\n",
    "df_train['Huoneisto'] = df_train['Huoneisto'].str.replace(' ', '')\n",
    "df_train['Huoneisto'] = df_train['Huoneisto'].str.replace('+', ',')\n",
    "df_train['Huoneisto'] = df_train['Huoneisto'].str.replace('...', \"\")\n",
    "df_train['Huoneisto'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Huoneisto'] = df_train['Huoneisto'].str.replace('/', ',')\n",
    "df_train['Huoneisto'] = df_train['Huoneisto'].str.replace('[0-9]+h', '', regex=True)\n",
    "df_train['Huoneisto'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Huoneisto'] = df_train['Huoneisto'].str.replace('^,', '',regex=True)\n",
    "df_train['Huoneisto'] = df_train['Huoneisto'].str.replace('^[-0-9]+', '',regex=True)\n",
    "df_train['Huoneisto'] = df_train['Huoneisto'].str.replace(',$', '',regex=True)\n",
    "df_train['Huoneisto'] = df_train['Huoneisto'].str.replace('^,', '',regex=True)\n",
    "df_train['Huoneisto'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huoneisto_split = df_train['Huoneisto'].str.split(',')\n",
    "exploded = huoneisto_split.explode()\n",
    "# exploded_unique = exploded.nunique()\n",
    "exploded_unique_count = exploded.value_counts()\n",
    "print(f\"Unique values: {exploded_unique_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Käytetään wikisivustoa https://fi.wikipedia.org/wiki/Luettelo_asuntokaupassa_k%C3%A4ytett%C3%A4vist%C3%A4_lyhenteist%C3%A4 termeille \n",
    "\n",
    "import re\n",
    "\n",
    "huoneisto_split = df_train['Huoneisto'].str.split(',')\n",
    "\n",
    "huoneisto_split = huoneisto_split.apply(lambda lst: [\"parveke\" if  re.search('^p$|^parv$', item) else item for item in lst])\n",
    "huoneisto_split = huoneisto_split.apply(lambda lst: [\"wc\" if  re.search('^w$', item) else item for item in lst])\n",
    "huoneisto_split = huoneisto_split.apply(lambda lst: [\"sauna\" if  re.search('^s$', item) else item for item in lst])\n",
    "huoneisto_split = huoneisto_split.apply(lambda lst: [\"2wc\" if  re.search('^erill.wc$|^2xwc$', item) else item for item in lst])\n",
    "huoneisto_split = huoneisto_split.apply(lambda lst: [\"kph\" if  re.search('^kh$', item) else item for item in lst])\n",
    "huoneisto_split = huoneisto_split.apply(lambda lst: [\"alkovi\" if  re.search('^alk$', item) else item for item in lst])\n",
    "huoneisto_split = huoneisto_split.apply(lambda lst: [\"lasit\" if  re.search('^l', item) else item for item in lst])\n",
    "huoneisto_split = huoneisto_split.apply(lambda lst: [\"avok\" if  re.search('^avokeitti&#$', item) else item for item in lst])\n",
    "\n",
    "\n",
    "exploded = huoneisto_split.explode()\n",
    "exploded_lkm = exploded.value_counts()\n",
    "print(f\"Unique values: {exploded_lkm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimi_lkm = 10\n",
    "suodatetut_idx = exploded_lkm[exploded_lkm >= minimi_lkm].index\n",
    "suodatettu_lista = huoneisto_split.apply(lambda lst: [item for item in lst if item in suodatetut_idx])\n",
    "suodatettu_lista = suodatettu_lista.apply(lambda lst: [\"määrittämätön\"] if len(lst) == 0 else lst)\n",
    "\n",
    "df_train['Huoneisto'] = suodatettu_lista.apply(lambda lst: ','.join(lst))\n",
    "\n",
    "# Tarkistetaan vielä vähän tymästi, että onko kaikki kunnossa\n",
    "huoneisto_split = df_train['Huoneisto'].str.split(',')\n",
    "exploded = huoneisto_split.explode()\n",
    "exploded_lkm = exploded.value_counts()\n",
    "print(f\"Uniikit arvot values: {exploded_lkm}\")\n",
    "print(f\"Määrä arvot: {exploded_lkm.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Huoneisto'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.shape)\n",
    "# Splitataan 'Huoneisto' -sarake ja muunnetaan se listaksi pilkun perusteella\n",
    "split_data = df_train['Huoneisto'].str.split(',')\n",
    "\n",
    "# Käytetään explode()-metodia muuntaaksemme listan elementit omiksi riveikseen\n",
    "exploded_data = split_data.explode()\n",
    "\n",
    "# Valinnainen: Suodatetaan pois harvinaiset kategoriat ennen one-hot-enkoodausta\n",
    "# value_counts = exploded_data.value_counts()\n",
    "# to_keep = value_counts[value_counts >= 5].index\n",
    "# filtered_data = exploded_data[exploded_data.isin(to_keep)]\n",
    "\n",
    "# Suoritetaan one-hot-enkoodaus\n",
    "one_hot_encoded = pd.get_dummies(exploded_data)\n",
    "\n",
    "# Summataan yhteen samat rivit, koska explode() luo useita rivejä samalle alkuperäiselle indeksille\n",
    "one_hot_summed = one_hot_encoded.groupby(one_hot_encoded.index).sum()\n",
    "\n",
    "# Yhdistetään one-hot-enkoodatut sarakkeet takaisin alkuperäiseen DataFrameen\n",
    "df_train = df_train.join(one_hot_summed)\n",
    "\n",
    "print(df_train.shape)\n",
    "\n",
    "print(df_train.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Talot.\"].value_counts()\n",
    "df_train[\"Talot.\"] = df_train[\"Talot.\"].astype('category')\n",
    "df_train[\"Talot.\"] = df_train[\"Talot.\"].cat.codes\n",
    "df_train[\"Talot.\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Krs\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Krs\"].isnull().sum()\n",
    "df_train[\"Krs\"] = df_train[\"Krs\"].fillna(\"0/0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Krs\"] = df_train[\"Krs\"].str.replace('^-', '', regex=True)\n",
    "df_train[\"Krs\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kerros_split = df_train[\"Krs\"].str.split('/', expand=True)  \n",
    "df_train[\"kerros\"] = kerros_split[0].astype(int)\n",
    "df_train[\"max_kerros\"] = kerros_split[1].astype(int)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_filtered = df_train.drop(['Huoneisto', 'Krs'], axis=1)\n",
    "df_train_filtered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_train_filtered.drop('Hinta', axis=1)\n",
    "y = df_train_filtered['Hinta']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score,  mean_absolute_error\n",
    "\n",
    "# Luodaan lineaarisen regressiomallin instanssi\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Suoritetaan ristiinvalidointi\n",
    "# cv määrittää ristiinvalidoinnin jakojen määrän\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "predictions = lr.predict(X_test)\n",
    "\n",
    "r2 = r2_score(y_test, predictions)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print(f\"Mean squared error: {mse:.2f}\\nMean absolute error: {mae:.2f}\")\n",
    "print(f\"Parhaan mallin R²-arvo: {r2}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "ax.scatter(y_test, predictions, edgecolors=(0, 0, 0))\n",
    "ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Measured')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.title('Measured vs. Predicted Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "virhe_mallit = [xgboost.XGBRegressor(objective='reg:absoluteerror'), xgboost.XGBRegressor(objective='reg:squarederror')]\n",
    "virhe_nimi = ['neg_mean_absolute_error', 'neg_mean_squared_error']\n",
    "\n",
    "for idx, malli in enumerate(virhe_mallit):\n",
    "\n",
    "    param_space = {\n",
    "        'n_estimators': np.arange(1, 500, 10),\n",
    "        'max_depth': np.arange(3, 11),\n",
    "        'learning_rate': [0.1, 0.01, 0.001],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "        'gamma': [0, 1, 5],\n",
    "        'reg_alpha': [0, 0.1, 0.5],\n",
    "        'reg_lambda': [1, 1.5, 2]\n",
    "    }\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=malli,\n",
    "        param_distributions=param_space,\n",
    "        cv=5,\n",
    "        n_jobs=-2,\n",
    "        n_iter= 1000, \n",
    "        verbose=1,   \n",
    "        scoring='neg_mean_absolute_error' \n",
    "    )\n",
    "\n",
    "    \n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    best_index = random_search.best_index_\n",
    "    cv_results = random_search.cv_results_\n",
    "    cv_splits = random_search.cv\n",
    "    best_scores = [cv_results[f'split{i}_test_score'][best_index] for i in range(cv_splits)]\n",
    "\n",
    "\n",
    "    print(f\"With error: {virhe_nimi[idx]}\")\n",
    "    for i, score in enumerate(best_scores):\n",
    "        print(f\"Ositus {i}: {-score}\") \n",
    "\n",
    "\n",
    "    best_model = random_search.best_estimator_\n",
    "    predictions = best_model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    print(f\"Mean squared error: {mse:.2f}\\nMean absolute error: {mae:.2f}\\nParhaan mallin R²-arvo: {r2:.4f}\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    ax.scatter(y_test, predictions, edgecolors=(0, 0, 0))\n",
    "    ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "    ax.set_xlabel('Measured')\n",
    "    ax.set_ylabel('Predicted')\n",
    "    plt.title(f'{virhe_nimi[idx]} Measured vs. Predicted Values')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler \n",
    "\n",
    "# Skaalataan numeeriset muuttujat\n",
    "robust_scaler = RobustScaler()\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "df_train_NN = df_train_filtered.copy()\n",
    "df_train_NN[['Pituusaste', 'Leveysaste']] = minmax_scaler.fit_transform(df_train_NN[['Pituusaste', 'Leveysaste']])\n",
    "df_train_NN['Rv'] = minmax_scaler.fit_transform(df_train_NN[['Rv']])\n",
    "df_train_NN['m2'] = minmax_scaler.fit_transform(df_train_NN[['m2']])\n",
    "\n",
    "# One hot koodataan kategoriset muuttujat\n",
    "df_hot = pd.get_dummies(df_train_NN['Kaupunginosa'], prefix='Kaupunginosa').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN['kerros'], prefix='kerros').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN['max_kerros'], prefix='max_kerros').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN['Kunto'], prefix='Kunto').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN['Hissi'], prefix='Hissi').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN['Asunnon tyyppi'], prefix='Asunnon tyyppi').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN[\"Talot.\"], prefix='Talot.').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "\n",
    "df_train_NN.drop(['Kaupunginosa', 'kerros', 'max_kerros', 'Kunto', 'Hissi', 'Asunnon tyyppi', \"Talot.\"], axis=1, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muodostetaan X ja y sekä jaetaan data harjoitus- ja testijoukkoihin\n",
    "\n",
    "X = df_train_NN.drop('Hinta', axis=1)\n",
    "y = df_train_NN['Hinta']\n",
    "\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras_tuner import Hyperband\n",
    "from keras import regularizers, layers, optimizers, initializers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "models_hyperband = []\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "r2_scores = []\n",
    "best_hyperparameters_hyperband = []\n",
    "\n",
    "search_time_start = time.time() \n",
    "\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(X_train.shape[1],)))\n",
    "\n",
    "    # Luodaan kerroksia käyttäen Hyperband-optimoinnin hyperparametreja\n",
    "    for i in range(hp.Int('num_layers', 1, 3)):  # Vaihteluväli kerrosten määrälle\n",
    "        model.add(layers.Dense(\n",
    "            units=hp.Int(f'units_{i}', min_value=8, max_value=512, step=32),\n",
    "            activation=hp.Choice(f'activation_{i}', values=['relu', 'linear', 'selu', 'elu', 'sigmoid', 'tanh']),\n",
    "            kernel_regularizer=regularizers.l1_l2(\n",
    "                l1=hp.Float(f'l1_reg_{i}', min_value=1e-6, max_value=1, sampling='log'),\n",
    "                l2=hp.Float(f'l2_reg_{i}', min_value=1e-6, max_value=1, sampling='log')),\n",
    "            kernel_initializer=hp.Choice('initializer', values=['he_normal', 'glorot_uniform', 'lecun_normal', 'glorot_normal'])\n",
    "            )\n",
    "        )\n",
    "        model.add(layers.Dropout(hp.Float(f'dropout_{i}', min_value=0.0, max_value=0.5, step=0.05)))\n",
    "\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "    optimizer_choice = hp.Choice('optimizer', values=['rmsprop', 'nadam', 'adamax'])\n",
    "    learning_rate = hp.Choice('learning_rate', values=[1.0, 1e-1, 1e-2, 1e-3])\n",
    "    \n",
    "    if optimizer_choice == 'rmsprop':\n",
    "        optimizer = optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'nadam':\n",
    "        optimizer = optimizers.Nadam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = optimizers.Adamax(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n",
    "\n",
    "# Callbacks määritelty\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=5, min_lr=1e-6, verbose=1),\n",
    "    TerminateOnNaN()\n",
    "]\n",
    "\n",
    "# Käytetään Hyperband-tuneria\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "\n",
    "\n",
    "    X_train_b, X_val_b = X_train[train_index], X_train[val_index]    \n",
    "    y_train_b, y_val_b = y_train[train_index], y_train[val_index]\n",
    "    y_train_b = tf.data.Dataset.from_tensor_slices(y_train_b)\n",
    "    X_train_b = tf.data.Dataset.from_tensor_slices(X_train_b)\n",
    "    train_dataset = tf.data.Dataset.zip((X_train_b, y_train_b)).batch(64)\n",
    "    train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    \n",
    "    tuner = Hyperband(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_epochs=300,\n",
    "    factor=3,\n",
    "    directory='NN_search',\n",
    "    project_name='kt_hyperband',\n",
    "    overwrite=True,\n",
    "    hyperband_iterations=2\n",
    "    )\n",
    "    \n",
    "    tuner.search(train_dataset, validation_data=(X_val_b, y_val_b), callbacks=callbacks, verbose=2)\n",
    "\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "    models_hyperband.append(best_model)\n",
    "    best_hyperparameters_hyperband.append(tuner.get_best_hyperparameters(num_trials=1)[0])\n",
    "\n",
    "    # Evaluoi malli\n",
    "    predictions = best_model.predict(X_test)\n",
    "    mse_scores.append(mean_squared_error(y_test, predictions))\n",
    "    mae_scores.append(mean_absolute_error(y_test, predictions))\n",
    "    r2_scores.append(r2_score(y_test, predictions))\n",
    "\n",
    "\n",
    "search_time_end = time.time()\n",
    "print(f\"Hyperband search took {search_time_end - search_time_start} seconds\")\n",
    "\n",
    "# Tulosetaan kaikki tulokset alkuun\n",
    "for idx, (mae, mse, r2) in enumerate(zip(mae_scores, mse_scores, r2_scores), start=1):\n",
    "    print(f\"Model {idx} - MAE: {mae}, MSE: {mse}, R2: {r2}\")\n",
    "\n",
    "# Käydään vielä eri mallit lävitse selvyyden vuoksi\n",
    "for idx, model in enumerate(models_hyperband):\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "    plt.figure(figsize=(20, 10)) \n",
    "    plt.scatter(y_test, predictions, edgecolors=(0, 0, 0))\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "    plt.xlabel('Measured')  \n",
    "    plt.ylabel('Predicted') \n",
    "    plt.title('Measured vs. Predicted Values')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Fold {idx} - MSE: {mse_scores[idx]}, MAE: {mae_scores[idx]}, R2: {r2_scores[idx]}\")\n",
    "    hp = best_hyperparameters_hyperband[idx]\n",
    "    print(f\"Best hyperparameters for model {i+1}:\")\n",
    "    for key in hp.values:\n",
    "        print(f\"{key}: {hp.get(key)}\")\n",
    "    print(\"-\" * 50)\n",
    "    model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.models import Model\n",
    "\n",
    "# Kerätään ensin kaikkien mallien ominaisuusvektorit\n",
    "X_train_features_list = []\n",
    "X_test_features_list = []\n",
    "\n",
    "for model in models_hyperband:\n",
    "    feature_extractor = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "    X_train_features = feature_extractor.predict(X_train)\n",
    "    X_test_features = feature_extractor.predict(X_test)\n",
    "    \n",
    "    X_train_features_list.append(X_train_features)\n",
    "    X_test_features_list.append(X_test_features)\n",
    "\n",
    "# Yhdistetään ominaisuusvektorit\n",
    "X_train_combined = np.concatenate(X_train_features_list, axis=1)\n",
    "X_test_combined = np.concatenate(X_test_features_list, axis=1)\n",
    "\n",
    "xgb = xgboost.XGBRegressor(objective ='reg:squarederror')\n",
    "param_space = {\n",
    "    'n_estimators': np.arange(1, 500, 20),\n",
    "    'max_depth': np.arange(2, 11),\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 1, 5],\n",
    "    'reg_alpha': [0, 0.1, 0.5],\n",
    "    'reg_lambda': [1, 1.5, 2]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=param_space,\n",
    "    cv=5,\n",
    "    n_jobs=-2,\n",
    "    n_iter=100,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "random_search.fit(X_train_combined, y_train)\n",
    "best_model = random_search.best_estimator_\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "predictions = best_model.predict(X_test_combined)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f\"MAE: {mae}, MSE: {mse}, R2: {r2}\")\n",
    "print(f\"Random search took {elapsed_time} seconds\")\n",
    "print(f\"Feature shape: {X_train_combined.shape}\")\n",
    "\n",
    "plt.figure(figsize=(20, 10)) \n",
    "plt.scatter(y_test, predictions, edgecolors=(0, 0, 0))\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "plt.xlabel('Measured')  \n",
    "plt.ylabel('Predicted') \n",
    "plt.title('Measured vs. Predicted Values NN Hyperband features')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras_tuner import BayesianOptimization\n",
    "from keras import regularizers, Sequential, layers, optimizers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler \n",
    "from tensorflow import keras\n",
    "import time\n",
    "from keras import initializers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "search_time_start = time.time()\n",
    "models_bayes = []\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "r2_scores = []\n",
    "\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(X_train.shape[1],)))\n",
    "\n",
    "    # Luodaan kerroksia käyttäen Hyperband-optimoinnin hyperparametreja\n",
    "    for i in range(hp.Int('num_layers', 1, 4)):  # Vaihteluväli kerrosten määrälle\n",
    "        model.add(layers.Dense(\n",
    "            units=hp.Int(f'units_{i}', min_value=8, max_value=512, step=32),\n",
    "            activation=hp.Choice(f'activation_{i}', values=['relu', 'linear', 'selu', 'elu', 'sigmoid', 'tanh']),\n",
    "            kernel_regularizer=regularizers.l1_l2(\n",
    "                l1=hp.Float(f'l1_reg_{i}', min_value=1e-6, max_value=1e-1, sampling='log'),\n",
    "                l2=hp.Float(f'l2_reg_{i}', min_value=1e-6, max_value=1e-1, sampling='log')),\n",
    "            kernel_initializer=hp.Choice('initializer', values=['he_normal', 'glorot_uniform', 'lecun_normal', 'glorot_normal'])\n",
    "            )\n",
    "        )\n",
    "        model.add(layers.Dropout(hp.Float(f'dropout_{i}', min_value=0.0, max_value=0.5, step=0.05)))\n",
    "\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "    optimizer_choice = hp.Choice('optimizer', values=['rmsprop', 'nadam', 'adamax', 'adam'])\n",
    "    learning_rate = hp.Choice('learning_rate', values=[1.0, 1e-1, 1e-2, 1e-3])\n",
    "    \n",
    "    if optimizer_choice == 'rmsprop':\n",
    "        optimizer = optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'nadam':\n",
    "        optimizer = optimizers.Nadam(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'adam':\n",
    "        optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = optimizers.Adamax(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n",
    "\n",
    "# Initialize the Bayesian Optimization tuner\n",
    "\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr_callback = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.8,  # New learning rate = learning rate * factor\n",
    "    patience=5,  # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    min_lr=1e-6,  # Lower bound on the learning rate\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "terminate_on_nan = TerminateOnNaN()\n",
    "\n",
    "best_hyperparameters_bayes = []\n",
    "\n",
    "start_time = time.time()\n",
    "kf = KFold(n_splits=5)\n",
    "models_bayes = []\n",
    "round = 0\n",
    "\n",
    "\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "\n",
    "    tuner = BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=420, \n",
    "    executions_per_trial=1,\n",
    "    directory='NN_search',\n",
    "    project_name='kt_bayesian',\n",
    "    overwrite=True,\n",
    "    max_consecutive_failed_trials=10,\n",
    "    max_retries_per_trial = 0\n",
    ")\n",
    "    \n",
    "    X_train_b, X_val_b = X_train[train_index], X_train[val_index]    \n",
    "    y_train_b, y_val_b = y_train[train_index], y_train[val_index]\n",
    "    y_train_b = tf.data.Dataset.from_tensor_slices(y_train_b)\n",
    "    X_train_b = tf.data.Dataset.from_tensor_slices(X_train_b)\n",
    "    train_dataset = tf.data.Dataset.zip((X_train_b, y_train_b)).batch(64)\n",
    "    train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    tuner.search(\n",
    "        train_dataset,\n",
    "        epochs=300,\n",
    "        validation_data=(X_val_b, y_val_b),\n",
    "        callbacks=[early_stopping_callback, reduce_lr_callback, terminate_on_nan],\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "  \n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "    models_bayes.append(best_model)\n",
    "    best_hyperparameters_bayes.append(tuner.get_best_hyperparameters(num_trials=1)[0])\n",
    "\n",
    "    predictions = best_model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    \n",
    "    # Tallenna suorituskykymetriikat\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    r2_scores.append(r2)\n",
    "\n",
    "search_time_end = time.time()\n",
    "print(f\"Bayesian search took {search_time_end - search_time_start} seconds\")\n",
    "\n",
    "# Tulosetaan kaikki tulokset alkuun\n",
    "for idx, (mae, mse, r2) in enumerate(zip(mae_scores, mse_scores, r2_scores), start=1):\n",
    "    print(f\"Model {idx} - MAE: {mae}, MSE: {mse}, R2: {r2}\")\n",
    "\n",
    "# Käydään vielä eri mallit lävitse selvyyden vuoksi\n",
    "for idx, model in enumerate(models_bayes):\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "    plt.figure(figsize=(20, 10)) \n",
    "    plt.scatter(y_test, predictions, edgecolors=(0, 0, 0))\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "    plt.xlabel('Measured')  \n",
    "    plt.ylabel('Predicted') \n",
    "    plt.title('Measured vs. Predicted Values')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Fold {idx} - MSE: {mse_scores[idx]}, MAE: {mae_scores[idx]}, R2: {r2_scores[idx]}\")\n",
    "    hp = best_hyperparameters_bayes[idx]\n",
    "    print(f\"Best hyperparameters for model {i+1}:\")\n",
    "    for key in hp.values:\n",
    "        print(f\"{key}: {hp.get(key)}\")\n",
    "    print(\"-\" * 50)\n",
    "    model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.models import Model\n",
    "\n",
    "# Kerätään ensin kaikkien mallien ominaisuusvektorit\n",
    "X_train_features_list = []\n",
    "X_test_features_list = []\n",
    "\n",
    "for model in models_bayes:\n",
    "    feature_extractor = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "    X_train_features = feature_extractor.predict(X_train)\n",
    "    X_test_features = feature_extractor.predict(X_test)\n",
    "    \n",
    "    X_train_features_list.append(X_train_features)\n",
    "    X_test_features_list.append(X_test_features)\n",
    "\n",
    "# Yhdistetään ominaisuusvektorit\n",
    "X_train_combined = np.concatenate(X_train_features_list, axis=1)\n",
    "X_test_combined = np.concatenate(X_test_features_list, axis=1)\n",
    "\n",
    "xgb = xgboost.XGBRegressor(objective ='reg:squarederror')\n",
    "param_space = {\n",
    "    'n_estimators': np.arange(1, 500, 20),\n",
    "    'max_depth': np.arange(2, 11),\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 1, 5],\n",
    "    'reg_alpha': [0, 0.1, 0.5],\n",
    "    'reg_lambda': [1, 1.5, 2]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=param_space,\n",
    "    cv=5,\n",
    "    n_jobs=-2,\n",
    "    n_iter=100,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "random_search.fit(X_train_combined, y_train)\n",
    "best_model = random_search.best_estimator_\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "predictions = best_model.predict(X_test_combined)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f\"MAE: {mae}, MSE: {mse}, R2: {r2}\")\n",
    "print(f\"Random search took {elapsed_time} seconds\")\n",
    "print(f\"Feature shape: {X_train_combined.shape}\")\n",
    "\n",
    "plt.figure(figsize=(20, 10)) \n",
    "plt.scatter(y_test, predictions, edgecolors=(0, 0, 0))\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "plt.xlabel('Measured')  \n",
    "plt.ylabel('Predicted') \n",
    "plt.title('Measured vs. Predicted Values NN Bayesian features')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "joda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
