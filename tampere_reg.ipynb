{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_train_filtered = pd.read_pickle('./data/df_train_filtered.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_train_filtered.drop('Hinta', axis=1)\n",
    "y = df_train_filtered['Hinta']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=df_train_filtered['Kaupunginosa'], random_state=42)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler \n",
    "\n",
    "# Skaalataan numeeriset muuttujat\n",
    "robust_scaler = RobustScaler()\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "df_train_NN = df_train_filtered.copy()\n",
    "df_train_NN[['Pituusaste', 'Leveysaste']] = minmax_scaler.fit_transform(df_train_NN[['Pituusaste', 'Leveysaste']])\n",
    "df_train_NN['Rv'] = minmax_scaler.fit_transform(df_train_NN[['Rv']])\n",
    "df_train_NN['m2'] = minmax_scaler.fit_transform(df_train_NN[['m2']])\n",
    "\n",
    "# One hot koodataan kategoriset muuttujat\n",
    "df_hot = pd.get_dummies(df_train_NN['Kaupunginosa'], prefix='Kaupunginosa').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN['kerros'], prefix='kerros').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN['max_kerros'], prefix='max_kerros').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN['Kunto'], prefix='Kunto').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN['Hissi'], prefix='Hissi').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN['Asunnon tyyppi'], prefix='Asunnon tyyppi').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN[\"Talot.\"], prefix='Talot.').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "\n",
    "df_train_NN.drop(['Kaupunginosa', 'kerros', 'max_kerros', 'Kunto', 'Hissi', 'Asunnon tyyppi', \"Talot.\"], axis=1, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muodostetaan X ja y sekä jaetaan data harjoitus- ja testijoukkoihin\n",
    "\n",
    "X = df_train_NN.drop('Hinta', axis=1)\n",
    "y = df_train_NN['Hinta']\n",
    "\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy().astype('float32')\n",
    "\n",
    "X_train_NN, X_test_NN, y_train_NN, y_test_NN = train_test_split(X, y, test_size=0.1, random_state=42, stratify=df_train_filtered['Kaupunginosa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from keras import regularizers, layers, optimizers, initializers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "import time \n",
    "from optuna.integration import TFKerasPruningCallback\n",
    "import os \n",
    "import pickle \n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "# Haun nimi\n",
    "study_name = 'rmsle5_random_2503'\n",
    "# Montako osittelua käytettiin\n",
    "folds = 5\n",
    "# Montako epochia kullekin osittelulle\n",
    "epochs_search = 100\n",
    "# Montako satunnaista hakua kieroksella\n",
    "num_random = 42\n",
    "# Montako TPE hakua kieroksella\n",
    "num_tpe = 0\n",
    "\n",
    "# Aika sekuntteina jota hakuun käytetän\n",
    "max_search_time = 36000\n",
    "# Neuroneiden maksimimäärä \n",
    "max_units = 512\n",
    "\n",
    "def rmsle_loss(y_true, y_pred):\n",
    "    # Asetetaan suuri rangaistusarvo, jos y_pred sisältää arvon nolla tai alle\n",
    "    penalty = tf.constant(1e5, dtype=tf.float32)\n",
    "    \n",
    "    # Maski, joka on tosi, kun y_pred on > 0\n",
    "    valid_mask = tf.math.greater(y_pred, 0.0)\n",
    "    \n",
    "    # Käytä maskia valitsemaan joko oikea RMSLE laskenta tai suuri rangaistus\n",
    "    safe_y_pred = tf.where(valid_mask, y_pred, penalty)\n",
    "    \n",
    "    # Laske RMSLE vain, jos y_pred on suurempi kuin 0, muuten palauta rangaistus\n",
    "    rmsle = tf.sqrt(tf.reduce_mean(tf.square(tf.math.log1p(safe_y_pred) - tf.math.log1p(y_true))))\n",
    "    \n",
    "    # Palauta suuri rangaistus, jos y_pred sisälsi nollan tai negatiivisen arvon\n",
    "    return tf.where(tf.reduce_any(~valid_mask), penalty, rmsle)\n",
    "\n",
    "\n",
    "def create_model(trial):\n",
    "        \n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=(X_train_NN.shape[1],)))\n",
    "    \n",
    "    num_units = trial.suggest_int(f'n_units_0', 4, 512, log=True)\n",
    "    dropout_rate = trial.suggest_float(f'dropout_0', 0.0, 0.5)\n",
    "    kernel_regularizer=regularizers.l1_l2(\n",
    "            l1= trial.suggest_float(f'l1_reg_0', 1e-6, 1, log=True),\n",
    "            l2= trial.suggest_float(f'l2_reg_0', 1e-6, 1, log=True)\n",
    "        )\n",
    "    activation = trial.suggest_categorical(f'activation_0', ['relu', 'tanh', 'selu', 'linear', 'sigmoid', 'elu'])    \n",
    "    model.add(keras.layers.Dense(num_units, activation=activation, kernel_regularizer=kernel_regularizer))    \n",
    "    model.add(keras.layers.Dropout(rate=dropout_rate))\n",
    "\n",
    "    num_units = trial.suggest_int(f'n_units_1', 4, 256, log=True)\n",
    "    dropout_rate = trial.suggest_float(f'dropout_1', 0.0, 0.5)\n",
    "    kernel_regularizer=regularizers.l1_l2(\n",
    "            l1= trial.suggest_float(f'l1_reg_1', 1e-6, 1, log=True),\n",
    "            l2= trial.suggest_float(f'l2_reg_1', 1e-6, 1, log=True)\n",
    "        )\n",
    "    activation = trial.suggest_categorical(f'activation_1', ['relu', 'tanh', 'selu', 'linear', 'sigmoid', 'elu'])    \n",
    "    model.add(keras.layers.Dense(num_units, activation=activation, kernel_regularizer=kernel_regularizer))    \n",
    "    model.add(keras.layers.Dropout(rate=dropout_rate))\n",
    "    \n",
    "    num_last = trial.suggest_int('n_units_last', 1, 32)\n",
    "    dropout_last = trial.suggest_float('dropout_last', 0.0, 0.5)\n",
    "    activation_last = trial.suggest_categorical('activation_last', ['relu', 'tanh', 'selu', 'linear', 'sigmoid', 'elu'])\n",
    "    kernel_regularizer_last = regularizers.l1_l2( \n",
    "        l1= trial.suggest_float('l1_reg_last', 1e-6, 1, log=True),\n",
    "        l2= trial.suggest_float('l2_reg_last', 1e-6, 1, log=True)\n",
    "    )\n",
    "    model.add(keras.layers.Dense(num_last, activation=activation_last, kernel_regularizer=kernel_regularizer_last))        \n",
    "    model.add(keras.layers.Dropout(rate=dropout_last))\n",
    "    model.add(keras.layers.Dense(1, activation='linear')) \n",
    "    \n",
    "    # Optimisaattorin ja oppimisnopeuden valinta\n",
    "    optimizer_options = ['adam', 'rmsprop', 'Nadam', 'adamax', 'Adagrad', 'Adadelta']\n",
    "    optimizer_selected = trial.suggest_categorical('optimizer', optimizer_options)\n",
    "    learning_rate = trial.suggest_float('lr', 1e-4, 1.0, log=True)\n",
    "    \n",
    "    if optimizer_selected == 'adam':\n",
    "        optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_selected == 'rmsprop':\n",
    "        optimizer = optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer_selected == 'Nadam':\n",
    "        optimizer = optimizers.Nadam(learning_rate=learning_rate)\n",
    "    elif optimizer_selected == 'Adagrad':\n",
    "        optimizer = optimizers.Adagrad(learning_rate=learning_rate)\n",
    "    elif optimizer_selected == 'Adadelta':\n",
    "        optimizer = optimizers.Adadelta(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = optimizers.Adamax(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=rmsle_loss, metrics=['mse', 'mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    model = create_model(trial)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 128, log=True)    \n",
    "    callbacks = [TFKerasPruningCallback(trial, 'val_loss'),\n",
    "                 ReduceLROnPlateau('val_loss', patience=5, factor=0.7), \n",
    "                 TerminateOnNaN()]\n",
    "\n",
    "    history = model.fit(X_train_b, y_train_b, epochs=epochs_search, validation_data=(X_val_b, y_val_b) ,batch_size=batch_size, verbose=0, callbacks=callbacks)\n",
    "    val_loss = np.min(history.history['val_loss'])\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "total_time_start = time.time()  \n",
    "search_time_start = time.time() \n",
    "num_completed_trials = 0\n",
    "search_rounds = 0\n",
    "time_taken = 0\n",
    "while time_taken < max_search_time:\n",
    "        \n",
    "    fold = 0 \n",
    "    kf = KFold(n_splits=folds)\n",
    "    \n",
    "    time_fold_start = time.time()    \n",
    "    for train_index, val_index in kf.split(X_train_NN):\n",
    "\n",
    "        print('-------------------')\n",
    "        print(f\"Starting fold {fold} search...\")\n",
    "        X_train_b, X_val_b = X_train_NN[train_index], X_train_NN[val_index]    \n",
    "        y_train_b, y_val_b = y_train_NN[train_index], y_train_NN[val_index]\n",
    "\n",
    "        fold_name = f'{study_name}_{fold}'\n",
    "       \n",
    "        study = optuna.create_study(direction='minimize',\n",
    "                                    pruner=optuna.pruners.HyperbandPruner(min_resource=5),\n",
    "                                    study_name=fold_name,\n",
    "                                    storage=f'sqlite:///tampere_reg.db',\n",
    "                                    load_if_exists=True                                 \n",
    "                                    )\n",
    "        \n",
    "    \n",
    "        fold_time = time.time()    \n",
    "\n",
    "        fold_random = time.time()\n",
    "        # # study.sampler = optuna.samplers.RandomSampler()\n",
    "        study.sampler = optuna.samplers.QMCSampler(warn_independent_sampling = False) # TODO tämä testiin, vaikutti paljon paremmalta kuin random \n",
    "        print(f'Random search for fold {fold}...')\n",
    "        study.optimize(objective, n_trials=num_random)\n",
    "        print(f'Time taken for random search: {str(timedelta(seconds=(time.time() - fold_random)))}')\n",
    "\n",
    "        # fold_tpe = time.time()  \n",
    "        # study.sampler = optuna.samplers.TPESampler(n_startup_trials=0)\n",
    "        # print(f'TPE search for fold {fold}...')\n",
    "        # study.optimize(objective, n_trials=num_tpe)\n",
    "        # print(f'Time taken for TPE search: {str(timedelta(seconds=(time.time() - fold_tpe)))}')\n",
    "\n",
    "        num_completed_trials += num_random + num_tpe\n",
    "        print('-------------------')\n",
    "        print(f'Finished fold {fold} search.')\n",
    "        print(f\"Time taken for this fold: {str(timedelta(seconds=(time.time() - fold_time)))}\")                \n",
    "        print(f'Fold {fold} best value so far: {study.best_value}')\n",
    "        print(f'Mean time for one trial this fold: {str(timedelta(seconds=(time.time() - fold_time) / (num_random + num_tpe)))}')\n",
    "\n",
    "        fold += 1\n",
    "    search_rounds += 1\n",
    "    \n",
    "    time_taken = time.time() - search_time_start\n",
    "    \n",
    "    print(f'\\n# Completed search round: {search_rounds} #')\n",
    "    print(f'Time taken for all folds this round: {str(timedelta(seconds=(time.time() - time_fold_start)))}')\n",
    "    print(f'Total time taken for search: {str(timedelta(seconds=(time.time() - search_time_start)))}')\n",
    "    print(f'Made trials this far: {num_completed_trials}')\n",
    "    print(f\"Current mean time for one trial: {str(timedelta(seconds=(time.time() - search_time_start) / num_completed_trials))}\\n\")\n",
    "\n",
    "####\n",
    "\n",
    "####\n",
    "\n",
    "# num_tpe = 42\n",
    "    # num_random = 0\n",
    "    # max_search_time = 7200\n",
    "    \n",
    "# while time_taken < max_search_time:\n",
    "        \n",
    "#     fold = 0 \n",
    "#     kf = KFold(n_splits=folds)\n",
    "    \n",
    "#     time_fold_start = time.time()    \n",
    "#     for train_index, val_index in kf.split(X_train_NN):\n",
    "\n",
    "#         print('-------------------')\n",
    "#         print(f\"Starting fold {fold} search...\")\n",
    "#         X_train_b, X_val_b = X_train_NN[train_index], X_train_NN[val_index]    \n",
    "#         y_train_b, y_val_b = y_train_NN[train_index], y_train_NN[val_index]\n",
    "\n",
    "#         fold_name = f'{study_name}_{fold}'\n",
    "       \n",
    "#         study = optuna.create_study(direction='minimize',\n",
    "#                                     pruner=optuna.pruners.HyperbandPruner(min_resource=20),\n",
    "#                                     study_name=fold_name,\n",
    "#                                     storage=f'sqlite:///tampere_reg.db',\n",
    "#                                     load_if_exists=True                                 \n",
    "#                                     )\n",
    "\n",
    "#         fold_time = time.time()    \n",
    "\n",
    "#         # fold_random = time.time()\n",
    "#         # study.sampler = optuna.samplers.RandomSampler()\n",
    "#         # print(f'Random search for fold {fold}...')\n",
    "#         # study.optimize(objective, n_trials=num_random)\n",
    "#         # print(f'Time taken for random search: {str(timedelta(seconds=(time.time() - fold_random)))}')\n",
    "\n",
    "#         fold_tpe = time.time()  \n",
    "#         study.sampler = optuna.samplers.TPESampler(n_startup_trials=0)\n",
    "#         print(f'TPE search for fold {fold}...')\n",
    "#         study.optimize(objective, n_trials=num_tpe)\n",
    "#         print(f'Time taken for TPE search: {str(timedelta(seconds=(time.time() - fold_tpe)))}')\n",
    "\n",
    "#         num_completed_trials += num_random + num_tpe\n",
    "#         print('-------------------')\n",
    "#         print(f'Finished fold {fold} search.')\n",
    "#         print(f\"Time taken for this fold: {str(timedelta(seconds=(time.time() - fold_time)))}\")                \n",
    "#         print(f'Fold {fold} best value so far: {study.best_value}')\n",
    "#         print(f'Mean time for one trial this fold: {str(timedelta(seconds=(time.time() - fold_time) / (num_random + num_tpe)))}')\n",
    "\n",
    "#         fold += 1\n",
    "#     search_rounds += 1\n",
    "    \n",
    "#     time_taken = time.time() - search_time_start\n",
    "    \n",
    "#     print(f'\\n# Completed search round: {search_rounds} #')\n",
    "#     print(f'Time taken for all folds this round: {str(timedelta(seconds=(time.time() - time_fold_start)))}')\n",
    "#     print(f'Total time taken for search: {str(timedelta(seconds=(time.time() - search_time_start)))}')\n",
    "#     print(f'Made trials this far: {num_completed_trials}')\n",
    "#     print(f\"Current mean time for one trial: {str(timedelta(seconds=(time.time() - search_time_start) / num_completed_trials))}\\n\")\n",
    "\n",
    "###\n",
    "\n",
    "print('='*20)    \n",
    "print(f'Finished search.')    \n",
    "print(f'Total time taken for all folds: {str(timedelta(seconds=(time.time() - search_time_start)))}')\n",
    "print(f'Made {num_completed_trials} trials in total.')\n",
    "print(f\"Mean time for one trial: {str(timedelta(seconds=(time.time() - search_time_start) / num_completed_trials))}\")\n",
    "print('='*20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import optuna\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from keras import regularizers, layers, optimizers, initializers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "import time \n",
    "import os \n",
    "\n",
    "def rmsle_score(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(np.log1p(y_true+1), np.log1p(y_pred+1)))\n",
    "\n",
    "folds = 5\n",
    "# Montako epochia kullekin parhaalle sovitetaan malli\n",
    "epochs_best_fit = 500\n",
    "# Montako paras otetaan mukaan osittelusta\n",
    "num_best = 4\n",
    "# Montako kertaa kullekin parhaalle sovitetaan malli\n",
    "num_best_fits = 1\n",
    "\n",
    "best_optuna_models = []\n",
    "best_val_scores = []\n",
    "best_optuna_trials = [] \n",
    "\n",
    "kf = KFold(n_splits=folds)\n",
    "fold_num = 0\n",
    "fitting_search_start = time.time()\n",
    "\n",
    "for train_index, val_index in kf.split(X_train_NN):\n",
    "\n",
    "    best_fitting_time = time.time()\n",
    "    print(f\"Fold {fold_num} Best best trial fitting...\")\n",
    "\n",
    "    X_train_b, X_val_b = X_train_NN[train_index], X_train_NN[val_index]    \n",
    "    y_train_b, y_val_b = y_train_NN[train_index], y_train_NN[val_index]\n",
    "    \n",
    "    fold_name = f'{study_name}_{fold_num}'\n",
    "       \n",
    "    study = optuna.create_study(                                \n",
    "                                study_name=fold_name,\n",
    "                                storage=f'sqlite:///tampere_reg.db',\n",
    "                                load_if_exists=True\n",
    "                                )\n",
    "\n",
    "    valid_trials = [trial for trial in study.trials if trial.value is not None]\n",
    "    sorted_trials = sorted(valid_trials, key=lambda trial: trial.value)\n",
    "    best_trials = sorted_trials[:num_best]\n",
    "    best_val = np.inf\n",
    "    best_model = None\n",
    "\n",
    "    print('='*30)\n",
    "    print(f'Fitting best trials for fold {fold_num}...')\n",
    "    fitting_fold_best_start = time.time()\n",
    "    \n",
    "    for trial in best_trials:\n",
    "\n",
    "        for fit_num in range(num_best_fits):\n",
    "            \n",
    "            print('-'*30)\n",
    "            print(f\"Trial ID: {trial.number}, Value: {trial.value}, fit number: {fit_num}\")\n",
    "\n",
    "            checkpoint_filepath = f'./NN_search/optuna_search_checkpoint.h5'\n",
    "            model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=checkpoint_filepath,\n",
    "                save_weights_only=True,\n",
    "                monitor='val_loss',\n",
    "                mode='min',\n",
    "                save_best_only=True)\n",
    "\n",
    "            best_callback = [model_checkpoint_callback,                  \n",
    "                            ReduceLROnPlateau('val_loss', patience=10, factor=0.8), \n",
    "                            TerminateOnNaN(),\n",
    "                            EarlyStopping(monitor='val_loss', patience=100, verbose=1)\n",
    "                        ]\n",
    "\n",
    "\n",
    "            model = create_model(trial)\n",
    "            model.fit(X_train_b, y_train_b, epochs=epochs_best_fit, validation_data=(X_val_b, y_val_b), batch_size=trial.params['batch_size'], verbose=0, callbacks=best_callback)\n",
    "            model.load_weights(checkpoint_filepath)\n",
    "\n",
    "            predictions = model.predict(X_val_b, verbose=0)\n",
    "            mse = mean_squared_error(y_val_b, predictions)\n",
    "            mae = mean_absolute_error(y_val_b, predictions)\n",
    "            r2 = r2_score(y_val_b, predictions)\n",
    "            rmsle = rmsle_score(y_val_b, predictions)\n",
    "\n",
    "                        \n",
    "            print(f'MSE:{mse:.5f}\\nMAE:{mae:.5f}\\nRMSLE:{rmsle:.5f}\\nR2:{r2:.5f}')\n",
    "\n",
    "            if rmsle < best_val:\n",
    "                best_model = model\n",
    "                best_val = rmsle\n",
    "                best_trial_num = trial.number\n",
    "                best_trial = trial\n",
    "                print(f'*** New best model for fold {fold_num} is Trial {best_trial_num} with RMSLE {best_val} ***')\n",
    "                print(f'Best trial hyperparameters: {trial.params}')\n",
    "    \n",
    "    if best_model is not None:\n",
    "\n",
    "        best_optuna_models.append(best_model)\n",
    "        best_val_scores.append(best_val)\n",
    "        best_optuna_trials.append(best_trial)\n",
    "        print('*'*40)\n",
    "        print(f\"Best model for fold {fold_num} RMSLE: {best_val}\\nTrial number: {best_trial_num}\\nHyperparameters: {best_trial.params}\")\n",
    "        print(f\"Time taken for best fitting in fold {fold_num}: {str(timedelta(seconds=(time.time() - best_fitting_time)) )}\")\n",
    "        print('*'*40)\n",
    "\n",
    "    fold_num += 1\n",
    "\n",
    "print('*'*40)\n",
    "print(f'Best models fitting time total:', str(timedelta(seconds=(time.time() - fitting_search_start))))\n",
    "print(f\"Total time taken for search and fitting best models: {str(timedelta(seconds=(time.time() - total_time_start)))}\")\n",
    "print('*'*40)   \n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "for i, (model, score) in enumerate(zip(best_optuna_models, best_val_scores)):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    directory = f\"./NN_search/{study_name}_foldmodel{i}_score_{score:.4f}_{timestamp}.h5\"\n",
    "    print(f\"Saving model {i} with score {score:.4f} to {directory}\")\n",
    "    model.save(directory)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "\n",
    "# Oletetaan, että rmsle_score ja create_model funktiot ovat määritelty\n",
    "study_name = 'rmsle5_random_2503'\n",
    "folds = 5\n",
    "epochs_best_fit = 500\n",
    "\n",
    "# Ladataan kaikki studyt ja etsitään globaalisti paras trial\n",
    "best_global_val = float('inf')\n",
    "best_global_trial = None\n",
    "best_optuna_models_global = []\n",
    "\n",
    "for fold_num in range(folds):\n",
    "    \n",
    "     \n",
    "    fold_name = f'{study_name}_{fold_num}'    \n",
    "    study = optuna.create_study(                                \n",
    "                                study_name=fold_name,\n",
    "                                storage=f'sqlite:///tampere_reg.db',\n",
    "                                load_if_exists=True\n",
    "                                )\n",
    "    valid_trials = [trial for trial in study.trials if trial.value is not None]\n",
    "    sorted_trials = sorted(valid_trials, key=lambda trial: trial.value)\n",
    "    best_trial = sorted_trials[0].value\n",
    "\n",
    "    if best_trial < best_global_val:\n",
    "        best_global_val = best_trial\n",
    "        best_global_trial = sorted_trials[0]\n",
    "        best_fold = fold_num\n",
    "        print(f'New best global trial value: {best_global_val:.4f} found in fold {best_fold}')\n",
    "        \n",
    "print(f'Best global trial value: {best_global_val:.4f} tahat found in fold {best_fold}')\n",
    "\n",
    "# Nyt meillä on paras trial, jota käytetään kaikkien foldien kouluttamiseen\n",
    "kf = KFold(n_splits=folds)\n",
    "fold_num = 0\n",
    "\n",
    "for train_index, val_index in kf.split(X_train_NN):\n",
    "    print(f\"Koulutetaan fold {fold_num} käyttäen parasta globaalia trialia...\")\n",
    "    \n",
    "    X_train_b, X_val_b = X_train_NN[train_index], X_train_NN[val_index]\n",
    "    y_train_b, y_val_b = y_train_NN[train_index], y_train_NN[val_index]\n",
    "\n",
    "    # Luodaan malli parhaan trialin parametreilla\n",
    "    model = create_model(best_global_trial)\n",
    "\n",
    "    checkpoint_filepath = f'./NN_search/optuna_search_checkpoint.h5'\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        save_weights_only=True,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_best_only=True)\n",
    "\n",
    "\n",
    "    best_callback = [model_checkpoint_callback,                  \n",
    "                        ReduceLROnPlateau('val_loss', patience=10, factor=0.8), \n",
    "                        TerminateOnNaN(),\n",
    "                        EarlyStopping(monitor='val_loss', patience=100, verbose=1)\n",
    "                    ]\n",
    "    \n",
    "    # Koulutetaan malli\n",
    "    model.fit(X_train_b, y_train_b, epochs=epochs_best_fit, validation_data=(X_val_b, y_val_b), batch_size=best_global_trial.params['batch_size'], verbose=0, callbacks=best_callback)\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    \n",
    "    # Tarkistetaan mallin suorituskykyä (tämä osa voi vaatia mukauttamista projektisi tarpeisiin)\n",
    "    predictions = model.predict(X_val_b)\n",
    "    rmsle = rmsle_score(y_val_b, predictions)\n",
    "    print(f\"Fold {fold_num} RMSLE: {rmsle}\")\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    directory = f\"./NN_search/{study_name}_best_foldmodel{fold_num}_score_{rmsle:.4f}_{timestamp}.h5\"\n",
    "    print(f\"Saving model {fold_num} with score {rmsle:.4f} to {directory}\")\n",
    "    model.save(directory)\n",
    "    best_optuna_models_global.append(model)\n",
    "    fold_num += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, model in enumerate(best_optuna_models):\n",
    "    print(f\"\\nModel {idx} Summary:\")\n",
    "    # model.summary()\n",
    "    \n",
    "    # Testaa mallia testidatalla\n",
    "    predictions = model.predict(X_test_NN, verbose = 0)\n",
    "    mse = mean_squared_error(y_test_NN, predictions)\n",
    "    mae = mean_absolute_error(y_test_NN, predictions)\n",
    "    r2 = r2_score(y_test_NN, predictions)\n",
    "    rmsle = rmsle_score(y_test_NN, predictions)\n",
    "    \n",
    "    print(f\"\\nModel {idx} Performance on Test Data:\")\n",
    "    print(f\"MSE: {mse:.3f}\")\n",
    "    print(f\"MAE: {mae:.3f}\")\n",
    "    print(f\"R2: {r2:.3f}\")\n",
    "    print(f\"RMSLE: {rmsle:.3f}\")\n",
    "    print(\"*\"*40)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Models with best global trial fitted')\n",
    "for idx, model in enumerate(best_optuna_models):\n",
    "    print(f\"\\nModel {idx} Summary:\")\n",
    "    # model.summary()\n",
    "    \n",
    "    # Testaa mallia testidatalla\n",
    "    predictions = model.predict(X_test_NN, verbose = 0)\n",
    "    mse = mean_squared_error(y_test_NN, predictions)\n",
    "    mae = mean_absolute_error(y_test_NN, predictions)\n",
    "    r2 = r2_score(y_test_NN, predictions)\n",
    "    rmsle = rmsle_score(y_test_NN, predictions)\n",
    "    \n",
    "    print(f\"\\nModel {idx} Performance on Test Data:\")\n",
    "    print(f\"MSE: {mse:.3f}\")\n",
    "    print(f\"MAE: {mae:.3f}\")\n",
    "    print(f\"R2: {r2:.3f}\")\n",
    "    print(f\"RMSLE: {rmsle:.3f}\")\n",
    "    print(\"*\"*40)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "import time\n",
    "from tensorflow.keras.models import Model\n",
    "import glob\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "study_name = 'rmsle5_random_2503'\n",
    "\n",
    "time_started_xgb = time.time()\n",
    "\n",
    "num_of_trials = 1420\n",
    "\n",
    "# Kerätään ensin kaikkien mallien ominaisuusvektorit\n",
    "X_train_features_list = []\n",
    "X_test_features_list = []\n",
    "features_names_list = []\n",
    "\n",
    "\n",
    "\n",
    "def rmsle_loss(y_true, y_pred):\n",
    "    penalty = tf.constant(1e5, dtype=tf.float32)\n",
    "    valid_mask = tf.math.greater(y_pred, 0.0)\n",
    "    safe_y_pred = tf.where(valid_mask, y_pred, penalty)\n",
    "    rmsle = tf.sqrt(tf.reduce_mean(tf.square(tf.math.log1p(safe_y_pred) - tf.math.log1p(y_true))))\n",
    "    return tf.where(tf.reduce_any(~valid_mask), penalty, rmsle)\n",
    "\n",
    "# Kun lataat mallin, määritä mukautettu häviöfunktio custom_objects-parametrissa\n",
    "custom_objects = {\"rmsle_loss\": rmsle_loss}\n",
    "\n",
    "model_best_vals = []\n",
    "best_optuna_models = []\n",
    "\n",
    "folds = 5\n",
    "# for fold_num in [1]:\n",
    "for fold_num in range(folds): # TODO testiä parhailla malleilla\n",
    "    patterns = [\n",
    "        f\"./NN_search/{study_name}_best_foldmodel{fold_num}_score_*.h5\",\n",
    "        f\"./NN_search/{study_name}_foldmodel{fold_num}_score_*.h5\"\n",
    "    ]\n",
    "    # model_files = glob.glob(pattern)\n",
    "    \n",
    "    # Etsi suurin score kunkin foldin mallitiedostoista\n",
    "    best_score = float('inf')\n",
    "    best_model_file = None\n",
    "    for pattern in patterns:\n",
    "        model_files = glob.glob(pattern)\n",
    "        for model_file in model_files:\n",
    "            score_part = model_file.split('_score_')[1]\n",
    "            score = float(score_part.split('_')[0])\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_model_file = model_file\n",
    "                    \n",
    "    model_best_vals.append(best_score)    \n",
    "    # Lataa parhaan mallin tiedosto\n",
    "    if best_model_file:\n",
    "        best_model = load_model(best_model_file, custom_objects=custom_objects)\n",
    "        best_optuna_models.append(best_model)\n",
    "        print(f\"Loaded best model for fold {fold_num} from {best_model_file} with score {best_score:.4f}\")\n",
    "    else:\n",
    "        print(f\"No model files found for fold {fold_num} matching pattern {pattern}\")\n",
    "\n",
    "\n",
    "# best_models_per_fold-listas\n",
    "\n",
    "original_feature_names = list(X_train.columns) \n",
    "\n",
    "for idx, model in enumerate(best_optuna_models):\n",
    "    feature_extractor = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "    X_train_features = feature_extractor.predict(X_train_NN)\n",
    "    X_test_features = feature_extractor.predict(X_test_NN)\n",
    "    \n",
    "    X_train_features_list.append(X_train_features)\n",
    "    X_test_features_list.append(X_test_features)\n",
    "\n",
    "    print(f'Model train feature shape: {X_train_features.shape}')\n",
    "    print(f'Model test feature shape: {X_test_features.shape}')\n",
    "\n",
    "    num_features = X_train_features.shape[1]\n",
    "    model_feature_names = [f\"model_{idx}_feature_{feature_idx}\" for feature_idx in range(num_features)]\n",
    "    features_names_list.extend(model_feature_names)\n",
    "\n",
    "combined_feature_names = original_feature_names + features_names_list\n",
    "\n",
    "# Yhdistetään ominaisuusvektorit\n",
    "X_train_combined = np.concatenate(X_train_features_list, axis=1)\n",
    "X_test_combined = np.concatenate(X_test_features_list, axis=1)\n",
    "\n",
    "X_train_combined = np.concatenate([X_train_combined, X_train], axis=1)   \n",
    "X_test_combined = np.concatenate([X_test_combined, X_test], axis=1)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "\n",
    "#### Ei valintaa \n",
    "# X_train_combined_selected = X_train_combined\n",
    "# X_test_combined_selected = X_test_combined\n",
    "# selected_features_names = combined_feature_names\n",
    "\n",
    "\n",
    "##### mutual_info_regression Valinta\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "\n",
    "max_feature = X_train_combined.shape[1] // 2\n",
    "\n",
    "selector = SelectKBest(mutual_info_regression, k=max_feature)\n",
    "\n",
    "X_train_combined_selected = selector.fit_transform(X_train_combined, y_train)\n",
    "X_test_combined_selected = selector.transform(X_test_combined)\n",
    "\n",
    "selected_indices = selector.get_support(indices=True)\n",
    "selected_features_names = np.array(combined_feature_names)[selected_indices]\n",
    "selected_features_scores = selector.scores_[selected_indices]\n",
    "\n",
    "print(\"Selected features and their scores:\")\n",
    "for name, score in zip(selected_features_names, selected_features_scores):\n",
    "    print(f\"{name}: {score}\")\n",
    "\n",
    "selected_features_names = selected_features_names.tolist()\n",
    "\n",
    "print(f'X_train_combined shape: {X_train_combined.shape}')\n",
    "print(f'X_train_combined_selected shape: {X_train_combined_selected.shape}')\n",
    "\n",
    "print(f'X_test_combined shape: {X_test_combined.shape}')\n",
    "print(f'X_test combined selected shape: {X_test_combined_selected.shape}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ##### PCA valinta\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Oletetaan, että X_train_combined ja X_test_combined ovat datasi\n",
    "# # Ja combined_feature_names on alkuperäisten featureiden nimilista\n",
    "\n",
    "# # Skaalataan data\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train_combined)\n",
    "# X_test_scaled = scaler.transform(X_test_combined)\n",
    "\n",
    "# # Sovellamme PCA:ta\n",
    "# pca = PCA(n_components=0.85) # tai voit määrittää n_components arvon eksplisiittisesti\n",
    "# X_train_combined_selected = pca.fit_transform(X_train_scaled)\n",
    "# X_test_combined_selected = pca.transform(X_test_scaled)\n",
    "\n",
    "# # Luodaan uudet feature-nimet pääkomponenteille\n",
    "# selected_features_names = [f\"PC{i+1}\" for i in range(X_train_combined_selected.shape[1])]\n",
    "\n",
    "# print(f\"Alkuperäinen featureiden määrä: {X_train_combined.shape[1]}\")\n",
    "# print(f\"Featureiden määrä PCA:n jälkeen: {X_train_combined_selected.shape[1]}\")\n",
    "\n",
    "# # Nyt sinulla on X_train_pca ja X_test_pca datat sekä niiden vastaavat feature-nimet\n",
    "# # Voit jatkaa näiden käyttämistä mallisi koulutukseen\n",
    "\n",
    "\n",
    "\n",
    "#####\n",
    "\n",
    "def rmsle(y_true, y_pred):\n",
    "    if np.any(y_pred <= 0):\n",
    "        return 1e6\n",
    "    return np.sqrt(np.mean(np.square(np.log1p(y_pred) - np.log1p(y_true))))\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "num_random = 420\n",
    "num_tpe = 42\n",
    "\n",
    "max_feature = X_train_combined.shape[1]\n",
    "\n",
    "def objective(trial):\n",
    "    # Määritetään parametrit, jotka optimoidaan\n",
    "    param = {\n",
    "        'device' : 'cuda',\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"dart\"]),\n",
    "        'objective': trial.suggest_categorical('objective', ['reg:squarederror', 'reg:absoluteerror']),        \n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log = True),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log = True),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 10), \n",
    "    }\n",
    "\n",
    "    num_boost_round = trial.suggest_int('num_boost_round', 1, 142)  \n",
    "\n",
    "    # K-fold cross-validation\n",
    "    kf = KFold(n_splits=5)\n",
    "    rmsle_scores = []\n",
    "  \n",
    "    for train_index, val_index in kf.split(X_train_combined_selected):\n",
    "        X_train_k, X_val_k = X_train_combined_selected[train_index], X_train_combined_selected[val_index]\n",
    "        y_train_k, y_val_k = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_train_k, label=y_train_k, feature_names=selected_features_names)\n",
    "        dval = xgb.DMatrix(X_val_k, label=y_val_k, feature_names=selected_features_names)\n",
    "\n",
    "        evals_result = {}\n",
    "        bst = xgb.train(param, dtrain, num_boost_round=num_boost_round, evals=[(dval, 'val')], evals_result=evals_result, verbose_eval=False, early_stopping_rounds=1000)\n",
    "        # Käytä paras iteraatio määrä laskemaan ennusteet ja RMSLE\n",
    "        best_iteration = bst.best_iteration\n",
    "        preds = bst.predict(dval, iteration_range=(0, best_iteration + 1))\n",
    "        loss = r2_score(y_val_k, preds)\n",
    "        rmsle_scores.append(loss)\n",
    "\n",
    "    average_rmsle = np.mean(rmsle_scores)\n",
    "    return average_rmsle\n",
    "\n",
    "# study = optuna.create_study(direction='maximize', \n",
    "#                             storage='sqlite:///tampere_reg.db', \n",
    "#                             study_name='0326_xgb_comb_R2', # TODO muuta nimeä tarvittaessa\n",
    "#                             load_if_exists=False) \n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "\n",
    "study.sampler = optuna.samplers.QMCSampler(warn_independent_sampling = False)\n",
    "print(f'Random sampling {num_random} trials...')\n",
    "study.optimize(objective, n_trials=num_random)\n",
    "study.sampler = optuna.samplers.TPESampler()\n",
    "print(f'TPE sampling {num_tpe} trials...')\n",
    "study.optimize(objective, n_trials=num_tpe)\n",
    "\n",
    "print(f'Time taken for XGBoost optimization: {str(timedelta(seconds=(time.time() - time_started_xgb)))}')\n",
    "print(f'Time taken for one trial: {str(timedelta(seconds=(time.time() - time_started_xgb) / (num_random + num_tpe)))}')\n",
    "\n",
    "# Parhaiden parametrien tulostus ja mallin koulutus\n",
    "print(f\"Best val: {study.best_trial.value}\")\n",
    "print(f'Best params: {study.best_params}')\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train_combined_selected, label=y_train, feature_names=selected_features_names)\n",
    "best_model = xgb.train(study.best_params, dtrain, num_boost_round=study.best_params['num_boost_round'])\n",
    "\n",
    "# Ennustukset ja evaluointi testidatalla\n",
    "dtest = xgb.DMatrix(X_test_combined_selected, label=y_test, feature_names=selected_features_names)\n",
    "predictions = best_model.predict(dtest)\n",
    "\n",
    "# Visualisoidaan ennustettuja arvoja verrattuna todellisiin arvoihin\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(y_test, predictions, edgecolors=(0, 0, 0))\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "plt.xlabel('Measured')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Measured vs. Predicted Values')\n",
    "plt.show()\n",
    "\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "rmsle_val = rmsle(y_test, predictions)\n",
    "print(f\"MAE: {mae}, MSE: {mse}, R2: {r2}, RMSLE: {rmsle_val}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 30))\n",
    "xgb.plot_importance(best_model, importance_type='weight', ax=ax)\n",
    "ax.set_title('Feature Importance by Weight', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Asetetaan toisen kuvaajan koko\n",
    "fig, ax = plt.subplots(figsize=(20, 30))\n",
    "xgb.plot_importance(best_model, importance_type='gain', ax=ax)\n",
    "ax.set_title('Feature Importance by Gain', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Asetetaan kolmannen kuvaajan koko\n",
    "fig, ax = plt.subplots(figsize=(20, 30))\n",
    "xgb.plot_importance(best_model, importance_type='cover', ax=ax)\n",
    "ax.set_title('Feature Importance by Cover', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = []\n",
    "\n",
    "for idx, model in enumerate(best_optuna_models):\n",
    "    pred = model.predict(X_train_NN, verbose=0).flatten()\n",
    "    predictions_train.append(pred)\n",
    "\n",
    "predictions_test = []\n",
    "\n",
    "for idx, model in enumerate(best_optuna_models):\n",
    "    pred = model.predict(X_test_NN, verbose=0).flatten()\n",
    "    predictions_test.append(pred)\n",
    "\n",
    "### Keskiarvo \n",
    "print('Keskiarvo ')\n",
    "predictions_mean = np.mean(predictions_test, axis=0)\n",
    "mse = mean_squared_error(y_test, predictions_mean)\n",
    "mae = mean_absolute_error(y_test, predictions_mean)\n",
    "r2 = r2_score(y_test, predictions_mean)\n",
    "rmsle_val = rmsle(y_test, predictions_mean)  # Oletetaan että sinulla on rmsle funktio määritelty\n",
    "print(f\"MAE: {mae}, MSE: {mse}, R2: {r2}, RMSLE: {rmsle_val}\\n\\n\")\n",
    "\n",
    "### Painotettu keskiarvo\n",
    "print('Painotettu keskiarvo käänteisillä')\n",
    "for pot in range (1,20):\n",
    "    print(f'Potenssi: {pot}')\n",
    "    weights = [1 / x**pot for x in model_best_vals]\n",
    "    w_sum = sum(weights)\n",
    "    weights = [x / w_sum for x in weights]\n",
    "    weighted_predictions = np.average(predictions_test, axis=0, weights=weights)\n",
    "    mse = mean_squared_error(y_test, weighted_predictions)\n",
    "    mae = mean_absolute_error(y_test, weighted_predictions)\n",
    "    r2 = r2_score(y_test, weighted_predictions)\n",
    "    rmsle_val = rmsle(y_test, weighted_predictions)  # Oletetaan että sinulla on rmsle funktio määritelty\n",
    "    print(f\"MAE: {mae}, MSE: {mse}, R2: {r2}, RMSLE: {rmsle_val}\")\n",
    "\n",
    "### Lineaarinen regressio \n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Oletetaan, että `predictions` on lista, joka sisältää kunkin mallin ennusteet testidatasetille\n",
    "X_meta_train = np.stack(predictions_train, axis=1)\n",
    "X_meta_test = np.stack(predictions_test, axis=1)\n",
    "# Koulutetaan meta-malli\n",
    "meta_model = LinearRegression()\n",
    "meta_model.fit(X_meta_train, y_train)\n",
    "\n",
    "# Käytetään meta-mallia ennustamaan\n",
    "linear_predictions = meta_model.predict(X_meta_test)\n",
    "\n",
    "print('Linear meta')\n",
    "mse = mean_squared_error(y_test, linear_predictions)\n",
    "mae = mean_absolute_error(y_test, linear_predictions)\n",
    "r2 = r2_score(y_test, linear_predictions)\n",
    "rmsle_val = rmsle(y_test, linear_predictions)  # Oletetaan että sinulla on rmsle funktio määritelty\n",
    "print(f\"MAE: {mae}, MSE: {mse}, R2: {r2}, RMSLE: {rmsle_val}\\n\\n\")\n",
    "\n",
    "### Tukivektorikone \n",
    "# time_svm = time.time()\n",
    "# import optuna\n",
    "# from sklearn.svm import SVR\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "# # Oletetaan, että 'X_train_svm' ja 'y_train' ovat koulutusdatasi ja -tavoitteet\n",
    "# X_train_svm = np.column_stack(predictions_train)\n",
    "# y_train = y_train  # Varmista, että tämä on koulutusdatan tavoitevektori\n",
    "# X_test_svm = np.column_stack(predictions_test)\n",
    "\n",
    "# def objective(trial):\n",
    "#     # Ehdota hyperparametrien arvoja\n",
    "#     kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid'])\n",
    "#     C = trial.suggest_float('C', 0.1, 100, log=True)\n",
    "#     gamma = trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
    "#     epsilon = trial.suggest_float('epsilon', 0.01, 1.0)\n",
    "#     degree = trial.suggest_int('degree', 1, 5) if kernel == 'poly' else 3  # Polynomiydinkerroin\n",
    "    \n",
    "#     # Luo ja kouluta SVR-malli ehdotetuilla hyperparametreilla\n",
    "#     svm_regressor = SVR(kernel=kernel, C=C, gamma=gamma, epsilon=epsilon, degree=degree)\n",
    "#     score = cross_val_score(svm_regressor, X_train_svm, y_train, n_jobs=-1, cv=5, scoring=make_scorer(rmsle)).mean()\n",
    "    \n",
    "#     return score\n",
    "\n",
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.sampler = optuna.samplers.QMCSampler(warn_independent_sampling = False)\n",
    "# study.optimize(objective, n_trials=1000)\n",
    "# print(f'Random sampling trials...')\n",
    "# study.sampler = optuna.samplers.TPESampler()\n",
    "# print(f'TPE sampling trials...')\n",
    "# study.optimize(objective, n_trials=142)\n",
    "\n",
    "# print(f'Best value: {study.best_value}')\n",
    "# print(f\"Best trial: {study.best_trial.params}\")\n",
    "\n",
    "# # Lataa parhaan mallin parametrit\n",
    "# best_params = study.best_trial.params\n",
    "\n",
    "# # Luo uusi SVR-malli parhaiden löydettyjen hyperparametrien kanssa\n",
    "# best_svm_regressor = SVR(kernel=best_params['kernel'], C=best_params['C'], \n",
    "#                          gamma=best_params['gamma'], epsilon=best_params['epsilon'], \n",
    "#                          degree=best_params.get('degree', 3))  # degree lisätään vain, jos kernel on 'poly'\n",
    "\n",
    "# # Kouluta malli koko koulutusdatasetillä\n",
    "# best_svm_regressor.fit(X_train_svm, y_train)\n",
    "\n",
    "# # Ennusta testidatan arvot\n",
    "# predictions_svm = best_svm_regressor.predict(X_test_svm)\n",
    "\n",
    "# # Arvioidaan mallin suorituskykyä testidatalla\n",
    "# mse = mean_squared_error(y_test, predictions_svm)\n",
    "# mae = mean_absolute_error(y_test, predictions_svm)\n",
    "# r2 = r2_score(y_test, predictions_svm)\n",
    "# rmsle_val = rmsle(y_test, predictions_svm)  # Logaritminen virhe\n",
    "\n",
    "# print(f\"Parhaan mallin tulokset testidatalla:\")\n",
    "# print(f\"MAE: {mae}, MSE: {mse}, R2: {r2}, RMSLE: {rmsle_val}\")\n",
    "# print(f'Time taken for SVM optimization: {str(timedelta(seconds=(time.time() - time_svm)))}')\n",
    "\n",
    "\n",
    "\n",
    "### XGBoost \n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "time_xgb = time.time()\n",
    "\n",
    "NN_names = [f'NN_{i}' for i in range(len(best_optuna_models))]\n",
    "\n",
    "X_train_XGB = np.column_stack(predictions_train)\n",
    "X_test_XGB = np.column_stack(predictions_test)\n",
    "\n",
    "def objective(trial):\n",
    "    # XGBoostin parametrit, jotka optimoidaan\n",
    "    param = {        \n",
    "        'objective': trial.suggest_categorical('objective', ['reg:squarederror', 'reg:absoluteerror']),\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"dart\"]),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log = True),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log = True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 9),\n",
    "        \"learning_rate\": trial.suggest_float(\"eta\", 1e-5, 1.0, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0)\n",
    "    }\n",
    "\n",
    "    num_boost_round = trial.suggest_int('num_boost_round', 1, 142)  \n",
    "\n",
    "    kf = KFold(n_splits=5)\n",
    "    rmsle_scores = []\n",
    "\n",
    "    for train_index, val_index in kf.split(X_train_XGB):\n",
    "        X_train_k, X_val_k = X_train_XGB[train_index], X_train_XGB[val_index]\n",
    "        y_train_k, y_val_k = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_train_k, label=y_train_k, feature_names=NN_names)\n",
    "        dval = xgb.DMatrix(X_val_k, label=y_val_k, feature_names=NN_names)\n",
    "\n",
    "        evals_result = {}\n",
    "        bst = xgb.train(param, dtrain, num_boost_round=num_boost_round, evals=[(dval, 'val')], evals_result=evals_result, verbose_eval=False, early_stopping_rounds=200)\n",
    "        # Käytä paras iteraatio määrä laskemaan ennusteet ja RMSLE\n",
    "        best_iteration = bst.best_iteration\n",
    "        preds = bst.predict(dval, iteration_range=(0, best_iteration + 1))\n",
    "        loss = r2_score(y_val_k, preds)\n",
    "        rmsle_scores.append(loss)\n",
    "        \n",
    "    return np.mean(rmsle_scores)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.sampler = optuna.samplers.QMCSampler(warn_independent_sampling = False)\n",
    "study.optimize(objective, n_trials=42)\n",
    "print(f'Random sampling trials...')\n",
    "study.sampler = optuna.samplers.TPESampler()\n",
    "print(f'TPE sampling trials...')\n",
    "study.optimize(objective, n_trials=5)\n",
    "\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(f\"RMSLE: {trial.value}\")\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))\n",
    "\n",
    "# Koulutetaan paras malli uudelleen koko datasetillä\n",
    "best_params = trial.params\n",
    "dtrain = xgb.DMatrix(X_train_XGB, label=y_train, feature_names= NN_names)\n",
    "final_model = xgb.train(best_params, dtrain)\n",
    "\n",
    "dtest = xgb.DMatrix(X_test_XGB, label=y_test, feature_names=NN_names)\n",
    "\n",
    "predictions_XGB = final_model.predict(dtest)\n",
    "mse = mean_squared_error(y_test, predictions_XGB)\n",
    "mae = mean_absolute_error(y_test, predictions_XGB)\n",
    "r2 = r2_score(y_test, predictions_XGB)\n",
    "rmsle_val = rmsle(y_test, predictions_XGB)  \n",
    "\n",
    "print(f\"Parhaan mallin tulokset testidatalla:\")\n",
    "print(f\"MAE: {mae}, MSE: {mse}, R2: {r2}, RMSLE: {rmsle_val}\")\n",
    "print(f'Time taken for XGBoost optimization: {str(timedelta(seconds=(time.time() - time_xgb)))}')\n",
    "\n",
    "# Visualisoidaan ennustettuja arvoja verrattuna todellisiin arvoihin\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(y_test, predictions_XGB, edgecolors=(0, 0, 0))\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "plt.xlabel('Measured')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Measured vs. Predicted Values')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 30))\n",
    "xgb.plot_importance(final_model, importance_type='weight', ax=ax)\n",
    "ax.set_title('Feature Importance by Weight', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Asetetaan toisen kuvaajan koko\n",
    "fig, ax = plt.subplots(figsize=(20, 30))\n",
    "xgb.plot_importance(final_model, importance_type='gain', ax=ax)\n",
    "ax.set_title('Feature Importance by Gain', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Asetetaan kolmannen kuvaajan koko\n",
    "fig, ax = plt.subplots(figsize=(20, 30))\n",
    "xgb.plot_importance(final_model, importance_type='cover', ax=ax)\n",
    "ax.set_title('Feature Importance by Cover', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "joda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
