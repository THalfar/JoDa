{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_train_filtered = pd.read_pickle('./data/df_train_filtered.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_train_filtered.drop('Hinta', axis=1)\n",
    "y = df_train_filtered['Hinta']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=df_train_filtered['Kaupunginosa'], random_state=42)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score,  mean_absolute_error\n",
    "\n",
    "def rmsle_score(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(np.log1p(y_true+1), np.log1p(y_pred+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "predictions = lr.predict(X_test)\n",
    "\n",
    "r2 = r2_score(y_test, predictions)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print(f\"Mean squared error: {mse:.2f}\\nMean absolute error: {mae:.2f}\\nR²-arvo: {r2:.2f}\\nRMSLE: {rmsle_score(y_test, predictions):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "ax.scatter(y_test, predictions, edgecolors=(0, 0, 0))\n",
    "ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Measured')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.title('Measured vs. Predicted Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "def rmsle(y_true, y_pred):\n",
    "    if np.any(y_pred <= 0):\n",
    "        return 1e6\n",
    "    return np.sqrt(np.mean(np.square(np.log1p(y_pred) - np.log1p(y_true))))\n",
    "\n",
    "original_feature_names = list(X_train.columns) \n",
    "y_train_alku = np.array(y_train)\n",
    "X_train_alku = np.array(X_train)\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': trial.suggest_categorical('objective', ['reg:squarederror', 'reg:absoluteerror']),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 10),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1, 10),\n",
    "    }\n",
    "\n",
    "    rmsle_scores = []\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_train_k, X_val_k = X_train_alku[train_index], X_train_alku[val_index]\n",
    "        y_train_k, y_val_k = y_train_alku[train_index], y_train_alku[val_index]\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_train_k, label=y_train_k, feature_names=original_feature_names)\n",
    "        dval = xgb.DMatrix(X_val_k, label=y_val_k, feature_names=original_feature_names)\n",
    "\n",
    "        bst = xgb.train(params, dtrain, num_boost_round=trial.suggest_int('num_boost_round', 1, 100),\n",
    "                        evals=[(dval, 'eval')], early_stopping_rounds=50, verbose_eval=False)\n",
    "        preds = bst.predict(dval)\n",
    "        rmsle_score = rmsle(y_val_k, preds)\n",
    "        rmsle_scores.append(rmsle_score)\n",
    "\n",
    "    average_rmsle = np.mean(rmsle_scores)\n",
    "    return average_rmsle\n",
    "\n",
    "study = optuna.create_study(direction='minimize', storage = 'sqlite:///tampere_reg.db', study_name='xgboost_baseline2603', load_if_exists=True)\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(f'Best value is {study.best_value}')\n",
    "\n",
    "# Koulutetaan malli parhailla hyperparametreilla uudelleen koko datasetilla\n",
    "best_params = study.best_trial.params\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=original_feature_names)\n",
    "best_model = xgb.train(best_params, dtrain, num_boost_round=best_params['num_boost_round'])\n",
    "\n",
    "dtest = xgb.DMatrix(X_test, label=y_test, feature_names=original_feature_names)\n",
    "predictions = best_model.predict(dtest)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "rmsle_val = rmsle(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print(f\"MAE: {mae}, MSE: {mse}, RMSLE: {rmsle_val}, R2: {r2}\")\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(y_test, predictions, edgecolors=(0, 0, 0))\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "plt.xlabel('Measured')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Measured vs. Predicted Values')\n",
    "plt.show()\n",
    "\n",
    "# plt.figure(figsize=(15, 30))\n",
    "# xgb.plot_importance(best_model, importance_type='weight', title='Feature Importance by Weight')\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(15, 30))\n",
    "# xgb.plot_importance(best_model, importance_type='gain', title='Feature Importance by Gain')\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(15, 30))\n",
    "# xgb.plot_importance(best_model, importance_type='cover', title='Feature Importance by Cover')\n",
    "# plt.show()\n",
    "\n",
    "# fig = optuna.visualization.plot_param_importances(study)\n",
    "# fig.update_layout(width=1200, height=1200)\n",
    "# fig.show()\n",
    "\n",
    "# # Leikkaus\n",
    "# fig = optuna.visualization.plot_slice(study)\n",
    "# fig.update_layout(width=1200, height=1200)\n",
    "# fig.show()\n",
    "\n",
    "# # Rinnakkaiskoordinaatit\n",
    "# fig = optuna.visualization.plot_parallel_coordinate(study)\n",
    "# fig.update_layout(width=1200, height=1200)\n",
    "# fig.show()\n",
    "\n",
    "# # Kontuuri\n",
    "# fig = optuna.visualization.plot_contour(study, params=['max_depth', 'learning_rate', 'num_boost_round'])\n",
    "# fig.update_layout(width=1200, height=1200)\n",
    "# fig.show()\n",
    "\n",
    "# # Empiirinen kertymäfunktio (EDF)\n",
    "# fig = optuna.visualization.plot_edf(study)\n",
    "# fig.update_layout(width=1200, height=1200)\n",
    "# fig.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler \n",
    "\n",
    "# Skaalataan numeeriset muuttujat\n",
    "robust_scaler = RobustScaler()\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "df_train_NN = df_train_filtered.copy()\n",
    "df_train_NN[['Pituusaste', 'Leveysaste']] = minmax_scaler.fit_transform(df_train_NN[['Pituusaste', 'Leveysaste']])\n",
    "df_train_NN['Rv'] = minmax_scaler.fit_transform(df_train_NN[['Rv']])\n",
    "df_train_NN['m2'] = minmax_scaler.fit_transform(df_train_NN[['m2']])\n",
    "\n",
    "# One hot koodataan kategoriset muuttujat\n",
    "df_hot = pd.get_dummies(df_train_NN['Kaupunginosa'], prefix='Kaupunginosa').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN['kerros'], prefix='kerros').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN['max_kerros'], prefix='max_kerros').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN['Kunto'], prefix='Kunto').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN['Hissi'], prefix='Hissi').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN['Asunnon tyyppi'], prefix='Asunnon tyyppi').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "df_hot = pd.get_dummies(df_train_NN[\"Talot.\"], prefix='Talot.').astype('int')\n",
    "df_train_NN = pd.concat([df_train_NN, df_hot], axis=1)\n",
    "\n",
    "\n",
    "df_train_NN.drop(['Kaupunginosa', 'kerros', 'max_kerros', 'Kunto', 'Hissi', 'Asunnon tyyppi', \"Talot.\"], axis=1, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muodostetaan X ja y sekä jaetaan data harjoitus- ja testijoukkoihin\n",
    "\n",
    "X = df_train_NN.drop('Hinta', axis=1)\n",
    "y = df_train_NN['Hinta']\n",
    "\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy().astype('float32')\n",
    "\n",
    "X_train_NN, X_test_NN, y_train_NN, y_test_NN = train_test_split(X, y, test_size=0.1, random_state=42, stratify=df_train_filtered['Kaupunginosa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from keras import regularizers, layers, optimizers, initializers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "import time \n",
    "from optuna.integration import TFKerasPruningCallback\n",
    "import os \n",
    "import pickle \n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "# Haun nimi\n",
    "study_name = 'rmsle5_random_2503'\n",
    "# Montako osittelua käytettiin\n",
    "folds = 5\n",
    "# Montako epochia kullekin osittelulle\n",
    "epochs_search = 100\n",
    "# Montako satunnaista hakua kieroksella\n",
    "num_random = 42\n",
    "# Montako TPE hakua kieroksella\n",
    "num_tpe = 0\n",
    "\n",
    "# Aika sekuntteina jota hakuun käytetän\n",
    "max_search_time = 36000\n",
    "# Neuroneiden maksimimäärä \n",
    "max_units = 512\n",
    "\n",
    "def rmsle_loss(y_true, y_pred):\n",
    "    # Asetetaan suuri rangaistusarvo, jos y_pred sisältää arvon nolla tai alle\n",
    "    penalty = tf.constant(1e5, dtype=tf.float32)\n",
    "    \n",
    "    # Maski, joka on tosi, kun y_pred on > 0\n",
    "    valid_mask = tf.math.greater(y_pred, 0.0)\n",
    "    \n",
    "    # Käytä maskia valitsemaan joko oikea RMSLE laskenta tai suuri rangaistus\n",
    "    safe_y_pred = tf.where(valid_mask, y_pred, penalty)\n",
    "    \n",
    "    # Laske RMSLE vain, jos y_pred on suurempi kuin 0, muuten palauta rangaistus\n",
    "    rmsle = tf.sqrt(tf.reduce_mean(tf.square(tf.math.log1p(safe_y_pred) - tf.math.log1p(y_true))))\n",
    "    \n",
    "    # Palauta suuri rangaistus, jos y_pred sisälsi nollan tai negatiivisen arvon\n",
    "    return tf.where(tf.reduce_any(~valid_mask), penalty, rmsle)\n",
    "\n",
    "\n",
    "def create_model(trial):\n",
    "        \n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=(X_train_NN.shape[1],)))\n",
    "    \n",
    "    num_units = trial.suggest_int(f'n_units_0', 4, 512, log=True)\n",
    "    dropout_rate = trial.suggest_float(f'dropout_0', 0.0, 0.5)\n",
    "    kernel_regularizer=regularizers.l1_l2(\n",
    "            l1= trial.suggest_float(f'l1_reg_0', 1e-6, 1, log=True),\n",
    "            l2= trial.suggest_float(f'l2_reg_0', 1e-6, 1, log=True)\n",
    "        )\n",
    "    activation = trial.suggest_categorical(f'activation_0', ['relu', 'tanh', 'selu', 'linear', 'sigmoid', 'elu'])    \n",
    "    model.add(keras.layers.Dense(num_units, activation=activation, kernel_regularizer=kernel_regularizer))    \n",
    "    model.add(keras.layers.Dropout(rate=dropout_rate))\n",
    "\n",
    "    num_units = trial.suggest_int(f'n_units_1', 4, 256, log=True)\n",
    "    dropout_rate = trial.suggest_float(f'dropout_1', 0.0, 0.5)\n",
    "    kernel_regularizer=regularizers.l1_l2(\n",
    "            l1= trial.suggest_float(f'l1_reg_1', 1e-6, 1, log=True),\n",
    "            l2= trial.suggest_float(f'l2_reg_1', 1e-6, 1, log=True)\n",
    "        )\n",
    "    activation = trial.suggest_categorical(f'activation_1', ['relu', 'tanh', 'selu', 'linear', 'sigmoid', 'elu'])    \n",
    "    model.add(keras.layers.Dense(num_units, activation=activation, kernel_regularizer=kernel_regularizer))    \n",
    "    model.add(keras.layers.Dropout(rate=dropout_rate))\n",
    "    \n",
    "    num_last = trial.suggest_int('n_units_last', 1, 32)\n",
    "    dropout_last = trial.suggest_float('dropout_last', 0.0, 0.5)\n",
    "    activation_last = trial.suggest_categorical('activation_last', ['relu', 'tanh', 'selu', 'linear', 'sigmoid', 'elu'])\n",
    "    kernel_regularizer_last = regularizers.l1_l2( \n",
    "        l1= trial.suggest_float('l1_reg_last', 1e-6, 1, log=True),\n",
    "        l2= trial.suggest_float('l2_reg_last', 1e-6, 1, log=True)\n",
    "    )\n",
    "    model.add(keras.layers.Dense(num_last, activation=activation_last, kernel_regularizer=kernel_regularizer_last))        \n",
    "    model.add(keras.layers.Dropout(rate=dropout_last))\n",
    "    model.add(keras.layers.Dense(1, activation='linear')) \n",
    "    \n",
    "    # Optimisaattorin ja oppimisnopeuden valinta\n",
    "    optimizer_options = ['adam', 'rmsprop', 'Nadam', 'adamax', 'Adagrad', 'Adadelta']\n",
    "    optimizer_selected = trial.suggest_categorical('optimizer', optimizer_options)\n",
    "    learning_rate = trial.suggest_float('lr', 1e-4, 1.0, log=True)\n",
    "    \n",
    "    if optimizer_selected == 'adam':\n",
    "        optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_selected == 'rmsprop':\n",
    "        optimizer = optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer_selected == 'Nadam':\n",
    "        optimizer = optimizers.Nadam(learning_rate=learning_rate)\n",
    "    elif optimizer_selected == 'Adagrad':\n",
    "        optimizer = optimizers.Adagrad(learning_rate=learning_rate)\n",
    "    elif optimizer_selected == 'Adadelta':\n",
    "        optimizer = optimizers.Adadelta(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = optimizers.Adamax(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=rmsle_loss, metrics=['mse', 'mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    model = create_model(trial)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 128, log=True)    \n",
    "    callbacks = [TFKerasPruningCallback(trial, 'val_loss'),\n",
    "                 ReduceLROnPlateau('val_loss', patience=5, factor=0.7), \n",
    "                 TerminateOnNaN()]\n",
    "\n",
    "    history = model.fit(X_train_b, y_train_b, epochs=epochs_search, validation_data=(X_val_b, y_val_b) ,batch_size=batch_size, verbose=0, callbacks=callbacks)\n",
    "    val_loss = np.min(history.history['val_loss'])\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "total_time_start = time.time()  \n",
    "search_time_start = time.time() \n",
    "num_completed_trials = 0\n",
    "search_rounds = 0\n",
    "time_taken = 0\n",
    "while time_taken < max_search_time:\n",
    "        \n",
    "    fold = 0 \n",
    "    kf = KFold(n_splits=folds)\n",
    "    \n",
    "    time_fold_start = time.time()    \n",
    "    for train_index, val_index in kf.split(X_train_NN):\n",
    "\n",
    "        print('-------------------')\n",
    "        print(f\"Starting fold {fold} search...\")\n",
    "        X_train_b, X_val_b = X_train_NN[train_index], X_train_NN[val_index]    \n",
    "        y_train_b, y_val_b = y_train_NN[train_index], y_train_NN[val_index]\n",
    "\n",
    "        fold_name = f'{study_name}_{fold}'\n",
    "       \n",
    "        study = optuna.create_study(direction='minimize',\n",
    "                                    pruner=optuna.pruners.HyperbandPruner(min_resource=5),\n",
    "                                    study_name=fold_name,\n",
    "                                    storage=f'sqlite:///tampere_reg.db',\n",
    "                                    load_if_exists=True                                 \n",
    "                                    )\n",
    "\n",
    "        fold_time = time.time()    \n",
    "\n",
    "        fold_random = time.time()\n",
    "        # # study.sampler = optuna.samplers.RandomSampler()\n",
    "        study.sampler = optuna.samplers.QMCSampler(warn_independent_sampling = False) # TODO tämä testiin, vaikutti paljon paremmalta kuin random \n",
    "        print(f'Random search for fold {fold}...')\n",
    "        study.optimize(objective, n_trials=num_random)\n",
    "        print(f'Time taken for random search: {str(timedelta(seconds=(time.time() - fold_random)))}')\n",
    "\n",
    "        # fold_tpe = time.time()  \n",
    "        # study.sampler = optuna.samplers.TPESampler(n_startup_trials=0)\n",
    "        # print(f'TPE search for fold {fold}...')\n",
    "        # study.optimize(objective, n_trials=num_tpe)\n",
    "        # print(f'Time taken for TPE search: {str(timedelta(seconds=(time.time() - fold_tpe)))}')\n",
    "\n",
    "        num_completed_trials += num_random + num_tpe\n",
    "        print('-------------------')\n",
    "        print(f'Finished fold {fold} search.')\n",
    "        print(f\"Time taken for this fold: {str(timedelta(seconds=(time.time() - fold_time)))}\")                \n",
    "        print(f'Fold {fold} best value so far: {study.best_value}')\n",
    "        print(f'Mean time for one trial this fold: {str(timedelta(seconds=(time.time() - fold_time) / (num_random + num_tpe)))}')\n",
    "\n",
    "        fold += 1\n",
    "    search_rounds += 1\n",
    "    \n",
    "    time_taken = time.time() - search_time_start\n",
    "    \n",
    "    print(f'\\n# Completed search round: {search_rounds} #')\n",
    "    print(f'Time taken for all folds this round: {str(timedelta(seconds=(time.time() - time_fold_start)))}')\n",
    "    print(f'Total time taken for search: {str(timedelta(seconds=(time.time() - search_time_start)))}')\n",
    "    print(f'Made trials this far: {num_completed_trials}')\n",
    "    print(f\"Current mean time for one trial: {str(timedelta(seconds=(time.time() - search_time_start) / num_completed_trials))}\\n\")\n",
    "\n",
    "####\n",
    "\n",
    "####\n",
    "\n",
    "# num_tpe = 42\n",
    "    # num_random = 0\n",
    "    # max_search_time = 7200\n",
    "    \n",
    "# while time_taken < max_search_time:\n",
    "        \n",
    "#     fold = 0 \n",
    "#     kf = KFold(n_splits=folds)\n",
    "    \n",
    "#     time_fold_start = time.time()    \n",
    "#     for train_index, val_index in kf.split(X_train_NN):\n",
    "\n",
    "#         print('-------------------')\n",
    "#         print(f\"Starting fold {fold} search...\")\n",
    "#         X_train_b, X_val_b = X_train_NN[train_index], X_train_NN[val_index]    \n",
    "#         y_train_b, y_val_b = y_train_NN[train_index], y_train_NN[val_index]\n",
    "\n",
    "#         fold_name = f'{study_name}_{fold}'\n",
    "       \n",
    "#         study = optuna.create_study(direction='minimize',\n",
    "#                                     pruner=optuna.pruners.HyperbandPruner(min_resource=20),\n",
    "#                                     study_name=fold_name,\n",
    "#                                     storage=f'sqlite:///tampere_reg.db',\n",
    "#                                     load_if_exists=True                                 \n",
    "#                                     )\n",
    "\n",
    "#         fold_time = time.time()    \n",
    "\n",
    "#         # fold_random = time.time()\n",
    "#         # study.sampler = optuna.samplers.RandomSampler()\n",
    "#         # print(f'Random search for fold {fold}...')\n",
    "#         # study.optimize(objective, n_trials=num_random)\n",
    "#         # print(f'Time taken for random search: {str(timedelta(seconds=(time.time() - fold_random)))}')\n",
    "\n",
    "#         fold_tpe = time.time()  \n",
    "#         study.sampler = optuna.samplers.TPESampler(n_startup_trials=0)\n",
    "#         print(f'TPE search for fold {fold}...')\n",
    "#         study.optimize(objective, n_trials=num_tpe)\n",
    "#         print(f'Time taken for TPE search: {str(timedelta(seconds=(time.time() - fold_tpe)))}')\n",
    "\n",
    "#         num_completed_trials += num_random + num_tpe\n",
    "#         print('-------------------')\n",
    "#         print(f'Finished fold {fold} search.')\n",
    "#         print(f\"Time taken for this fold: {str(timedelta(seconds=(time.time() - fold_time)))}\")                \n",
    "#         print(f'Fold {fold} best value so far: {study.best_value}')\n",
    "#         print(f'Mean time for one trial this fold: {str(timedelta(seconds=(time.time() - fold_time) / (num_random + num_tpe)))}')\n",
    "\n",
    "#         fold += 1\n",
    "#     search_rounds += 1\n",
    "    \n",
    "#     time_taken = time.time() - search_time_start\n",
    "    \n",
    "#     print(f'\\n# Completed search round: {search_rounds} #')\n",
    "#     print(f'Time taken for all folds this round: {str(timedelta(seconds=(time.time() - time_fold_start)))}')\n",
    "#     print(f'Total time taken for search: {str(timedelta(seconds=(time.time() - search_time_start)))}')\n",
    "#     print(f'Made trials this far: {num_completed_trials}')\n",
    "#     print(f\"Current mean time for one trial: {str(timedelta(seconds=(time.time() - search_time_start) / num_completed_trials))}\\n\")\n",
    "\n",
    "###\n",
    "\n",
    "print('='*20)    \n",
    "print(f'Finished search.')    \n",
    "print(f'Total time taken for all folds: {str(timedelta(seconds=(time.time() - search_time_start)))}')\n",
    "print(f'Made {num_completed_trials} trials in total.')\n",
    "print(f\"Mean time for one trial: {str(timedelta(seconds=(time.time() - search_time_start) / num_completed_trials))}\")\n",
    "print('='*20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# for fold in range(folds):\n",
    "    \n",
    "#     study_name_fold = f'{study_name}_{fold}'\n",
    "#     print('*'*50)\n",
    "#     print(f'{study_name_fold}')\n",
    "#     print('*'*50)\n",
    "#     study = optuna.load_study(study_name=study_name_fold, storage='sqlite:///tampere_reg.db')\n",
    "    \n",
    "#     # fig = optuna.visualization.plot_param_importances(study)\n",
    "#     # fig.update_layout(title=f'Param Importance for Fold {fold}', width=1800, height=1800)\n",
    "#     # fig.show()\n",
    "\n",
    "#     # fig = optuna.visualization.plot_slice(study)\n",
    "#     # fig.update_layout(title=f'Slice for Fold {fold}', width=800, height=800)\n",
    "#     # fig.show()\n",
    "    \n",
    "#     fig = optuna.visualization.plot_parallel_coordinate(study)\n",
    "#     fig.update_layout(width=2000, height=2000)\n",
    "#     fig.show()\n",
    "\n",
    "#     # fig = optuna.visualization.plot_contour(study, params=['n_units_0', 'n_units_1', 'n_units_last'])\n",
    "#     # fig.update_layout(width=1800, height=1800)\n",
    "#     # fig.show()\n",
    "\n",
    "#     # fig = optuna.visualization.plot_contour(study)\n",
    "#     # fig.update_layout(width=3600, height=3600)\n",
    "#     # fig.show()\n",
    "\n",
    "#     # fig = optuna.visualization.plot_edf(study)\n",
    "#     # fig.update_layout(width=1800, height=1800)\n",
    "#     # fig.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import optuna\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from keras import regularizers, layers, optimizers, initializers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "import time \n",
    "import os \n",
    "\n",
    "\n",
    "folds = 5\n",
    "# Montako epochia kullekin parhaalle sovitetaan malli\n",
    "epochs_best_fit = 500\n",
    "# Montako paras otetaan mukaan osittelusta\n",
    "num_best = 6\n",
    "# Montako kertaa kullekin parhaalle sovitetaan malli\n",
    "num_best_fits = 1\n",
    "\n",
    "best_optuna_models = []\n",
    "best_val_scores = []\n",
    "best_optuna_trials = [] \n",
    "\n",
    "kf = KFold(n_splits=folds)\n",
    "fold_num = 0\n",
    "fitting_search_start = time.time()\n",
    "\n",
    "for train_index, val_index in kf.split(X_train_NN):\n",
    "\n",
    "    best_fitting_time = time.time()\n",
    "    print(f\"Fold {fold_num} Best best trial fitting...\")\n",
    "\n",
    "    X_train_b, X_val_b = X_train_NN[train_index], X_train_NN[val_index]    \n",
    "    y_train_b, y_val_b = y_train_NN[train_index], y_train_NN[val_index]\n",
    "    \n",
    "    fold_name = f'{study_name}_{fold_num}'\n",
    "       \n",
    "    study = optuna.create_study(                                \n",
    "                                study_name=fold_name,\n",
    "                                storage=f'sqlite:///tampere_reg.db',\n",
    "                                load_if_exists=True\n",
    "                                )\n",
    "\n",
    "    valid_trials = [trial for trial in study.trials if trial.value is not None]\n",
    "    sorted_trials = sorted(valid_trials, key=lambda trial: trial.value)\n",
    "    best_trials = sorted_trials[:num_best]\n",
    "    best_val = np.inf\n",
    "    best_model = None\n",
    "\n",
    "    print('='*30)\n",
    "    print(f'Fitting best trials for fold {fold_num}...')\n",
    "    fitting_fold_best_start = time.time()\n",
    "    \n",
    "    for trial in best_trials:\n",
    "\n",
    "        for fit_num in range(num_best_fits):\n",
    "            \n",
    "            print('-'*30)\n",
    "            print(f\"Trial ID: {trial.number}, Value: {trial.value}, fit number: {fit_num}\")\n",
    "\n",
    "            checkpoint_filepath = f'./NN_search/optuna_search_checkpoint.h5'\n",
    "            model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=checkpoint_filepath,\n",
    "                save_weights_only=True,\n",
    "                monitor='val_loss',\n",
    "                mode='min',\n",
    "                save_best_only=True)\n",
    "\n",
    "            best_callback = [model_checkpoint_callback,                  \n",
    "                            ReduceLROnPlateau('val_loss', patience=10, factor=0.8), \n",
    "                            TerminateOnNaN(),\n",
    "                            EarlyStopping(monitor='val_loss', patience=100, verbose=1)\n",
    "                        ]\n",
    "\n",
    "\n",
    "            model = create_model(trial)\n",
    "            model.fit(X_train_b, y_train_b, epochs=epochs_best_fit, validation_data=(X_val_b, y_val_b), batch_size=trial.params['batch_size'], verbose=0, callbacks=best_callback)\n",
    "            model.load_weights(checkpoint_filepath)\n",
    "\n",
    "            predictions = model.predict(X_val_b, verbose=0)\n",
    "            mse = mean_squared_error(y_val_b, predictions)\n",
    "            mae = mean_absolute_error(y_val_b, predictions)\n",
    "            r2 = r2_score(y_val_b, predictions)\n",
    "            rmsle = rmsle_score(y_val_b, predictions)\n",
    "\n",
    "                        \n",
    "            print(f'MSE:{mse:.5f}\\nMAE:{mae:.5f}\\nRMSLE:{rmsle:.5f}\\nR2:{r2:.5f}')\n",
    "\n",
    "            if rmsle < best_val:\n",
    "                best_model = model\n",
    "                best_val = rmsle\n",
    "                best_trial_num = trial.number\n",
    "                best_trial = trial\n",
    "                print(f'*** New best model for fold {fold_num} is Trial {best_trial_num} with RMSLE {best_val} ***')\n",
    "                print(f'Best trial hyperparameters: {trial.params}')\n",
    "    \n",
    "    if best_model is not None:\n",
    "\n",
    "        best_optuna_models.append(best_model)\n",
    "        best_val_scores.append(best_val)\n",
    "        best_optuna_trials.append(best_trial)\n",
    "        print('*'*40)\n",
    "        print(f\"Best model for fold {fold_num} RMSLE: {best_val}\\nTrial number: {best_trial_num}\\nHyperparameters: {best_trial.params}\")\n",
    "        print(f\"Time taken for best fitting in fold {fold_num}: {str(timedelta(seconds=(time.time() - best_fitting_time)) )}\")\n",
    "        print('*'*40)\n",
    "\n",
    "    fold_num += 1\n",
    "\n",
    "print('*'*40)\n",
    "print(f'Best models fitting time total:', str(timedelta(seconds=(time.time() - fitting_search_start))))\n",
    "print(f\"Total time taken for search and fitting best models: {str(timedelta(seconds=(time.time() - total_time_start)))}\")\n",
    "print('*'*40)   \n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "for i, (model, score) in enumerate(zip(best_optuna_models, best_val_scores)):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    directory = f\"./NN_search/{study_name}_foldmodel{i}_score_{score:.4f}_{timestamp}.h5\"\n",
    "    print(f\"Saving model {i} with score {score:.4f} to {directory}\")\n",
    "    model.save(directory)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, model in enumerate(best_optuna_models):\n",
    "    print(f\"\\nModel {idx} Summary:\")\n",
    "    # model.summary()\n",
    "    \n",
    "    # Testaa mallia testidatalla\n",
    "    predictions = model.predict(X_test_NN, verbose = 0)\n",
    "    mse = mean_squared_error(y_test_NN, predictions)\n",
    "    mae = mean_absolute_error(y_test_NN, predictions)\n",
    "    r2 = r2_score(y_test_NN, predictions)\n",
    "    rmsle = rmsle_score(y_test_NN, predictions)\n",
    "    \n",
    "    print(f\"\\nModel {idx} Performance on Test Data:\")\n",
    "    print(f\"MSE: {mse:.3f}\")\n",
    "    print(f\"MAE: {mae:.3f}\")\n",
    "    print(f\"R2: {r2:.3f}\")\n",
    "    print(f\"RMSLE: {rmsle:.3f}\")\n",
    "    print(\"*\"*40)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "import time\n",
    "from tensorflow.keras.models import Model\n",
    "import glob\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "time_started_xgb = time.time()\n",
    "\n",
    "num_of_trials = 100\n",
    "\n",
    "# Kerätään ensin kaikkien mallien ominaisuusvektorit\n",
    "X_train_features_list = []\n",
    "X_test_features_list = []\n",
    "features_names_list = []\n",
    "\n",
    "best_optuna_models = []\n",
    "\n",
    "def rmsle_loss(y_true, y_pred):\n",
    "    penalty = tf.constant(1e5, dtype=tf.float32)\n",
    "    valid_mask = tf.math.greater(y_pred, 0.0)\n",
    "    safe_y_pred = tf.where(valid_mask, y_pred, penalty)\n",
    "    rmsle = tf.sqrt(tf.reduce_mean(tf.square(tf.math.log1p(safe_y_pred) - tf.math.log1p(y_true))))\n",
    "    return tf.where(tf.reduce_any(~valid_mask), penalty, rmsle)\n",
    "\n",
    "# Kun lataat mallin, määritä mukautettu häviöfunktio custom_objects-parametrissa\n",
    "custom_objects = {\"rmsle_loss\": rmsle_loss}\n",
    "\n",
    "# for fold_num in [1,3,4]:\n",
    "for fold_num in range(folds): # TODO testiä parhailla malleilla\n",
    "    pattern = f\"./NN_search/{study_name}_foldmodel{fold_num}_score_*.h5\"  # Oletetaan, että mallit on tallennettu .h5-muodossa\n",
    "    model_files = glob.glob(pattern)\n",
    "    \n",
    "    # Etsi suurin score kunkin foldin mallitiedostoista\n",
    "    best_score = -float('inf')\n",
    "    best_model_file = None\n",
    "    for model_file in model_files:\n",
    "        score_part = model_file.split('_score_')[1]  # Erottaa score osan tiedostonimestä        \n",
    "        score = float(score_part.split('_')[0])  # Muuttaa scoren float-arvoksi\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_model_file = model_file\n",
    "    \n",
    "    # Lataa parhaan mallin tiedosto\n",
    "    if best_model_file:\n",
    "        best_model = load_model(best_model_file, custom_objects=custom_objects)\n",
    "        best_optuna_models.append(best_model)\n",
    "        print(f\"Loaded best model for fold {fold_num} from {best_model_file} with score {best_score:.4f}\")\n",
    "    else:\n",
    "        print(f\"No model files found for fold {fold_num} matching pattern {pattern}\")\n",
    "\n",
    "\n",
    "# best_models_per_fold-listas\n",
    "\n",
    "original_feature_names = list(X_train.columns) \n",
    "\n",
    "for idx, model in enumerate(best_optuna_models):\n",
    "    feature_extractor = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "    X_train_features = feature_extractor.predict(X_train_NN)\n",
    "    X_test_features = feature_extractor.predict(X_test_NN)\n",
    "    \n",
    "    X_train_features_list.append(X_train_features)\n",
    "    X_test_features_list.append(X_test_features)\n",
    "\n",
    "    print(f'Model train feature shape: {X_train_features.shape}')\n",
    "    print(f'Model test feature shape: {X_test_features.shape}')\n",
    "\n",
    "    num_features = X_train_features.shape[1]\n",
    "    model_feature_names = [f\"model_{idx}_feature_{feature_idx}\" for feature_idx in range(num_features)]\n",
    "    features_names_list.extend(model_feature_names)\n",
    "\n",
    "combined_feature_names = original_feature_names + features_names_list\n",
    "\n",
    "# Yhdistetään ominaisuusvektorit\n",
    "X_train_combined = np.concatenate(X_train_features_list, axis=1)\n",
    "X_test_combined = np.concatenate(X_test_features_list, axis=1)\n",
    "\n",
    "X_train_combined = np.concatenate([X_train_combined, X_train], axis=1)   \n",
    "X_test_combined = np.concatenate([X_test_combined, X_test], axis=1)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "\n",
    "def rmsle(y_true, y_pred):\n",
    "    if np.any(y_pred <= 0):\n",
    "        return 1e6\n",
    "    return np.sqrt(np.mean(np.square(np.log1p(y_pred) - np.log1p(y_true))))\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "\n",
    "max_feature = X_train_combined.shape[1]\n",
    "\n",
    "def objective(trial):\n",
    "    # Määritetään parametrit, jotka optimoidaan\n",
    "    param = {\n",
    "        'tree_method': 'hist',          \n",
    "        'objective': trial.suggest_categorical('objective', ['reg:squarederror', 'reg:absoluteerror']),        \n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 10),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1, 10)        \n",
    "    }\n",
    "    num_boost_round = trial.suggest_int('num_boost_round', 1, 500, log=True)\n",
    "\n",
    "    # K-fold cross-validation\n",
    "    kf = KFold(n_splits=5)\n",
    "    rmsle_scores = []\n",
    "\n",
    "    select_amount = trial.suggest_int('select_amount', 1, max_feature)\n",
    "    selector = SelectKBest(mutual_info_regression, k=select_amount)\n",
    "    X_train_combined_selected = selector.fit_transform(X_train_combined, y_train)\n",
    "    selected_features = np.array(combined_feature_names)[selector.get_support()]\n",
    "    selected_features = selected_features.tolist()\n",
    "\n",
    "    for train_index, val_index in kf.split(X_train_combined_selected):\n",
    "        X_train_k, X_val_k = X_train_combined_selected[train_index], X_train_combined_selected[val_index]\n",
    "        y_train_k, y_val_k = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_train_k, label=y_train_k, feature_names=selected_features)\n",
    "        dval = xgb.DMatrix(X_val_k, label=y_val_k, feature_names=selected_features)\n",
    "\n",
    "        evals_result = {}\n",
    "        bst = xgb.train(param, dtrain, num_boost_round=num_boost_round, evals=[(dval, 'val')], evals_result=evals_result, verbose_eval=False, early_stopping_rounds=200)\n",
    "        # Käytä paras iteraatio määrä laskemaan ennusteet ja RMSLE\n",
    "        best_iteration = bst.best_iteration\n",
    "        preds = bst.predict(dval, iteration_range=(0, best_iteration + 1))\n",
    "        loss = rmsle(y_val_k, preds)\n",
    "        rmsle_scores.append(loss)\n",
    "\n",
    "    average_rmsle = np.mean(rmsle_scores)\n",
    "    return average_rmsle\n",
    "\n",
    "# study = optuna.create_study(direction='minimize', \n",
    "#                             storage='sqlite:///tampere_reg.db', \n",
    "#                             study_name='xgb_combined_selection_2503', # TODO muuta nimeä tarvittaessa\n",
    "#                             load_if_exists=False) \n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "\n",
    "# study.sampler = optuna.samplers.RandomSampler()\n",
    "study.sampler = optuna.samplers.QMCSampler(warn_independent_sampling = False)\n",
    "print(f'Random sampling {num_of_trials} trials...')\n",
    "study.optimize(objective, n_trials=num_of_trials)\n",
    "study.sampler = optuna.samplers.TPESampler()\n",
    "print(f'TPE sampling {num_of_trials} trials...')\n",
    "study.optimize(objective, n_trials=num_of_trials)\n",
    "\n",
    "\n",
    "print(f'Time taken for XGBoost optimization: {str(timedelta(seconds=(time.time() - time_started_xgb)))}')\n",
    "print(f'Time taken for one trial: {str(timedelta(seconds=(time.time() - time_started_xgb) / (num_of_trials*2)))}')\n",
    "print(f'X_train_combined shape: {X_train_combined.shape}')\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "\n",
    "# Parhaiden parametrien tulostus ja mallin koulutus\n",
    "print(f\"Best trial: {study.best_trial.params}\")\n",
    "best_params = study.best_trial.params\n",
    "\n",
    "best_selector = SelectKBest(mutual_info_regression, k=best_params['select_amount'])\n",
    "X_train_combined_selected = best_selector.fit_transform(X_train_combined, y_train)\n",
    "selected_features = np.array(combined_feature_names)[best_selector.get_support()]\n",
    "selected_features = selected_features.tolist()\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train_combined_selected, label=y_train, feature_names=selected_features)\n",
    "best_model = xgb.train(best_params, dtrain, num_boost_round=best_params['num_boost_round'])\n",
    "\n",
    "X_test_combined_selected = best_selector.transform(X_test_combined)\n",
    "\n",
    "# Ennustukset ja evaluointi testidatalla\n",
    "dtest = xgb.DMatrix(X_test_combined_selected, label=y_test, feature_names=selected_features)\n",
    "predictions = best_model.predict(dtest)\n",
    "\n",
    "# Visualisoidaan ennustettuja arvoja verrattuna todellisiin arvoihin\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(y_test, predictions, edgecolors=(0, 0, 0))\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "plt.xlabel('Measured')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Measured vs. Predicted Values')\n",
    "plt.show()\n",
    "\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "rmsle_val = rmsle(y_test, predictions)\n",
    "print(f\"MAE: {mae}, MSE: {mse}, R2: {r2}, RMSLE: {rmsle_val}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 30))\n",
    "xgb.plot_importance(best_model, importance_type='weight', ax=ax)\n",
    "ax.set_title('Feature Importance by Weight', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Asetetaan toisen kuvaajan koko\n",
    "fig, ax = plt.subplots(figsize=(20, 30))\n",
    "xgb.plot_importance(best_model, importance_type='gain', ax=ax)\n",
    "ax.set_title('Feature Importance by Gain', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Asetetaan kolmannen kuvaajan koko\n",
    "fig, ax = plt.subplots(figsize=(20, 30))\n",
    "xgb.plot_importance(best_model, importance_type='cover', ax=ax)\n",
    "ax.set_title('Feature Importance by Cover', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "joda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
